Hello, my name is Andrej and I've been training deep neural networks for a bit more than a decade. And in this lecture I'd like to show you what neural network training looks like under the hood. So in particular we are going to start with a blank Jupyter notebook and by the end of this lecture we will define and train a neural net and you'll get to see everything that goes on under the hood and exactly sort of how that works on an intuitive level. Now specifically what I would like to do is I would like to take you through building of micrograd. Now micrograd is this library that I released on GitHub about two years ago but at the time I only uploaded the source code and you'd have to go in by yourself and really figure out how it works. So in this lecture I will take you through it step by step and kind of comment on all the pieces of it. So what is micrograd and why is it interesting? Thank you. Micrograd is basically an autograd engine. Autograd is short for automatic gradient and really what it does is it implements back propagation. Now back propagation is this algorithm that allows you to efficiently evaluate the gradient of some kind of a loss function with respect to the weights of a neural network and what that allows us to do then is we can iteratively tune the weights of that neural network to minimize the loss function and therefore improve the accuracy of the network. So back propagation would be at the mathematical core of any modern deep neural network library like say PyTorch or JAX. So the functionality of micrograd is I think best illustrated by an example. So if we just scroll down here you'll see that micrograd basically allows you to build out mathematical expressions and here what we are doing is we have an expression that we're building out where you have two inputs a and b and you'll see that a and b are negative four and two but we are wrapping those values into this value object that we are going to build out as part of micrograd. So this value object will wrap the numbers themselves and then we are going to build out a mathematical expression here where a and b are transformed into c d and eventually e f and g and I'm showing some of the functionality of micrograd and the operations that it supports. So you can add two value objects, you can multiply them, you can raise them to a constant power, you can offset by one, negate, squash at zero, square, divide by constant, divide by it, etc. And so we're building out an expression graph with these two inputs a and b and we're creating an output value of g and micrograd will in the background build out this entire mathematical expression. So it will for example know that c is also a value, c was a result of an addition operation and the child nodes of c are a and b because the and it will maintain pointers to a and b value objects. So we'll basically know exactly how all of this is laid out and then not only can we do what we call the forward pass where we actually look at the value of g of course, that's pretty straightforward, we will access that using the dot data attribute and so the output of the forward pass, the value of g, is 24.7 it turns out. But the big deal is that we can also take this g value object and we can call dot backward and this will basically initialize backpropagation at the node g. And what backpropagation is going to do is it's going to start at g and it's going to go backwards through that expression graph and it's going to recursively apply the chain rule from calculus. And what that allows us to do then is we're going to evaluate basically the derivative of g with respect to all the internal nodes like e, d, and c but also with respect to the inputs a and b. And then we can actually query this derivative of g with respect to a, for example that's a.grad, in this case it happens to be 138, and the derivative of g with respect to b which also happens to be here 645. And this derivative we'll see soon is very important information because it's telling us how a and b are affecting g through this mathematical expression. So in particular a.grad is 138, so if we slightly nudge a and make it slightly larger, 138 is telling us that g will grow and the slope of that growth is going to be 138 and the slope of growth of b is going to be 645. So that's going to tell us about how g will respond if a and b get tweaked a tiny amount in a positive direction. Now you might be confused about what this expression is that we built out here and this expression by the way is completely meaningless. I just made it up, I'm just flexing about the kinds of operations that are supported by micrograd. What we actually really care about are neural networks but it turns out that neural networks are just mathematical expressions just like this one but actually slightly a bit less crazy even. Neural networks are just a mathematical expression, they take the input data as an input and they take the weights of a neural network as an input and it's a mathematical expression and the output are your predictions of your neural net or the loss function, we'll see this in a bit. But basically neural networks just happen to be a certain class of mathematical expressions but back propagation is actually significantly more general. It doesn't actually care about neural networks at all, it only cares about arbitrary mathematical expressions and then we happen to use that machinery for training of neural networks. Now one more note I would like to make at this stage is that as you see here micrograd is a scalar valued autograd engine so it's working on the you know level of individual scalars like negative 4 and 2 and we're taking neural nets and we're breaking them down all the way to these atoms of individual scalars and all the little pluses and times and it's just excessive and so obviously you would never be doing any of this in production. It's really just done for pedagogical reasons because it allows us to not have to deal with these n-dimensional tensors that you would use in modern deep neural network library. So this is really done so that you understand and refactor out back propagation and chain rule and understanding of neural training and then if you actually want to train bigger networks you have to be using these tensors but none of the math changes, this is done purely for efficiency. We are basically taking all the scalars all the scalar values we're packaging them up into tensors which are just arrays of these scalars and then because we have these large arrays we're making operations on those large arrays that allows us to take advantage of the parallelism in a computer and all those operations can be done in parallel and then the whole thing runs faster but really none of the math changes and they're done purely for efficiency so I don't think that it's pedagogically useful to be dealing with tensors from scratch and I think and that's why I fundamentally wrote micrograd because you can understand how things work at the fundamental level and then you can speed it up later. Okay so here's the fun part. My claim is that micrograd is what you need to train neural networks and everything else is just efficiency so you'd think that micrograd would be a very complex piece of code and that turns out to not be the case. So if we just go to micrograd and you'll see that there's only two files here in micrograd. This is the actual engine, it doesn't know anything about neural nets and this is the entire neural nets library on top of micrograd. So engine and nn.py. So the actual back propagation autograd engine that gives you the power of neural networks is literally 100 lines of code of like very simple python which we'll understand by the end of this lecture and then nn.py, this neural network library built on top of the autograd engine is like a joke. It's like we have to define what is a neuron and then we have to define what is a layer of neurons and then we define what is a multilayer perceptron which is just a sequence of layers of neurons and so it's just a total joke. So basically there's a lot of power that comes from only 150 lines of code and that's all you need to understand to understand neural network training and everything else is just efficiency and of course there's a lot to efficiency but fundamentally that's all that's happening. Okay so now let's dive right in and implement micrograd step by step. The first thing I'd like to do is I'd like to make sure that you have a very good understanding intuitively of what a derivative is and exactly what information it gives you. So let's start with some basic imports that I copy-paste in every jupyter notebook always and let's define a function, a scalar valued function f of x as follows. So I just made this up randomly. I just wanted a scalar valued function that takes a single scalar x and returns a single scalar y and we can call this function of course so we can pass in say 3.0 and get 20 back. Now we can also plot this function to get a sense of its shape. You can tell from the mathematical expression that this is probably a parabola, it's a quadratic and so if we just create a set of scalar values that we can feed in using for example a range from negative 5 to 5 in steps of 0.25. So this is so x is just from negative 5 to 5 not including 5 in steps of 0.25 and we can actually call this function on this numpy array as well so we get a set of y's if we call f on x's and these y's are basically also applying the function on every one of these elements independently and we can plot this using matplotlib. So plt.plot x's and y's and we get a nice parabola. So previously here we fed in 3.0 somewhere here and we received 20 back which is here the y-coordinate. So now I'd like to think through what is the derivative of this function at any single input point x. So what is the derivative at different points x of this function? Now if you remember back to your calculus class you've probably derived derivatives so we take this mathematical expression 3x squared minus 4x plus 5 and you would write out on a piece of paper and you would apply the product rule and all the other rules and derive the mathematical expression of the great derivative of the original function and then you could plug in different texts and see what the derivative is. We're not going to actually do that because no one in neural networks actually writes out the expression for the neural net. It would be a massive expression, it would be thousands, tens of thousands of terms. No one actually derives the derivative of course and so we're not going to take this kind of like symbolic approach. Instead what I'd like to do is I'd like to look at the definition of derivative and just make sure that we really understand what the derivative is measuring, what it's telling you about the function. And so if we just look up derivative we see that okay so this is not a very good definition of derivative. This is a definition of what it means to be differentiable but if you remember from your calculus it is the limit as h goes to zero of f of x plus h minus f of x over h. So basically what it's saying is if you slightly bump up your at some point x that you're interested in or a and if you slightly bump up you know you slightly increase it by small number h how does the function respond with what sensitivity does it respond where is the slope at that point does the function go up or does it go down and by how much and that's the slope of that function the the slope of that response at that point and so we can basically evaluate the derivative here numerically by taking a very small h of course the definition would ask us to take h to zero we're just going to pick a very small h 0.001 and let's say we're interested in 0.3.0 so we can look at f of x of course as 20 and now f of x plus h so if we slightly nudge x in a positive direction how is the function going to respond and just looking at this do you expand do you expect f of x plus h to be slightly greater than 20 or do you expect it to be slightly lower than 20 and since this 3 is here and this is 20 if we slightly go positively the function will respond positively so you'd expect this to be slightly greater than 20 and now by how much is telling you the sort of the the strength of that slope right the the size of the slope so f of x plus h minus f of x this is how much the function responded in a positive direction and we have to normalize by the run so we have the rise over run to get the slope so this of course is just a numerical approximation of the slope because we have to make h very very small to converge to the exact amount now if i'm doing too many zeros at some point i'm going to i'm going to get an incorrect answer because we're using floating point arithmetic and the representations of all these numbers in computer memory is finite and at some point we get into trouble so we can converge towards the right answer with this approach but basically at 3 the slope is 14 and you can see that by taking 3x squared minus 4x plus 5 and differentiating it in our head so 3x squared would be 6x minus 4 and then we plug in x equals 3 so that's 18 minus 4 is 14 so this is correct so that's at 3 now how about the slope at say negative 3 would you expect what would you expect for the slope now telling the exact value is really hard but what is the sign of that slope so at negative 3 if we slightly go in the positive direction at x the function would actually go down and so that tells you that the slope would be negative so we'll get a slight number below below 20 and so if we take the slope we expect something negative negative 22 okay and at some point here of course the slope would be zero now for this specific function i looked it up previously and it's at point uh 2 over 3 so at roughly 2 over 3 that's somewhere here this this derivative would be zero so basically at that precise point yeah at that precise point if we nudge in a positive direction the function doesn't respond this stays the same almost and so that's why the slope is zero okay now let's look at a bit more complex case so we're going to start you know complexifying a bit so now we have a function here with output variable d that is a function of three scalar inputs a b and c so a b and c are some specific values three inputs into our expression graph and a single output d and so if we just print d we get four and now what i like to do is i'd like to again look at the derivatives of d with respect to a b and c and uh think through uh again just the intuition of what this derivative is telling us so in order to evaluate this derivative we're going to get a bit hacky here we're going to again have a very small value of h and then we're going to fix the inputs at some values that we're interested in so these are the this is the point a b c at which we're going to be evaluating the the derivative of d with respect to all a b and c at that point so there are the inputs and now we have d1 is that expression and then we're going to for example look at the derivative of d with respect to a so we'll take a and we'll bump it by h and then we'll get d2 to be the exact same function and now we're going to print um you know f1 d1 is d1 d2 is d2 and print slope so the derivative or slope here will be um of course d2 minus d1 divide h so d2 minus d1 is how much the function increased uh when we bumped the uh the specific input that we're interested in by a tiny amount and this is the normalized by this is the normalized by h to get the slope so um yeah so this so i just run this we're going to print d1 which we know is four now d2 will be bumped a will be bumped by h so let's just think through a little bit uh what d2 will be uh printed out here in particular d1 will be four will d2 be a number slightly greater than four or slightly lower than four and that's going to tell us the sign of the derivative so we're bumping a by h b is minus three c is 10 so you can just intuitively think through this derivative and what it's doing a will be slightly more positive and but b is a negative number so if a is slightly more positive because b is negative three we're actually going to be adding less to d so you'd actually expect that the value of the function will go down so let's just see this yeah and so we went from four to 3.9996 and that tells you that the slope will be negative and then um will be a negative number because we went down and then the exact number of slope will be exact amount of slope is negative three and you can also convince yourself that negative three is the right answer um mathematically and analytically because if you have a times b plus c and you are you know you have calculus then uh differentiating a times b plus c with respect to a gives you just b and indeed the value of b is negative three which is the derivative that we have so you can tell that that's correct so now if we do this with b so if we bump b by a little bit in a positive direction we'd get different slopes so what is the influence of b on the output d so if we bump b by a tiny amount in a positive direction then because a is positive we'll be adding more to d right so um and now what is the what is the sensitivity what is the slope of that addition and it might not surprise you that this should be two and why is it two because d of d by db differentiating with respect to b would be would give us a and the value of a is two so that's also working well and then if c gets bumped a tiny amount in h by h then of course a times b is unaffected and now c becomes slightly bit higher what does that do to the function it makes it slightly bit higher because we're simply adding c and it makes it slightly bit higher by the exact same amount that we added to c and so that tells you that the slope is one that will be the the rate at which d will increase as we scale c okay so we now have some intuitive sense of what this derivative is telling you about the function and we'd like to move to neural networks now as i mentioned neural networks will be pretty massive expressions mathematical expressions so we need some data structures that maintain these expressions and that's what we're going to start to build out now so we're going to build out this value object that i showed you in the readme page of micrograd so let me copy paste a skeleton of the first very simple value object so class value takes a single scalar value that it wraps and keeps track of and that's it so we can for example do value of 2.0 and then we can get we can look at its content and python will internally use the wrapper function to return this string like that so this is a value object that we're going to call value object with data equals two that we're creating here. Now we'd like to be able to have not just like two values but we'd like to do a block b, right? We'd like to add them. So currently you would get an error because Python doesn't know how to add two value objects so we have to tell it. So here's addition. So you have to basically use these special double underscore methods in Python to define these operators for these objects. So if we call the, if we use this plus operator, Python will internally call a dot add of b. That's what will happen internally. And so b will be the other and self will be a. And so we see that what we're going to return is a new value. And it's just, it's going to be wrapping the plus of their data. But remember now because data is the actual like numbered Python number. So this operator here is just the typical floating point plus addition now. It's not an addition of value objects and will return a new value. So now a plus b should work and it should print value of negative one because that's two plus minus three. There we go. Okay. Let's now implement multiply just so we can recreate this expression here. So multiply, I think it won't surprise you will be fairly similar. So instead of add, we're going to be using mul and then here, of course, we want to do times. And so now we can create a C value object, which will be 10.0. And now we should be able to do a times b. Well, let's just do a times b first. That's value of negative six now. And by the way, I skipped over this a little bit. Suppose that I didn't have the wrapper function here, then it's just that you'll get some kind of an ugly expression. So what wrapper is doing is it's providing us a way to print out like a nicer looking expression in Python. So we don't just have something cryptic. We actually are, you know, it's value of negative six. So this gives us a times, and then this, we should now be able to add C to it because we've defined and told the Python how to do mul and add. And so this will call, this will basically be equivalent to a.mul of b. And then this new value object will be dot add of c. So this is going to be a.mul of b. And then this is going to be dot add of c. And so let's see if that worked. Yep, so that worked well. That gave us four, which is what we expect from bit four. And I believe we can just call them manually as well. There we go. So yeah. Okay, so now what we are missing is the connected tissue of this expression. As I mentioned, we want to keep these expression graphs. So we need to know and keep pointers about what values produce what other values. So here, for example, we are going to introduce a new variable, which we'll call children. And by default, it will be an empty tuple. And then we're actually going to keep a slightly different variable in the class, which we'll call underscore prev, which will be the set of children. This is how I did it in the original micrograd, looking at my code here. I can't remember exactly the reason. I believe it was efficiency, but this underscore children will be a tuple for convenience. But then when we actually maintain it in the class, it will be just this set, I believe for efficiency. So now, when we are creating a value like this with a constructor, children will be empty and prev will be the empty set. But when we are creating a value through addition or multiplication, we're going to feed in the children of this value, which in this case is self and other. So those are the children here. So now we can do d.prev, and we'll see that the children of the, we now know, are this value of negative six and value of 10. And this, of course, is the value resulting from a times b and the c value, which is 10. Now, the last piece of information we don't know. So we know now the children of every single value, but we don't know what operation created this value. So we need one more element here. Let's call it underscore op. And by default, this is the empty set for leaves. And then we'll just maintain it here. And now the operation will be just a simple string. And in the case of addition, it's plus, in the case of multiplication, it's times. So now we not just have d.prev, we also have a d.op. And we know that d was produced by an addition of those two values. And so now we have the full mathematical expression, and we're building out this data structure, and we know exactly how each value came to be by word expression and from what other values. Now, because these expressions are about to get quite a bit larger, we'd like a way to nicely visualize these expressions that we're building out. So for that, I'm going to copy paste a bunch of slightly scary code that's going to visualize these expression graphs for us. So here's the code, and I'll explain it in a bit. But first, let me just show you what this code does. Basically, what it does is it creates a new function, draw. that we can call on some root node, and then it's going to visualize it. So if we call draw. on d, which is this final value here, that is a times b plus c, it creates something like this. So this is d, and you see that this is a times b, creating an interpreted value, plus c, gives us this output node d. So that's draw. on d. And I'm not going to go through this in complete detail. You can take a look at Graphvis and its API. Graphvis is an open source graph visualization software. And what we're doing here is we're building out this graph in Graphvis API. And you can basically see that trace is this helper function that enumerates all of the nodes and edges in the graph. So that just builds a set of all the nodes and edges. And then we iterate through all the nodes, and we create special node objects for them using dot node. And then we also create edges using dot dot edge. And the only thing that's slightly tricky here is you'll notice that I basically add these fake nodes, which are these operation nodes. So for example, this node here is just like a plus node. And I create these special op nodes here, and I connect them accordingly. So these nodes, of course, are not actual nodes in the original graph. They're not actually a value object. The only value objects here are the things in squares. Those are actual value objects or representations thereof. And these op nodes are just created in this draw dot routine so that it looks nice. Let's also add labels to these graphs just so we know what variables are where. So let's create a special underscore label. Or let's just do label equals empty by default and save it in each node. And then here, we're going to do label as A, label as B, label as C. And then let's create a special E equals A times B. And E dot label will be E. It's kind of naughty. And E will be E plus C. And a D dot label will be B. OK. So nothing really changes. I just added this new E function, new E variable. And then here, when we are printing this, I'm going to print the label here. So this will be a percent S bar. And this will be N dot label. And so now, we have the label on the left here. So this is A, B creating E. And then E plus C creates D, just like we have it here. And finally, let's make this expression just one layer deeper. So D will not be the final output node. Instead, after D, we are going to create a new value object called F. We're going to start running out of variables soon. F will be negative 2.0. And its label will, of course, just be F. And then L, capital L, will be the output of our graph. And L will be T times F. OK. So L will be negative 8, is the output. So now, we don't just draw a D, we draw L. OK. And somehow, the label of L was undefined. Oops. L dot label has to be explicitly given to it. There we go. So L is the output. So let's quickly recap what we've done so far. We are able to build out mathematical expressions using only plus and times so far. They are scalar valued along the way. And we can do this forward pass and build out a mathematical expression. So we have multiple inputs here, A, B, C, and F, going into a mathematical expression that produces a single output, L. And this here is visualizing the forward pass. So the output of the forward pass is negative 8. That's the value. Now, what we'd like to do next is we'd like to run backpropagation. And in backpropagation, we are going to start here at the end, and we're going to reverse and calculate the gradient along all these intermediate values. And really, what we're computing for every single value here, we're going to compute the derivative of that node with respect to L. So the derivative of L with respect to L is just 1. And then we're going to derive what is the derivative of L with respect to F, with respect to D, with respect to C, with respect to E, with respect to B, and with respect to A. And in a neural network setting, you'd be very interested in the derivative of basically this loss function, L, with respect to the weights of a neural network. And here, of course, we have just these variables, A, B, C, and F. But some of these will eventually represent the weights of a neural net. And so we'll need to know how those weights are impacting the loss function. So we'll be interested basically in the derivative of the output with respect to some of its leaf nodes. And those leaf nodes will be the weights of the neural net. And the other leaf nodes, of course, will be the data itself. But usually, we will not want or use the derivative of the loss function with respect to data because the data is fixed. But the weights will be iterated on using the gradient information. So next, we are going to create a variable inside the value class that maintains the derivative of L with respect to that value. And we will call this variable grad. So there is a.data, and there is a self.grad. And initially, it will be 0. And remember that 0 basically means no effect. So at initialization, we're assuming that every value does not impact, does not affect the output. Because if the gradient is 0, that means that changing this variable is not changing the loss function. So by default, we assume that the gradient is 0. And then now that we have grad, and it's 0.0, we are going to be able to visualize it here after data. So here, grad is 0.4f. And this will be in.grad. And now we are going to be showing both the data and the grad initialized at 0. And we are just about getting ready to calculate the backpropagation. And of course, this grad, again, as I mentioned, is representing the derivative of the output, in this case L, with respect to this value. So this is the derivative of L with respect to f, with respect to d, and so on. So let's now fill in those gradients and actually do backpropagation manually. So let's start filling in these gradients and start all the way at the end, as I mentioned here. First, we are interested to fill in this gradient here. So what is the derivative of L with respect to L? In other words, if I change L by a tiny amount h, how much does L change? It changes by h. So it's proportional, and therefore, the derivative will be 1. We can, of course, measure these or estimate these numerical gradients numerically, just like we've seen before. So if I take this expression and I create a def lol function here and put this here. Now, the reason I'm creating a gating function lol here is because I don't want to pollute or mess up the global scope here. This is just kind of like a little staging area. And as you know, in Python, all of these will be local variables to this function. So I'm not changing any of the global scope here. So here, L1 will be L. And then copy pasting this expression, we're going to add a small amount h in, for example, A. And this would be measuring the derivative of L with respect to A. So here, this will be L2. And then we want to print that derivative. So print L2 minus L1, which is how much L changed, and then normalize it by h. So this is the rise over run. And we have to be careful because L is a value node, so we actually want its data. So these are floats, dividing by h. And this should print the derivative of L with respect to A, because A is the one that we bumped a little bit by h. So what is the derivative of L with respect to A? It's 6. And obviously, if we change L by h, then that would be here, effectively. This looks really awkward, but changing L by h, you see the derivative here is 1. That's kind of like the base case of what we are doing here. So basically, we can come up here, and we can manually set L.grad to 1. This is our manual backpropagation. L.grad is 1, and let's redraw. And we'll see that we filled in grad is 1 for L. We're now going to continue the backpropagation. So let's here look at the derivatives of L with respect to D and F. Let's do D first. So what we are interested in, if I create a Markdown on here, is we'd like to know, basically we have that L is D times F, and we'd like to know what is DL by DD. What is that? And if you know your calculus, L is D times F, so what is DL by DD? It would be F. And if you don't believe me, we can also just derive it, because the proof would be fairly straightforward. We go to the definition of the derivative, which is F of X plus H minus F of X divide H, as a limit. Limit of H goes to 0 of this kind of expression. So when we have L is D times F, then increasing D by H would give us the output of D plus H times F. That's basically F of X plus H, right? Minus D times F, and then divide H. And symbolically, expanding out here, we would have basically D times F plus H times F minus D times F divide H. And then you see how the DF minus DF cancels, so you're left with H times F divide H, which is F. So in the limit as H goes to 0 of derivative definition, we just get F in the case of D times F. So symmetrically, DL by DF will just be D. So what we have is that F dot grad, we see now, is just the value of D, which is 4. And we see that D dot grad is just the value of F. And so the value of F is negative 2. So we'll set those manually. Let me erase this Markdown node, and then let's redraw what we have. Okay, and let's just make sure that we're not doing anything wrong. Okay, and let's just make sure that these were correct. So we seem to think that DL by DD is negative 2, so let's double check. Let me erase this plus H from before. And now we want the derivative with respect to F. So let's just come here when I create F, and let's do a plus H here, and this should print the derivative of L with respect to F, so we expect to see 4. Yeah, and this is 4 up to floating point funkiness. And then DL by DD should be F, which is negative 2. Grad is negative 2. So if we again come here and we change D, D dot data plus equals H right here. So we expect, so we've added a little H, and then we see how L changed, and we expect to print negative 2. There we go. So we've numerically verified. What we're doing here is kind of like an inline gradient check. Gradient check is when we are deriving this backpropagation and getting the derivative with respect to all the intermediate results, and then numerical gradient is just estimating it using small step size. Now we're getting to the crux of backpropagation. So this will be the most important node to understand, because if you understand the gradient for this node, you understand all of backpropagation and all of training of neural nets, basically. So we need to derive DL by DC. In other words, the derivative of L with respect to C, because we've computed all these other gradients already. Now we're coming here, and we're continuing the backpropagation manually. So we want DL by DC to be equal to DL by DC. So we want DL by DC to be equal to DL by DC, and then we'll also derive DL by DE. Now here's the problem. How do we derive DL by DC? We actually know the derivative L with respect to D, so we know how L is sensitive to D. But how is L sensitive to C? So if we wiggle C, how does that impact L through D? So we know DL by DC, and we also here know how C impacts D. And so just very intuitively, if you know the impact that C is having on D and the impact that D is having on L, then you should be able to somehow put that information together to figure out how C impacts L. And indeed, this is what we can actually do. So in particular, we know, just concentrating on D first, let's look at how, what is the derivative basically of D with respect to C? So in other words, what is DD by DC? So here we know that D is C times C plus E. That's what we know. And now we're interested in DD by DC. If you just know your calculus again and you remember, then differentiating C plus E with respect to C, you know that that gives you 1.0. And we can also go back to the basics and derive this, because again, we can go to our f of x plus h minus f of x divided by h. That's the definition of a derivative as h goes to 0. And so here, focusing on C and its effect on D, we can basically do the f of x plus h will be C is incremented by h plus C. That's the first evaluation of our function, minus C plus E, and then divide h. And so what is this? Just expanding this out, this will be C plus h plus C minus C minus E, divide h, and then you see here how C minus C cancels, E minus E cancels, we're left with h over h, which is 1.0. And so by symmetry also, D by D E will be 1.0 as well. So basically the derivative of a sum expression is very simple, and this is the local derivative. So I call this the local derivative because we have the final output value all the way at the end of this graph, and we're now like a small node here, and this is a little plus node. And the little plus node doesn't know anything about the rest of the graph that it's embedded in. All it knows is that it did a plus. It took a C and an E, added them, and created D. And this plus node also knows the local influence of C on D, or rather the derivative of D with respect to C, and it also knows the derivative of D with respect to E. But that's not what we want, that's just the local derivative. What we actually want is D L by D C. And L is here just one step away, but in the general case, this little plus node could be embedded in like a massive graph. So again, we know how L impacts D, and now we know how C and E impact D. How do we put that information together to write D L by D C? And the answer of course is the chain rule in Calculus. And so I pulled up a chain rule here from Cupidia, and I'm gonna go through this very briefly. So chain rule, Wikipedia sometimes can be very confusing, and Calculus can be very confusing. Like this is the way I learned chain rule, and it was very confusing. Like what is happening? It's just complicated. So I like this expression much better. If a variable Z depends on a variable Y, which itself depends on a variable X, then Z depends on X as well, obviously through the intermediate variable Y. And in this case, the chain rule is expressed as, if you want D Z by D X, then you take the D Z by D Y, and you multiply it by D Y by D X. So the chain rule fundamentally is telling you how we chain these derivatives together correctly. So to differentiate through a function composition, we have to apply a multiplication of those derivatives. So that's really what chain rule is telling us. And there's a nice little intuitive explanation here, which I also think is kind of cute. The chain rule states that knowing the instantaneous rate of change of Z with respect to Y, and Y relative to X allows one to calculate the instantaneous rate of change of Z relative to X as a product of those two rates of change, simply the product of those two. So here's a good one. If a car travels twice as fast as a bicycle, and the bicycle is four times as fast as walking men, then the car travels two times four, eight times as fast as the man. And so this makes it very clear that the correct thing to do sort of is to multiply. So cars twice as fast as bicycle, and bicycle is four times as fast as man. So the car will be eight times as fast as the man. And so we can take these intermediate rates of change, if you will, and multiply them together. And that justifies the chain rule intuitively. So have a look at chain rule. But here, really what it means for us is, there's a very simple recipe for deriving what we want, which is DL by DC. And what we have so far is we know, want, and we know, what is the impact of D on L. So we know DL by DD, the derivative of L with respect to DD. We know that that's negative two. And now because of this local reasoning that we've done here, we know DD by DC. So how does C impact D? And in particular, this is a plus node. So the local derivative is simply 1.0. It's very simple. And so the chain rule tells us that DL by DC, going through this intermediate variable, will just be simply DL by DD times DD by DC. That's chain rule. So this is identical to what's happening here, except Z is our L, Y is our D, and X is our C. So we literally just have to multiply these. And because these local derivatives, like DD by DC, are just one, we basically just copy over DL by DD, because this is just times one. So because DL by DD is negative two, what is DL by DC? Well, it's the local gradient, 1.0, times DL by DD, which is negative two. So literally what a plus node does, you can look at it that way, is it literally just routes the gradient, because the plus nodes' local derivatives are just one. And so in the chain rule, one times DL by DD is just DL by DD. And so that derivative just gets routed to both C and to E in this case. So basically we have that E dot grad, or let's start with C, since that's the one we've looked at, is negative two times one, negative two. And in the same way, by symmetry, E dot grad will be negative two. That's the claim. So we can set those. We can redraw. And you see how we just assign negative two, negative two? So this backpropagating signal, which is carrying the information of like, what is the derivative of L with respect to all the intermediate nodes? We can imagine it almost like flowing backwards through the graph, and a plus node will simply distribute the derivative to all the leaf nodes, sorry, to all the children nodes. Children nodes of it. So this is the claim, and now let's verify it. So let me remove the plus H here from before. And now instead what we wanna do is we wanna increment C. So C dot data will be incremented by H. And when I run this, we expect to see negative two. Negative two. And then of course for E, so E dot data plus equals H, and we expect to see negative two. Simple. So those are the derivatives of these internal nodes. And now we're going to recurse our way backwards again. And we're again going to apply the chain rule. So here we go, our second application of chain rule, and we will apply it all the way through the graph. We just happen to only have one more node remaining. We have that DL by DE, as we have just calculated, is negative two. So we know that. So we know the derivative of L with respect to E. And now we want DL by DA, right? And the chain rule is telling us that that's just DL by DE, negative two, times the local gradient. So what is the local gradient? Basically DE by DA. We have to look at that. So I'm a little times node inside a massive graph, and I only know that I did A times B, and I produced an E. So now what is DE by DA and DE by DB? That's the only thing that I sort of know about. That's my local gradient. So because we have that E is A times B, we're asking what is DE by DA? And of course, we just did that here. We had a times node, so I'm not going to re-derive it. But if you want to differentiate this with respect to A, you'll just get B, right? The value of B, which in this case is negative 3.0. So basically we have that DL by DA. Well, let me just do it right here. We have that A dot grad, and we are applying chain rule here, is DL by DE, which we see here is negative two, is negative two times, what is DE by DA? It's the value of B, which is negative three. That's it. And then we have B dot grad, is again, DL by DE, which is negative two, just the same way, times what is DE by DB? Is the value of A, which is 2.0. That's the value of A. So these are our claimed derivatives. Let's redraw. And we see here that A dot grad turns out to be six, because that is negative two times negative three. And B dot grad is negative four times, sorry, is negative two times two, which is negative four. So those are our claims. Let's delete this and let's verify them. We have A here, A dot data plus equals H. So the claim is that A dot grad is six. Let's verify, six. And we have B dot data plus equals H. So nudging B by H and looking at what happens, we claim it's negative four. And indeed, it's negative four, plus minus, again, float oddness. And that's it. That was the manual backpropagation all the way from here to all the leaf nodes. And we've done it piece by piece. And really all we've done is, as you saw, we iterated through all the nodes one by one and locally applied the chain rule. We always know what is the derivative of L with respect to this little output. And then we look at how this output was produced. This output was produced through some operation, and we have the pointers to the children nodes of this operation. And so in this little operation, we know what the local derivatives are, and we just multiply them onto the derivative always. So we just go through and recursively multiply on the local derivatives. And that's what backpropagation is. It's just a recursive application of chain rule, backwards through the computation graph. Let's see this power in action just very briefly. What we're going to do is we're going to nudge our inputs to try to make L go up. So in particular, what we're doing is we want a.data. We're going to change it. And if we want L to go up, that means we just have to go in the direction of the gradient. So a should increase in the direction of gradient by like some small step amount. This is the step size. And we don't just want this for b, but also for b, also for c, also for f. Those are leaf nodes, which we usually have control over. And if we nudge in direction of the gradient, we expect a positive influence on L. So we expect L to go up positively. So it should become less negative. It should go up to say negative six or something like that. It's hard to tell exactly. And we'd have to rerun the forward pass. So let me just do that here. This would be the forward pass. F would be unchanged. This is effectively the forward pass. And now if we print L.data, we expect, because we nudged all the values, all the inputs in the direction of gradient, we expect a less negative L. We expect it to go up. So maybe it's negative six or so. Let's see what happens. Okay, negative seven. And this is basically one step of an optimization that will end up running. And really this gradient just give us some power because we know how to influence the final outcome. And this will be extremely useful for training NOLETS as well as CMC. So now I would like to do one more example of manual backpropagation using a bit more complex and useful example. We are going to backpropagate through a neuron. So we want to eventually build up neural networks. And in the simplest case, these are multilayer perceptrons as they're called. So this is a two layer neural net. And it's got these hidden layers made up of neurons. And these neurons are fully connected to each other. Now, biologically, neurons are very complicated devices, but we have very simple mathematical models of them. And so this is a very simple mathematical model of a neuron. You have some inputs, Xs, and then you have these synapses that have weights on them. So the Ws are weights. And then the synapse interacts with the input to this neuron multiplicatively. So what flows to the cell body of this neuron is W times X, but there's multiple inputs. So there's many W times Xs flowing to the cell body. The cell body then has also like some bias. So this is kind of like the innate sort of trigger happiness of this neuron. So this bias can make it a bit more trigger happy or a bit less trigger happy, regardless of the input. But basically we're taking all the W times X of all the inputs, adding the bias, and then we take it through an activation function. And this activation function is usually some kind of a squashing function, like a sigmoid or tanh or something like that. So as an example, we're going to use the tanh in this example. NumPy has a np.tanh, so we can call it on a range and we can plot it. This is the tanh function. And you see that the inputs as they come in get squashed on the Y coordinate here. So right at zero, we're going to get exactly zero. And then as you go more positive in the input, then you'll see that the function will only go up to one and then plateau out. And so if you pass in very positive inputs, we're going to cap it smoothly at one. And on the negative side, we're going to cap it smoothly to negative one. So that's tanh, and that's the squashing function or an activation function. And what comes out of this neuron is just the activation function applied to the dot product of the weights and the inputs. So let's write one out. I'm going to copy paste because I don't want to type too much. But okay, so here we have the inputs X1, X2. So this is a two-dimensional neuron. So two inputs are going to come in. These are thought of as the weights of this neuron, weights W1, W2. And these weights, again, are the synaptic strings for each input. And this is the bias of the neuron, B. And now what we want to do is, according to this model, we need to multiply X1 times W1 and X2 times W2. And then we need to add bias on top of it. And it gets a little messy here, but all we are trying to do is X1, W1 plus X2, W2 plus B. And these are multiplied here. Except I'm doing it in small steps so that we actually have pointers to all these intermediate nodes. So we have X1, W1 variable, X times X2, W2 variable, and I'm also labeling them. So N is now the cell body raw activation without the activation function for now. And this should be enough to basically plot it. So draw dot of N gives us X1 times W1, X2 times W2 being added. Then the bias gets added on top of this. And this N is this sum. So we're now going to take it through an activation function. And let's say we use the tanh so that we produce the output. So what we'd like to do here is we'd like to do the output, and I'll call it O, is N dot tanh. Okay, but we haven't yet written the tanh. Now, the reason that we need to implement another tanh function here is that tanh is a hyperbolic function, and we've only so far implemented a plus and a times, and you can't make a tanh out of just pluses and times. You also need exponentiation. So tanh is this kind of a formula here. You can use either one of these, and you see that there is exponentiation involved, which we have not implemented yet for our low value node here. So we're not gonna be able to produce tanh yet, and we have to go back up and implement something like it. Now, one option here is we could actually implement exponentiation, right? And we could return the exp of a value instead of a tanh of a value, because if we had exp, then we have everything else that we need, so because we know how to add, and we know how to multiply, so we'd be able to create tanh if we knew how to exp. But for the purposes of this example, I specifically wanted to show you that we don't necessarily need to have the most atomic pieces in this value object. We can actually create functions at arbitrary points of abstraction. They can be complicated functions, but they can be also very, very simple functions, like a plus, and it's totally up to us. The only thing that matters is that we know how to differentiate through any one function. So we take some inputs and we make an output. The only thing that matters, it can be arbitrarily complex function, as long as you know how to create the local derivative. If you know the local derivative of how the inputs impact the output, then that's all you need. So we're going to cluster up all of this expression, and we're not gonna break it down to its atomic pieces. We're just going to directly implement tanh. So let's do that. Def tanh, and then out will be a value of, and we need this expression here. So let me actually copy-paste. Let's grab n, which is a sub dot theta. And then this, I believe, is the tanh. Math dot exp of two, no, n minus one over two n plus one. Maybe I can call this x, just so that it matches exactly. Okay, and now this will be t, and children of this node, there's just one child, and I'm wrapping it in a tuple, so this is a tuple of one object, just self. And here, the name of this operation will be tanh. And we're going to return that. Okay. So now value should be equal to. should be implementing 10H. And now we can scroll all the way down here and we can actually do n.10H and that's going to return the 10H output of n. And now we should be able to draw a dot of O, not of n. So let's see how that worked. There we go. N went through 10H to produce this output. So now 10H is a sort of, our little micro grad supported node here as an operation. And as long as we know the derivative of 10H, then we'll be able to back propagate through it. Now let's see this 10H in action. Currently it's not squashing too much because the input to it is pretty low. So if the bias was increased to say eight, then we'll see that what's flowing into the 10H now is two and 10H is squashing it to 0.96. So we're already hitting the tail of this 10H and it will sort of smoothly go up to one and then plateau out over there. Okay, so now I'm going to do something slightly strange. I'm going to change this bias from eight to this number, 6.88, et cetera. And I'm going to do this for specific reasons because we're about to start back propagation and I want to make sure that our numbers come out nice. They're not like very crazy numbers. They're nice numbers that we can sort of understand in our head. Let me also add O's label. O is short for output here. So that's the O. Okay, so 0.88 flows into 10H, comes out 0.7, so on. So now we're going to do back propagation and we're going to fill in all the gradients. So what is the derivative O with respect to all the inputs here? And of course, in a typical neural network setting, what we really care about the most is the derivative of these neurons on the weights specifically, the W2 and W1, because those are the weights that we're going to be changing part of the optimization. And the other thing that we have to remember is here, we have only a single neuron, but in the neural net, you typically have many neurons and they're connected. So this is only like one small neuron, a piece of a much bigger puzzle, and eventually there's a loss function that sort of measures the accuracy of the neural net and we're back propagating with respect to that accuracy and trying to increase it. So let's start off back propagation here in the end. What is the derivative of O with respect to O? The base case sort of we know always is that the gradient is just 1.0. So let me fill it in and then let me split out the drawing function here. Drawing function here and then here, cell clear this output here. Okay. So now when we draw O, we'll see that O that grad is 1. So now we're going to back propagate through the 10H. So to back propagate through 10H, we need to know the local derivative of 10H. So if we have that O is 10H of N, then what is DO by DN? Now, what you could do is you could come here and you could take this expression and you could do your calculus derivative taking, and that would work, but we can also just scroll down Wikipedia here into a section that hopefully tells us that derivative D by DX of 10H of X is any of these. I like this one, 1 minus 10H square of X. So this is 1 minus 10H of X squared. So basically what this is saying is that DO by DN is 1 minus 10H of N squared. And we already have 10H of N, it's just O. So it's 1 minus O squared. So O is the output here. So the output is this number, O dot data is this number. And then what this is saying is that DO by DN is 1 minus this squared. So 1 minus O dot data squared is 0.5 conveniently. So the local derivative of this 10H operation here is 0.5. And so that would be DO by DN. So we can fill in that N dot grad is 0.5. We'll just fill it in. So this is exactly 0.5, 1.5. So now we're going to continue the backpropagation. This is 0.5 and this is a plus node. So how is backprop going to, what is backprop going to do here? And if you remember our previous example, a plus is just a distributor of gradient. So this gradient will simply flow to both of these equally. And that's because the local derivative of this operation is one for every one of its nodes. So one times 0.5 is 0.5. So therefore we know that this node here, which we called this, it's grad, it's just 0.5. And we know that B dot grad is also 0.5. So let's set those and let's draw. So those are 0.5. Continuing, we have another plus. 0.5 again, we'll just distribute. So 0.5 will flow to both of these. So we can set theirs. X to W2 as well, dot grad is 0.5. And let's redraw. Pluses are my favorite operations to backpropagate through because it's very simple. So now what's flowing into these expressions is 0.5. And so really again, keep in mind what the derivative is telling us at every point in time along here. This is saying that if we want the output of this neuron to increase, then the influence on these expressions is positive on the output. Both of them are positive contribution to the output. So now backpropagating to X2 and W2 first. This is a times node. So we know that the local derivative is the other term. So if we want to calculate X2 dot grad, then can you think through what it's going to be? So X2 dot grad will be W2 dot data times this X2 W2 dot grad, right? And W2 dot grad will be X2 dot data times X2 W2 dot grad, right? So that's the little local piece of chain rule. Let's set them and let's redraw. So here we see that the gradient on our weight two is zero because X2's data was zero, right? But X2 will have the gradient 0.5 because data here was one. And so what's interesting here, right, is because the input X2 was zero, then because of the way the times works, of course, this gradient will be zero. And to think about intuitively why that is, derivative always tells us the influence of this on the final output. If I wiggle W2, how is the output changing? It's not changing because we're multiplying by zero. So because it's not changing, there is no derivative and zero is the correct answer because we're squashing that zero. And let's do it here. 0.5 should come here and flow through this times. And so we'll have that X1.grad is, can you think through a little bit what this should be? Local derivative of times with respect to X1 is going to be W1. So W1's data times X1W1.grad. And W1.grad will be X1.data times X1W1.grad. X1W2, W1.grad. Let's see what those came out to be. So this is 0.5, so this would be negative 1.5 and this would be one. And we've back-propagated through this expression. These are the actual final derivatives. So if we want this neuron's output to increase, we know that what's necessary is that, W2, we have no gradient. W2 doesn't actually matter to this neuron right now, but this neuron, this weight should go up. So if this weight goes up, then this neuron's output would have gone up and proportionally because the gradient is one. Okay, so doing the back-propagation manually is obviously ridiculous. So we are now going to put an end to this suffering and we're going to see how we can implement the backward pass a bit more automatically. We're not going to be doing all of it manually out here. It's now pretty obvious to us by example, how these pluses and times are back-propagating gradients. So let's go up to the value object and we're going to start co-define what we've seen in the examples below. So we're going to do this by storing a special self.backward, and underscore backward. And this will be a function, which is going to do that little piece of chain rule. At each little node that took inputs and produced output, we're going to store how we are going to chain the outputs gradient into the inputs gradients. So by default, this will be a function that doesn't do anything. And you can also see that here in the value in micro grad. So we have this backward function, by default doesn't do anything. This is an empty function. And that would be sort of the case, for example, for a leaf node. For a leaf node, there's nothing to do. But now if, when we're creating these out values, these out values are an addition of self and other. And so we'll want to self set outs backward to be the function that propagates the gradient. So let's define what should happen. And we're going to store it in a closure. Let's define what should happen when we call outs grad. For addition, our job is to take outs grad and propagate it into self grad and other dot grad. So basically we want to self dot grad to something. And we want to set others dot grad to something. And the way we saw below how chain rule works, we want to take the local derivative times the sort of global derivative, I should call it, which is the derivative of the final output of the expression with respect to outs data. With respect to out. So the local derivative of self in an addition is 1.0. So it's just 1.0 times outs grad. That's the chain rule. And others dot grad will be 1.0 times out grad. And what you, basically what you're saying is that outs grad will simply be copied onto self grad and others grad as we saw happens for an addition operation. So we're going to later call this function to propagate the gradient having done an addition. Let's now do multiplication. We're going to also define a dot backward. And we're going to set its backward to be backward. Backward to being backward. And we want to chain out grad into self dot grad and others dot grad. And this will be a little piece of chain rule for multiplication. So we'll have, so what should this be? Can you think through? So what is the local derivative here? The local derivative was others dot data and then, oops, others dot data and then times out dot grad. That's chain rule. And here we have self dot data times out dot grad. That's what we've been doing. And finally here for 10H, that's backward. And then we want to set outs backward to be just backward. And here we need to back propagate. We have out dot grad and we want to chain it into self dot grad. And self dot grad will be the local derivative of this operation that we've done here, which is 10H. And so we saw that the local gradient is one minus the 10H of X squared, which here is T. That's the local derivative because that's T is the output of this 10H. So one minus T squared is the local derivative and then gradient has to be multiplied because of the chain rule. So outs grad is chained through the local gradient into self dot grad. And that should be basically it. So we're going to redefine our value node. We're going to swing all the way down here. And we're going to redefine our expression. Make sure that all the grads are zero. Okay, but now we don't have to do this manually anymore. We are going to basically be calling the dot backward in the right order. So first we want to call outs dot backward. So O was the outcome of 10H, right? So calling those outs backward will be this function. This is what it will do. Now we have to be careful because there's a times out dot grad and out dot grad remember is initialized to zero. So here we see grad zero. So as a base case, we need to set O's dot grad to 1.0 to initialize this with one. And then once this is one, we can call O dot backward. And what that should do is it should propagate this grad through 10H. So the local derivative times the global derivative which is initialized at one. So this should, so I thought about redoing it, but I figured I should just leave the error in here because it's pretty funny. Why is not an object not callable? It's because I screwed up. We're trying to save these functions. So this is correct. This here, we don't want to call the function because that returns none. These functions return none. We just want to store the function. So let me redefine the value object. And then we're going to come back in, redefine the expression, draw dot. Everything is great. O dot grad is one. O dot grad is one. And now this should work of course. Okay, so O dot backward should, this grad should now be 0.5 if we redraw. And if everything went correctly, 0.5, yay. Okay, so now we need to call ns dot grad. Ns dot backward, sorry. Ns backward. So that seems to have worked. So ns dot backward routed the gradient to both of these. So this is looking great. Now we could of course call b dot grad, b dot backward, sorry. What's going to happen? Well, b doesn't have a backward. b is backward because b is a leaf node. b is backward is by initialization the empty function. So nothing would happen, but we can call it on it. But when we call this one, this backward, then we expect this 0.5 to get further routed, right? So there we go, 0.5, 0.5. And then finally, we want to call it here on x2w2 and on x1w1. Let's do both of those. And there we go. So we get 0.5, negative 1.5 and one, exactly as we did before. But now we've done it through calling dot backward sort of manually. So we have one last piece to get rid of, which is us calling underscore backward manually. So let's think through what we are actually doing. We've laid out a mathematical expression and now we're trying to go backwards through that expression. So going backwards through the expression just means that we never want to call a dot backward for any node before we've done sort of everything after it. So we have to do everything after it before we're ever gonna call dot backward on any one node. We have to get all of its full dependencies. Everything that it depends on has to propagate to it before we can continue back propagation. So this ordering of graphs can be achieved using something called topological sort. So topological sort is basically a laying out of a graph such that all the edges go only from left to right, basically. So here we have a graph, it's a direct acyclic graph, a DAG, and this is two different topological orders of it, I believe, where basically you'll see that it's a laying out of the nodes such that all the edges go only one way from left to right. And implementing topological sort, you can look in Wikipedia and so on. I'm not going to go through it in detail. But basically, this is what builds a topological graph. We maintain a set of visited nodes, and then we are going through, starting at some root node, which for us is O, that's where we want to start the topological sort. And starting at O, we go through all of its children, and we need to lay them out from left to right. And basically, this starts at O. If it's not visited, then it marks it as visited, and then it iterates through all of its children and calls build topological on them. And then after it's gone through all the children, it adds itself. So basically, this node that we're going to call it on, like say O, is only going to add itself to the topo list after all of the children have been processed. And that's how this function is guaranteeing that you're only going to be in the list once all your children are in the list, and that's the invariant that is being maintained. So if we build topo on O, and then inspect this list, we're going to see that it ordered our value objects. And the last one is the value of 0.706. 7 which is the output. So this is O and then this is N and then all the other nodes get laid out before it. So that builds the topological graph and really what we're doing now is we're just calling dot underscore backward on all of the nodes in a topological order. So if we just reset the gradients, they're all zero. What did we do? We started by setting O dot grad to be 1. That's the base case. Then we built a topological order and then we went for node in reversed of topo. Now in the reverse order because this list goes from, you know, we need to go through it in reversed order. So starting at O, node dot backward. And this should be it. There we go. Those are the correct derivatives. Finally we are going to hide this functionality. So I'm going to copy this and we're going to hide it inside the value class because we don't want to have all that code lying around. So instead of an underscore backward, we're now going to define an actual backward. So that's backward without the underscore. And that's going to do all the stuff that we just derived. So let me just clean this up a little bit. So we're first going to build a topological graph starting at self. So build topo of self will populate the topological order into the topo list which is a local variable. Then we set self dot grads to be 1. And then for each node in the reversed list, so starting at us and going to all the children, underscore backward. And that should be it. So save. Come down here. Redefine. Okay all the grads are 0. And now what we can do is O dot backward without the underscore. And there we go. And that's backpropagation. At least for one neuron. Now we shouldn't be too happy with ourselves actually because we have a bad bug. And we have not surfaced the bug because of some specific conditions that we have to think about right now. So here's the simplest case that shows the bug. Say I create a single node A and then I create a B that is A plus A. And then I call backward. So what's going to happen is A is 3 and then B is A plus A. So there's two arrows on top of each other here. Then we can see that B is of course the forward pass works. B is just A plus A which is 6. But the gradient here is not actually correct. Then we calculate it automatically. And that's because of course just doing calculus in your head the derivative of B with respect to A should be 2. 1 plus 1. It's not 1. Intuitively what's happening here right so B is the result of A plus A and then we call backward on it. So let's go up and see what that does. B is a result of addition so out is B. And then when we call backward what happened is self.grad was set to 1 and then other.grad was set to 1. But because we're doing A plus A, self and other are actually the exact same object. So we are overriding the gradient. We are setting it to 1 and then we are setting it again to 1. And that's why it stays at 1. So that's a problem. There's another way to see this in a little bit more complicated expression. So here we have A and B. And then D will be the multiplication of the two and E will be the addition of the two. And then we multiply E times D to get F and then we call F dot backward. And these gradients if you check will be incorrect. So fundamentally what's happening here again is basically we're going to see an issue anytime we use a variable more than once. Until now in these expressions above every variable is used exactly once so we didn't see the issue. But here if a variable is used more than once what's going to happen during backward pass? We're back propagating from F to E to D so far so good but now E calls it backward and it deposits its gradients to A and B but then we come back to D and call backward and it overwrites those gradients at A and B. So that's obviously a problem. And the solution here if you look at the multivariate case of the chain rule and its generalization there, the solution there is basically that we have to accumulate these gradients. These gradients add. And so instead of setting those gradients we can simply do plus equals. We need to accumulate those gradients. Plus equals, plus equals, plus equals, plus equals. And this will be okay remember because we are initializing them at zero. So they start at zero and then any contribution that flows backwards will simply add. So now if we redefine this one because the plus equals this now works. Because A.grad started at zero and we called B.backward we deposit one and then we deposit one again and now this is two which is correct. And here this will also work and we'll get correct gradients because when we call E.backward we will deposit the gradients from this branch and then we get to back to D.backward it will deposit its own gradients and then those gradients simply add on top of each other. And so we just accumulate those gradients and that fixes the issue. Okay now before we move on let me actually do a bit of cleanup here and delete some of these some of this intermediate work. So I'm not gonna need any of this now that we've derived all of it. We are going to keep this because I want to come back to it. Delete the 10H, delete our modicum example, delete the step, delete this, keep the code that draws, and then delete this example and leave behind only the definition of value. And now let's come back to this non-linearity here that we implemented the 10H. Now I told you that we could have broken down 10H into its explicit atoms in terms of other expressions if we had the exp function. So if you remember 10H is defined like this and we chose to develop 10H as a single function and we can do that because we know it's derivative and we can back propagate through it. But we can also break down 10H into an explicit as a function of exp. And I would like to do that now because I want to prove to you that you get all the same results and all the same gradients but also because it forces us to implement a few more expressions. It forces us to do exponentiation, addition, subtraction, division, and things like that. And I think it's a good exercise to go through a few more of these. Okay so let's scroll up to the definition of value. And here one thing that we currently can't do is we can't do like a value of say 2.0 but we can't do you know here for example we want to add a constant 1 and we can't do something like this. And we can't do it because it says int object has no attribute data. That's because a plus 1 comes right here to add and then other is the integer 1. And then here Python is trying to access 1.data and that's not a thing. That's because basically 1 is not a value object and we only have addition for value objects. So as a matter of convenience so that we can create expressions like this and make them make sense we can simply do something like this. Basically we let other alone if other is an instance of value but if it's not an instance of value we're going to assume that it's a number like an integer or float and we're going to simply wrap it in value and then other will just become value of other and then other will have a data attribute and this should work. So if I just say this, redefine value, then this should work. There we go. Okay and now let's do the exact same thing for multiply because we can't do something like this again for the exact same reason. So we just have to go to mul and if other is not a value then let's wrap it in value. Let's redefine value and now this works. Now here's a kind of unfortunate and not obvious part. A times 2 works, we saw that, but 2 times a is that gonna work? You'd expect it to right? But actually it will not and the reason it won't is because Python doesn't know like when when you do a times 2 basically so a times 2 Python will go and it will basically do something like a dot mul of 2. That's basically what it will call but to it 2 times a is the same as 2 dot mul of a and it doesn't 2 can't multiply value and so it's really confused about that. So instead what happens is in Python the way this works is you are free to define something called the R mul and R mul is kind of like a fallback. So if Python can't do 2 times a it will check if by any chance a knows how to multiply 2 and that will be called into R mul. So because Python can't do 2 times a it will check is there an R mul in value and because there is it will now call that and what we'll do here is we will swap the order of the operands. So basically 2 times a will redirect to R mul and R mul will basically call a times 2 and that's how that will work. So redefining that with R mul, 2 times a becomes 4. Okay now looking at the other elements that we still need we need to know how to exponentiate and how to divide. So let's first do the exponentiation part. We're going to introduce a single function exp here and exp is going to mirror tanh in the sense that it's a simple single function that transform a single scalar value and outputs a single scalar value. So we pop out the Python number, we use math.exp to exponentiate it, create a new value object, everything that we've seen before. The tricky part of course is how do you back propagate through e to the x? And so here you can potentially pause the video and think about what should go here. Okay so basically we need to know what is the local derivative of e to the x. So d by dx of e to the x is famously just e to the x and we've already just calculated e to the x and it's inside out.data. So we can do out.data times and out.grad, that's the chain rule. So we're just chaining on to the current running grad and this is what the expression looks like. It looks a little confusing but this is what it is and that's the exponentiation. So redefining we should now be able to call a.exp and hopefully the backward pass works as well. Okay and the last thing we'd like to do of course is we'd like to be able to divide. Now I actually will implement something slightly more powerful than division because division is just a special case of something a bit more powerful. So in particular just by rearranging if we have some kind of a b equals value of 4.0 here we'd like to basically be able to do a divide b and we'd like this to be able to give us 0.5. Now division actually can be reshuffled as follows. If we have a divide b that's actually the same as a multiplying 1 over b and that's the same as a multiplying b to the power of negative 1. And so what I'd like to do instead is I basically like to implement the operation of x to the k for some constant k. So it's an integer or a float and we would like to be able to differentiate this and then as a special case negative 1 will be division. And so I'm doing that just because it's more general and yeah you might as well do it that way. So basically what I'm saying is we can redefine division which we will put here somewhere. Yeah we can put it here somewhere. What I'm saying is that we can redefine division so self divide other this can actually be rewritten as self times other to the power of negative 1. And now value raised to the power of negative 1 we have now defined that. So we need to implement the pow function. Where am I gonna put the pow function? Maybe here somewhere. This is the skeleton for it. So this function will be called when we try to raise a value to some power and other will be that power. Now I'd like to make sure that other is only an int or a float. Usually other is some kind of a different value object but here other will be forced to be an int or a float. Otherwise the math won't work for what we're trying to achieve in the specific case. That would be a different derivative expression if we wanted other to be a value. So here we create the up the value which is just you know this data raised to the power of other and other here could be for example negative 1. That's what we are hoping to achieve. And then this is the backward stub and this is the fun part which is what is the chain rule expression here for back for back propagating through the power function where the power is to the power of some kind of a constant. So this is the exercise and maybe pause the video here and see if you can figure it out yourself as to what we should put here. Okay so you can actually go here and look at derivative rules as an example and we see lots of derivative rules that you can hopefully know from calculus. In particular what we're looking for is the power rule because that's telling us that if we're trying to take d by dx of x to the n which is what we're doing here then that is just n times x to the n minus 1 right. Okay so that's telling us about the local derivative of this power operation. So all we want here basically n is now other and self.data is x and so this now becomes other which is n times self.data which is now a Python int or a float. It's not a value object we're accessing the data attribute raised to the power of other minus 1 or n minus 1. I can put brackets around this but this doesn't matter because power takes precedence over multiply in Python so that would have been okay and that's the local derivative only but now we have to chain it and we chain it just simply by multiplying by on top grad that's chain rule and this should technically work and we're gonna find out soon but now if we do this this should now work and we get 0.5 so the forward pass works but does the backward pass work and I realized that we actually also have to know how to subtract so right now a minus b will not work to make it work we need one more piece of code here and basically this is the subtraction and the way we're going to implement subtraction is we're gonna implement it by addition of a negation and then to implement negation we're gonna multiply by negative 1 so just again using the stuff we've already built and just expressing it in terms of what we have and a minus b is now working okay so now let's scroll again to this expression here for this neuron and let's just compute the backward pass here once we've defined O and let's draw it so here's the gradients for all these leaf nodes for this two-dimensional neuron that has a tanh that we've seen before so now what I'd like to do is I'd like to break up this tanh into this expression here so let me copy paste this here and now instead of we'll preserve the label and we will change how we define O so in particular we're going to implement this formula here so we need e to the 2x minus 1 over e to the x plus 1 so e to the 2x we need to take 2 times n and we need to exponentiate it that's e to the 2x and then because we're using it twice let's create an intermediate variable e and then define O as e minus 1 over e plus 1 and that should be it and then we should be able to draw dot of O so now before I run this what do we expect to see number one we're expecting to see a much longer graph here because we've broken up tanh into a bunch of other operations but those operations are mathematically equivalent and so what we're expecting to see is number one the same result here so the forward pass works and number two because of that mathematical equivalence we expect to see the same backward pass and the same gradients on these leaf nodes so these gradients should be identical so let's run this so number one let's verify that instead of a single tanh node we have now exp and we have plus we have times negative 1 this is the division and we end up with the same forward pass here and then the gradients we have to be careful because they're in slightly different order potentially the gradients for w2 x2 should be 0 and 0.5 w2 and x2 are 0 and 0.5 and w1 x1 are 1 and negative 1.5 1 and negative 1.5 so that means that both our forward passes and backward passes were correct because this turned out to be equivalent to tanh before and so the reason I wanted to go through this exercise is number one we got to practice a few more operations and writing more backwards passes and number two I wanted to illustrate the point that the the level at which you implement your operations is totally up to you you can implement backward passes for tiny expressions like a single individual plus or a single times or you can implement them for say tanh which is a kind of a potentially you can see it as a composite operation because it's made up of all these more atomic operations but really all of this is kind of like a fake concept all that matters is we have some kind of inputs and some kind of an output and this output is a function of the inputs in some way and as long as you can do forward pass and the backward pass of that little operation it doesn't matter what that operation is and how composite it is if you can write the local gradients you can chain the gradient and you can continue back propagation so the design of what those functions are is completely up to you so now I would like to show you how you can do the exact same thing but using a modern deep neural network library like for example PyTorch which I've roughly modeled micrograd by and so PyTorch is something you would use in production and I'll show you how you can do the exact same thing but in PyTorch API so I'm just going to copy paste it in and walk you through it a little bit this is what it looks like so we're going to import PyTorch and then we need to define these values objects like we have here. Now, Micrograd is a scalar-valued engine, so we only have scalar values like 2.0. But in PyTorch, everything is based around tensors. And like I mentioned, tensors are just n-dimensional arrays of scalars. So that's why things get a little bit more complicated here. I just need a scalar-valued tensor, a tensor with just a single element. But by default, when you work with PyTorch, you would use more complicated tensors like this. So if I import PyTorch, then I can create tensors like this. And this tensor, for example, is a 2x3 array of scalars in a single compact representation. So we can check its shape. We see that it's a 2x3 array, and so on. So this is usually what you would work with in the actual libraries. So here, I'm creating a tensor that has only a single element, 2.0. And then I'm casting it to be double, because Python is by default using double precision for its floating point numbers. So I'd like everything to be identical. By default, the data type of these tensors will be float32. So it's only using a single precision float. So I'm casting it to double, so that we have float64, just like in Python. So I'm casting to double, and then we get something similar to value of 2. The next thing I have to do is, because these are leaf nodes, by default, PyTorch assumes that they do not require gradients. So I need to explicitly say that all of these nodes require gradients. Okay, so this is going to construct scalar-valued one-element tensors. Make sure that PyTorch knows that they require gradients. Now, by default, these are set to false, by the way, because of efficiency reasons, because usually you would not want gradients for leaf nodes, like the inputs to the network. And this is just trying to be efficient in the most common cases. So once we've defined all of our values in PyTorch land, we can perform arithmetic, just like we can here in micrograd land. So this would just work. And then there's a torch.tanh also. And what we get back is a tensor again, and we can, just like in micrograd, it's got a data attribute, and it's got grad attributes. So these tensor objects, just like in micrograd, have a dot data and a dot grad. And the only difference here is that we need to call a dot item, because otherwise PyTorch dot item basically takes a single tensor of one element, and it just returns that element, stripping out the tensor. So let me just run this, and hopefully we are going to get, this is going to print the forward pass, which is 0.707. And this will be the gradients, which hopefully are 0.50, negative 1.5, and 1. So if we just run this, there we go, 0.7, so the forward pass agrees, and then 0.50, negative 1.5, and 1. So PyTorch agrees with us. And just to show you here, basically, O, here's a tensor with a single element, and it's a double, and we can call dot item on it to just get the single number out. So that's what item does. And O is a tensor object, like I mentioned, and it's got a backward function, just like we've implemented. And then all of these also have a dot grad, so like x2, for example, has a grad, and it's a tensor. And we can pop out the individual number with dot item. So basically, Torch can do what we did in micrograd as a special case when your tensors are all single element tensors. But the big deal with PyTorch is that everything is significantly more efficient because we are working with these tensor objects, and we can do lots of operations in parallel on all of these tensors. But otherwise, what we've built very much agrees with the API of PyTorch. Okay, so now that we have some machinery to build out pretty complicated mathematical expressions, we can also start building up neural nets. And as I mentioned, neural nets are just a specific class of mathematical expressions. So we're going to start building out a neural net piece by piece, and eventually we'll build out a two-layer, multi-layer, layer perceptron, as it's called, and I'll show you exactly what that means. Let's start with a single individual neuron. We've implemented one here, but here I'm going to implement one that also subscribes to the PyTorch API in how it detects the number of PyTorch API in how it designs its neural network modules. So just like we saw that we can match the API of PyTorch on the autograd side, we're going to try to do that on the neural network modules. So here's class neuron, and just for the sake of efficiency, I'm going to copy-paste some sections that are relatively straightforward. So the constructor will take number of inputs to this neuron, which is how many inputs come to a neuron. So this one, for example, has three inputs. And then it's going to create a weight that is some random number between negative one and one for every one of those inputs, and a bias that controls the overall trigger happiness of this neuron. And then we're going to implement a def underscore underscore call of self and x, some input x. And really what we want to do here is w times x plus b, where w times x here is a dot product specifically. Now if you haven't seen call, let me just return 0.0 here for now. The way this works now is we can have an x which is say like 2.0, 3.0, then we can initialize a neuron that is two-dimensional, because these are two numbers, and then we can feed those two numbers into that neuron to get an output. And so when you use this notation, n of x, Python will use call. So currently call just returns 0.0. Now we'd like to actually do the forward pass of this neuron instead. So what we're going to do here first is we need to basically multiply all of the elements of w with all of the elements of x pairwise. We need to multiply them. So the first thing we're going to do is we're going to zip up salta w and x. And in Python, zip takes two iterators and it creates a new iterator that iterates over the tuples of their corresponding entries. So for example, just to show you, we can print this list and still return 0.0 here. So we see that these w's are paired up with the x's, w with x. And now what we want to do is for wi xi in, we want to multiply wi times xi, and then we want to sum all of that together to come up with an activation and add also salta b on top. So that's the raw activation. And then of course we need to pass that through a non-linearity. So what we're going to be returning is act.10h. And here's out. So now we see that we are getting some outputs and we get a different output from a neuron each time because we are initializing different weights and biases. And then to be a bit more efficient here actually, sum, by the way, takes a second optional parameter, which is the start. And by default, the start is 0. So these elements of this sum will be added on top of 0 to begin with. But actually we can just start with salta b and then we just have an expression like this. And then the generator expression here must be parenthesized in Python. There we go. Yep, so now we can forward a single neuron. Next up, we're going to define a layer of neurons. So here we have a schematic for a MLP. So we see that these MLPs, each layer, this is one layer, has actually a number of neurons, and they're not connected to each other, but all of them are fully connected to the input. So what is a layer of neurons? It's just a set of neurons evaluated independently. So in the interest of time, I'm going to do something fairly straightforward here. It's literally a layer. It's just a list of neurons. And then how many neurons do we have? We take that as an input argument here. How many neurons do you want in your layer? Number of outputs in this layer. And so we just initialize completely independent neurons with this given dimensionality. And when we call on it, we just independently evaluate them. So now instead of a neuron, we can make a layer of neurons. They are two-dimensional neurons, and let's have three of them. And now we see that we have three independent evaluations of three different neurons. Okay, and finally, let's complete this picture and define an entire multi-layer perceptron, or MLP. And as we can see here, in an MLP, these layers just feed into each other sequentially. So let's come here, and I'm just going to copy the code here in the interest of time. So an MLP is very similar. We're taking the number of inputs as before, but now instead of taking a single nout, which is number of neurons in a single layer, we're going to take a list of nouts, and this list defines the sizes of all the layers that we want in our MLP. So here we just put them all together and then iterate over consecutive pairs of these sizes and create layer objects for them. And then in the call function, we are just calling them sequentially. So that's an MLP, really. And let's actually re-implement this picture. So we want three input neurons and then two layers of four and an output unit. So we want a three-dimensional input. Say this is an example input. We want three inputs into two layers of four and one output. And this, of course, is an MLP. And there we go. That's a forward pass of an MLP. To make this a little bit nicer, you see how we have just a single element, but it's wrapped in a list because layer always returns lists? For convenience, return outs at zero if len outs is exactly a single element, else return full list. And this will allow us to just get a single value out at the last layer that only has a single neuron. And finally, we should be able to draw dot of n of x. And as you might imagine, these expressions are now getting relatively involved. So this is an entire MLP that we're defining now, all the way until a single output. And so obviously, you would never differentiate on pen and paper these expressions. But with micrograd, we will be able to back propagate all the way through this and back propagate into these weights of all these neurons. So let's see how that works. Okay, so let's create ourselves a very simple example dataset here. So this dataset has four examples. And so we have four possible inputs into the neural net. And we have four desired targets. So we'd like the neural net to assign or output 1.0 when it's fed this example, negative one when it's fed these examples, and one when it's fed this example. So it's a very simple binary classifier neural net basically that we would like here. Now let's think what the neural net currently thinks about these four examples. We can just get their predictions. Basically, we can just call n of x for x and xs. And then we can print. So these are the outputs of the neural net on those four examples. So the first one is 0.91, but we'd like it to be 1. So we should push this one higher. This one we want to be higher. This one says 0.88, and we want this to be negative 1. This is 0.88, we want it to be negative 1. And this one is 0.88, we want it to be 1. So how do we make the neural net and how do we tune the weights to better predict the desired targets? And the trick used in deep learning to achieve this is to calculate a single number that somehow measures the total performance of your neural net. And we call this single number the loss. So the loss first is a single number that we're going to define that basically measures how well the neural net is performing. Right now we have the intuitive sense that it's not performing very well because we're not very much close to this. So the loss will be high, and we'll want to minimize the loss. So in particular, in this case, what we're going to do is we're going to implement the mean squared error loss. So what this is doing is we're going to basically iterate for y ground truth and y output in zip of y's n-y thread. So we're going to pair up the ground truths with the predictions, and the zip iterates over tuples of them. And for each y ground truth and y output, we're going to subtract them and square them. So let's first see what these losses are. These are individual loss components. And so basically for each one of the four, we are taking the prediction and the ground truth, we are subtracting them and squaring them. So because this one is so close to its target, 0.91 is almost 1, subtracting them gives a very small number. So here we would get like a negative 0.1. And then squaring it just makes sure that regardless of whether we are more negative or more positive, we always get a positive number. Instead of squaring, we could also take, for example, the absolute value. We need to discard the sign. And so you see that the expression is arranged so that you only get 0 exactly when y out is equal to y ground truth. When those two are equal, so your prediction is exactly the target, you are going to get 0. And if your prediction is not the target, you are going to get some other number. So here, for example, we are way off. And so that's why the loss is quite high. And the more off we are, the greater the loss will be. So we don't want high loss, we want low loss. And so the final loss here will be just the sum of all of these numbers. So you see that this should be 0 roughly plus 0 roughly, but plus 7. So loss should be about 7 here. And now we want to minimize the loss. We want the loss to be low, because if loss is low, then every one of the predictions is equal to its target. So the loss, the lowest it can be is 0, and the greater it is, the worse off the neural net is predicting. So now, of course, if we do loss.backward, something magical happened when I hit Enter. And the magical thing, of course, that happened is that we can look at n.layers.neuron, n.layers at, say, like the first layer, .neurons at 0, because remember that MLP has the layers, which is a list, and each layer has neurons, which is a list, and that gives us an individual neuron. And then it's got some weights. And so we can, for example, look at the weights at 0. Oops, it's not called weights. It's called w. And that's a value. But now this value also has a grad because of the backward pass. And so we see that because this gradient here on this particular weight of this particular neuron of this particular layer is 0, the gradient is 0. So we can look at the gradient of this particular neuron of this particular layer is negative. We see that its influence on the loss is also negative. So slightly increasing this particular weight of this neuron of this layer would make the loss go down. And we actually have this information for every single one of our neurons and all of their parameters. Actually, it's worth looking at also the draw.loss, by the way. So previously, we looked at the draw. of a single neuron forward pass, and that was the expression. But what is this expression? We actually forwarded every one of those four examples, and then we have the loss on top of them with the mean squared error. And so this is a really massive graph because this graph that we've built up now, oh my gosh, this graph that we've built up now, which is kind of excessive, it's excessive because it has four forward passes of a neural net for every one of the examples. And then it has the loss on top, and it ends with the value of the loss, which was 7.12. And this loss will now back propagate through all the four forward passes, all the way through just every single intermediate value of the neural net, all the way back to, of course, the parameters of the weights, which are the input. So these weight parameters here are inputs to this neural net. And these numbers here, these scalars, are inputs to the neural net. So if we went around here, we will probably find some of these examples, this 1.0, potentially maybe this 1.0, or some of the others. And you'll see that they all have gradients as well. The thing is, these gradients on the input data are not that useful to us. And that's because the input data seems to be not changeable. It's a given to the problem. And so it's a fixed input. We're not going to be changing it or messing with it, even though we do have gradients for it. But some of these gradients here will be for the neural network parameters. The Ws and the Bs. And those, of course, we want to change. OK, so now we're going to want some convenience code to gather up all of the parameters of the neural net so that we can operate on all of them simultaneously. And every one of them, we will nudge a tiny amount based on the gradient information. So let's collect the parameters of the neural net all in one array. So let's create a parameters of self that just returns self.w, which is a list, concatenated with a list of self.b. So this will just return a list. List plus list just gives you a list. So that's parameters of neuron. And I'm calling it this way because also PyTorch has parameters on every single NN module. And it does exactly what we're doing here. It just returns the parameter tensors for us as the parameter scalars. Now, layer is also a module, so it will have parameters, self. And basically what we want to do here is something like this, like params is here, and then for neuron in self.neurons, we want to get neuron.parameters, and we want to params.extend. So these are the parameters of this neuron. And then we want to put them on top of params, so params.extend of p's. And then we want to return params. So this is way too much code. So actually, there's a way to simplify this, which is return p for neuron in self.neurons for p in neuron.parameters. So it's a single list comprehension. In Python, you can sort of nest them like this, and you can then create the desired array. So these are identical. We can take this And then let's do the same here. Def parameters self and return a parameter for layer in self.layers for p in layer.parameters And that should be good Now let me pop out this so we don't re-initialize our network because we need to re-initialize our Okay, so unfortunately we will have to probably re-initialize the network because we just had functionality Because this class of course we I want to get all the end up parameters, but that's not going to work because this is the old class Okay, so unfortunately we do have to re-initialize the network which will change some of the numbers But let me do that so that we pick up the new API we can now do end up parameters And these are all the weights and biases inside the entire neural net So in total this MLP has 41 parameters and Now we'll be able to change them If we recalculate the loss here, we see that unfortunately we have slightly different Predictions and slightly different loss But that's okay Okay, so we see that this neurons gradient is slightly negative. We can also look at its data right now Which is 0.85. So this is the current value of this neuron and this is its gradient on the loss So what we want to do now is we want to iterate for every p in End up parameters. So for all the 41 parameters in this neural net, we actually want to change p.data slightly according to the gradient information Okay, so dot dot dot to do here But this will be basically a tiny update in this gradient descent scheme and gradient descent we are thinking of the gradient as a vector pointing in the direction of increased loss And so in gradient descent we are modifying p.data By a small step size in the direction of the gradient So the step size as an example could be like a very small number like 0.01 is the step size times p.grad Right? But we have to think through some of the signs here. So In particular working with this specific example here. We see that if we just left it like this then this neurons value Would be currently increased by a tiny amount of the gradient The gradient is negative. So this value of this neuron would go slightly down. It would become like 0.8, you know 4 or something like that But if this neurons value goes lower That would actually increase the loss. That's because The derivative of this neuron is negative. So increasing this makes the loss go down So increasing it is what we want to do instead of decreasing it So basically what we're missing here is we're actually missing a negative sign And again this other interpretation and that's because we want to make this a little bit more complicated So we don't want to maximize the loss. We want to decrease it and the other interpretation as I mentioned is you can think of the gradient vector So basically just the vector of all the gradients As pointing in the direction of increasing The loss but then we want to decrease it. So we actually want to go in the opposite direction And so you can convince yourself that this sort of like that's the right thing here with the negative because we want to minimize the loss So if we nudge all the parameters by a tiny amount then we'll see that this data will have changed a little bit So now this neuron is a tiny amount greater value So 0.854 went to 0.857 And that's a good thing because slightly increasing this neuron Data makes the loss go down according to the gradient and so the correcting has happened sign-wise And so now what we would expect of course is that because we've changed all these parameters We expect that the loss should have gone down a bit So we want to re-evaluate the loss. Let me basically This is just a data definition that hasn't changed but the forward pass here of the network we can recalculate And actually let me do it outside here so that we can compare the two loss values So here if I recalculate the loss We'd expect the new loss now to be slightly lower than the previous loss So let's say I have a loss of 0.857 And I recalculate the loss. We'd expect the new loss now to be slightly lower than this number So hopefully what we're getting now is a tiny bit lower than 4.84 4.36 Okay, and remember the way we've arranged this is that low loss means that our predictions are matching the targets So our predictions now are probably slightly closer to the targets And now all we have to do is we have to iterate this process So again, we've done the forward pass and this is the loss Now we can loss.backward Let me take these out and we can do a step size And now we should have a slightly lower loss 4.36 goes to 3.9 And okay, so We've done the forward pass. Here's the backward pass Nudge And now the loss is 3.66 3.47 And you get the idea we just continue doing this and this is gradient descent We're just iteratively doing forward pass backward pass update Forward pass backward pass update and the neural net is improving its predictions So here if we look at y pred now y pred We see that This value should be getting closer to one So this value should be getting more positive. These should be getting more negative and this one should be also getting more positive So if we just iterate this a few more times Actually, we may be able to afford to go a bit faster. Let's try a slightly higher learning rate Whoops, okay. There we go. So now we're at 0.31 If you go too fast, by the way, if you try to make it too big of a step you may actually overstep It's overconfidence because again, remember we don't actually know exactly about the loss function The loss function has all kinds of structure and we only know about the very local Dependence of all these parameters on the loss, but if we step too far We may step into you know a part of the loss that is completely different And that can destabilize training and make your loss actually blow up even So the loss is now 0.04. So actually the predictions should be really quite close. Let's take a look So you see how this is almost 1, almost negative 1, almost 1. We can continue going So yep backward update Oops, there we go. So we went way too fast and We actually overstepped so we got too eager. Where are we now? Oops Okay 7E negative 9. So this is very very low loss and the predictions Are basically perfect So somehow we Basically we were doing way too big updates and we briefly exploded but then somehow we ended up getting into a really good spot So usually this learning rate and the tuning of it is a subtle art You want to set your learning rate If it's too low you're going to take way too long to converge But if it's too high the whole thing gets unstable and you might actually even explode the loss Depending on your loss function So finding the step size to be just right It's a pretty simple task When you're using sort of vanilla gradient descent But we happened to get into a good spot We can look at n.parameters So this is the setting of weights and biases That makes our network predict the desired targets very very close and Basically we've successfully trained a neural net Okay let's make this a tiny bit more real So let's say Okay let's make this a tiny bit more respectable And implement an actual training loop And what that looks like So this is the data definition that stays This is the forward pass So for k in range We're going to Take a bunch of steps First We do the forward pass We validate the loss Let's re-initialize the neural net from scratch And here's the data And we first do the forward pass Then we do the backward pass And then we do an update That's gradient descent And then we should be able to iterate this And we should be able to print the current step The current loss Let's just print the Number of the loss And That should be it And then the learning rate 0.01 is a little too small 0.1 we saw is a little bit dangerously too high Let's go somewhere in between And we'll optimize this for Not 10 steps but let's go for 20 steps Let me erase all of this Junk And let's run the optimization And you see how we've actually converged slower In a more controlled manner And got to a loss that is very low So I expect Y-Pred to be quite good There we go And That's it Okay so this is kind of embarrassing but we actually have a really terrible bug In here And it's a subtle bug And it's a very common bug And I can't believe I've done it for the 20th time in my life Especially on camera And I could have reshot the whole thing but I think it's pretty funny And you know you get to appreciate a bit What working with neural nets may be Is like sometimes We are guilty of A common bug I've actually tweeted The most common neural net mistakes a long time ago now And I'm not going to go into the details of this But I'm going to say That I'm not going to be doing this And I'm not really going to explain Any of these except for We are guilty of number 3 You forgot to zero grad before dot backward What is that? Basically what's happening And it's a subtle bug and I'm not sure if you saw it Is that All of these weights here Have a dot data and a dot grad And the dot grad Starts at zero And then we do backward And we fill in the gradients And then we do an update on the data But we don't flush the grad It stays there So when we do the second forward pass And we do backward again Remember that all the backward operations Do a plus equals on the grad And so these gradients just add up And they never get reset to zero So basically we didn't Zero grad So here's how we zero grad before Backward We need to reset all the parameters And we need to make sure that P dot grad is set to zero We need to reset it to zero Just like it is in the constructor So remember all the way here For all these value nodes Grad is reset to zero And then all these backward passes do a plus equals On that grad But we need to make sure that We reset these grads to zero So that when we do backward All of them start at zero and the actual backward pass Emulates the loss derivatives Into the grads So this is zero grad in PyTorch And we will get a slightly different optimization Let's reset the neural net The data is the same This is now I think correct And we get a much more You know we get a much more Slower descent We still end up with pretty good results And we can continue this a bit more To get down lower And we can continue this To get down lower And lower and lower Yeah So the only reason That the previous thing worked It's extremely buggy The only reason that worked is that This is a very very simple problem And it's very easy For this neural net to fit this data And so the grads Ended up accumulating And it effectively gave us a massive step size And it made us converge extremely fast But basically Now we have to do more steps To get to very low values of loss And get Y-pred to be really good We can try to Step a bit greater Yeah We're going to get closer and closer to 1, minus 1, 1 So Working with neural nets is sometimes Tricky because You may have lots of You may have lots of bugs in the code And Your network might actually work Just like ours worked But chances are is that if we had a more complex problem Then actually this bug would have Made us not optimize the loss very well And we were only able to get away with it because The problem is very simple So let's now bring everything together And summarize what we learned What are neural nets? Neural nets are these mathematical expressions Fairly simple mathematical expressions In the case of multilayer perceptron That take Input as the data And they take input, the weights and the parameters Of the neural net Mathematical expression for the forward pass Followed by a loss function And the loss function tries to measure the accuracy Of the predictions And usually the loss will be low when your predictions Are matching your targets Or where the network is basically behaving well So we manipulate the loss function So that when the loss is low The network is doing what you want it to do On your problem And then we backward the loss Use backpropagation to get the gradient And then we know how to tune all the parameters To decrease the loss locally But then we have to iterate that process Many times in what's called a gradient descent So we simply follow the gradient Information and that minimizes The loss and the loss is arranged so that When the loss is minimized The network is doing what you want it to do And So we just have a blob of neural stuff And we can make it do arbitrary things And that's what gives neural nets their power This is a very tiny network With 41 parameters But you can build significantly More complicated neural nets With billions At this point almost trillions of parameters And it's a massive blob of neural tissue Simulated neural tissue Roughly speaking And you can make it do Extremely complex problems And neural nets then have all kinds of Very fascinating emergent properties In When you try to make them do Significantly hard problems As in the case of GPT for example We have massive amounts of text from the internet And we're trying to get a neural net To predict, to take like a few words And try to predict the next word in a sequence That's the learning problem And it turns out that when you train this on all of internet The neural net actually has like really remarkable Emergent properties And the neural net would have hundreds of billions of parameters But It works on fundamentally the exact same principles The neural net of course will be a bit more complex But otherwise The value in the gradient Is there And will be identical And the gradient descent would be there And would be basically identical But people usually use slightly different updates This is a very simple stochastic gradient descent update And the loss function Would not be a mean squared error It's something called the cross entropy loss For predicting the next token So there's a few more details but fundamentally The neural network setup and neural network training Is identical and pervasive And now you understand intuitively How that works under the hood In the beginning of this video I told you that by the end of it You would understand everything in micrograd And then we'd slowly build it up Let me briefly prove that to you So I'm going to step through all the code that is in micrograd As of today Actually, potentially some of the code will change by the time you watch this video Because I intend to continue developing micrograd But let's look at what we have so far at least init.py is empty When you go to engine.py That has the value Everything here you should mostly recognize So we have the dot data dot grad attributes We have the backward function We have the previous set of children and the operation That produced this value We have addition, multiplication And raising to a scalar power We have the relu nonlinearity Which is a slightly different type of nonlinearity Than tanh that we used in this video Both of them are nonlinearities And notably tanh is not actually present In micrograd as of right now But I intend to add it later We have the backward which is identical And then all of these other operations Which are built up on top of operations Here So values should be very recognizable except for the nonlinearity used in this video There's no massive difference between relu And tanh and sigmoid and these other nonlinearities They're all roughly equivalent And can be used in MLPs So I use tanh because it's a bit smoother And because it's a little bit more complicated than relu And therefore it's stressed a little bit more The local gradients And working with those derivatives Which I thought would be useful nn.py is the neural networks library As I mentioned So you should recognize identical implementation of neuron Layer and MLP Notably, or not so much We have a class module here There's a parent class of all these modules I did that because there's an nn.module class In PyTorch And so this exactly matches that API And nn.module in PyTorch has also a zero grad Which I refactored out here So that's the end of micrograd really Then there's a test Which you'll see basically creates Two chunks of code One in micrograd and one in PyTorch And we'll make sure that the forward and the backward paths Agree identically For a slightly less complicated expression And slightly more complicated expression Everything agrees So we agree with PyTorch on all of these operations And finally there's a demo That IpyYMB here And it's a bit more complicated binary classification demo Than the one I covered in this lecture So we only had a tiny data set of four examples Here we have a bit more complicated example With lots of blue points And lots of red points And we're trying to again build a binary classifier To distinguish two-dimensional points As red or blue It's a bit more complicated MLP here It's a bigger MLP The loss is a bit more complicated Because it supports batches So because our data set was so tiny We always did a forward pass On the entire data set of four examples But when your data set is like a million examples What we usually do in practice Is we basically pick out Some random subset, we call that a batch And then we only process the batch Forward, backward, and update So we don't have to forward the entire training set So this supports batching Because there's a lot more examples here We do a forward pass The loss is slightly more different This is a max margin loss That I implement here The one that we used was the mean squared error loss because it's the simplest one. There's also the binary cross-entropy loss. All of them can be used for binary classification and don't make too much of a difference in the simple examples that we looked at so far. There's something called L2 regularization used here. This has to do with generalization of the neural net and controls the overfitting in machine learning setting, but I did not cover these concepts in this video, potentially later. And the training loop you should recognize. So forward, backward, with, zero grad, and update, and so on. You'll notice that in the update here, the learning rate is scaled as a function of number of iterations and it shrinks. And this is something called learning rate decay. So in the beginning, you have a high learning rate. And as the network sort of stabilizes near the end, you bring down the learning rate to get some of the fine details in the end. And in the end, we see the decision surface of the neural net and we see that it learned to separate out the red and the blue area based on the data points. So that's the slightly more complicated example in the demo.ipy.yamb that you're free to go over. But yeah, as of today, that is micrograd. I also wanted to show you a little bit of real stuff so that you get to see how this is actually implemented in a production-grade library like PyTorch. So in particular, I wanted to show, I wanted to find and show you the backward pass for 10h in PyTorch. So here in micrograd, we see that the backward pass for 10h is 1 minus t square, where t is the output of the 10h of x, times out that grad, which is the chain rule. So we're looking for something that looks like this. Now, I went to PyTorch, which has an open source GitHub code base, and I looked through a lot of its code. And honestly, I spent about 15 minutes and I couldn't find 10h. And that's because these libraries, unfortunately, they grow in size and entropy. And if you just search for 10h, you get apparently 2,800 results and 406 files. So I don't know what these files are doing, honestly, and why there are so many mentions of 10h. But unfortunately, these libraries are quite complex. They're meant to be used, not really inspected. Eventually, I did stumble on someone who tries to change the 10h backward code for some reason. And someone here pointed to the CPU kernel and the CUDA kernel for 10h backward. So this, so basically, it depends on if you're using PyTorch on a CPU device or on a GPU, which these are different devices, and I haven't covered this. But this is the 10h backward kernel for CPU. And the reason it's so large is that, number one, this is like if you're using a complex type, which we haven't even talked about. If you're using a specific data type of BFloat16, which we haven't talked about. And then if you're not, then this is the kernel. And deep here, we see something that resembles our backward pass. So they have A times 1 minus B squared. So this B here must be the output of the 10h, and this is the out.grad. So here we found it deep inside PyTorch on this location, for some reason inside binary ops kernel, when 10h is not actually a binary op. And then this is the GPU kernel. We're not complex. We're here, and here we go with one line of code. So we did find it, but basically, unfortunately, these code bases are very large, and micrograd is very, very simple. But if you actually want to use real stuff, finding the code for it, you'll actually find that difficult. I also wanted to show you a little example here, where PyTorch is showing you how you can register a new type of function that you want to add to PyTorch as a Lego building block. So here, if you want to, for example, add a Legendre polynomial 3, here's how you could do it. You will register it as a class that subclasses Torch.rgrad.function. And then you have to tell PyTorch how to forward your new function and how to backward through it. So as long as you can do the forward pass of this little function piece that you want to add, and as long as you know the local derivative, the local gradients, which are implemented in the backward, PyTorch will be able to back propagate through your function, and then you can use this as a Lego block in a larger Lego castle of all the different Lego blocks that PyTorch already has. And so that's the only thing you have to tell PyTorch, and everything would just work. And you can register new types of functions in this way, following this example. And that is everything that I wanted to cover in this lecture. So I hope you enjoyed building out micrograd with me. I hope you find it interesting, insightful, and yeah, I will probably see you in the next video. And yeah, I will post a lot of the links that are related to this video in the video description below. I will also probably post a link to a discussion forum or discussion group where you can ask questions related to this video, and then I can answer or someone else can answer your questions. And I may also do a follow-up video that answers some of the most common questions. But for now, that's it. I hope you enjoyed it. If you did, then please like and subscribe so that YouTube knows to feature this video to more people. And that's it for now. I'll see you later. Now here's the problem. We know DL by... Wait, what is the problem? And that's everything I wanted to cover in this lecture. So I hope you enjoyed us building out micrograd... micrograb? Okay, now let's do the exact same thing for multiply because we can't do something like A times 2. Oops. I know what happened there. Hi everyone, hope you're well. And next up what I'd like to do is I'd like to build out Makemore. Like Micrograd before it, Makemore is a repository that I have on my GitHub web page. You can look at it. But just like with Micrograd, I'm going to build it out step by step and I'm going to spell everything out. So we're going to build it out slowly and together. Now, what is Makemore? Makemore, as the name suggests, makes more of things that you give it. So here's an example. Names.txt is an example dataset to Makemore. And when you look at Names.txt, you'll find that it's a very large dataset of names. So here's lots of different types of names. In fact, I believe there are 32,000 names that I've sort of found randomly on a government website. And if you train Makemore on this dataset, it will learn to make more of things like this. And in particular, in this case, that will mean more things that sound name-like, but are actually unique names. And maybe if you have a baby and you're trying to assign a name, maybe you're looking for a cool new sounding unique name, Makemore might help you. So here are some example generations from the neural network once we train it on our dataset. So here's some example unique names that it will generate. Don't tell, I rot, Zendi, and so on. And so all these sort of sound name-like, but they're not, of course, names. So under the hood, Makemore is a character-level language model. So what that means is that it is treating every single line here as an example. And within each example, it's treating them all as sequences of individual characters. So R-E-E-S-E is this example, and that's the sequence of characters. And that's the level on which we are building out Makemore. And what it means to be a character-level language model, then, is that it's just sort of modeling those sequences of characters, and it knows how to predict the next character in the sequence. Now, we're actually going to implement a large number of character-level language models in terms of the neural networks that are involved in predicting the next character in a sequence. So very simple bigram and bag-of-word models, multilayered perceptrons, recurrent neural networks, all the way to modern transformers. In fact, the transformer that we will build will be basically the equivalent transformer to GPT-2, if you have heard of GPT. So that's kind of a big deal. It's a modern network, and by the end of the series, you will actually understand how that works on the level of characters. Now, to give you a sense of the extensions here, after characters, we will probably spend some time on the word level, so that we can generate documents of words, not just little segments of characters, but we can generate entire large, much larger documents. And then we're probably going to go into images and image-text networks, such as DALI, stable diffusion, and so on. But for now, we have to start here, character-level language modeling. Let's go. So like before, we are starting with a completely blank Jupyter Notebook page. The first thing is, I would like to basically load up the dataset, names.txt. So we're going to open up names.txt for reading, and we're going to read in everything into a massive string. And then, because it's a massive string, we'd only like the individual words and put them in the list. So let's call splitlines on that string to get all of our words as a Python list of strings. So basically, we can look at, for example, the first 10 words, and we have that it's a list of Emma, Olivia, Ava, and so on. And if we look at the top of the page here, that is indeed what we see. So that's good. This list actually makes me feel that this is probably sorted by frequency. But, okay, so these are the words. Now, we'd like to actually learn a little bit more about this dataset. Let's look at the total number of words. We expect this to be roughly 32,000. And then what is the, for example, shortest word? So min of len of each word for w in words. So the shortest word will be length 2, and max of len w for w in words. So the longest word will be 15 characters. So let's now think through our very first language model. As I mentioned, a character-level language model is predicting the next character in a sequence, given already some concrete sequence of characters before it. Now, what we have to realize here is that every single word here, like Isabella, is actually quite a few examples packed in to that single word. Because what is an existence of a word like Isabella in the dataset telling us, really? It's saying that the character i is a very likely character to come first in a sequence of a name. The character s is likely to come after i. The character a is likely to come after is. The character b is very likely to come after isa. And so on, all the way to a following Isabelle. And then there's one more example actually packed in here. And that is that after there's Isabella, the word is very likely to end. So that's one more sort of explicit piece of information that we have here, that we have to be careful with. And so there's a lot packed into a single individual word, in terms of the statistical structure of what's likely to follow in these character sequences. And then of course we don't have just an individual word. We actually have 32,000 of these. And so there's a lot of structure here to model. Now in the beginning, what I'd like to start with, is I'd like to start with building a bigram language model. Now in a bigram language model, we're always working with just two characters at a time. So we're only looking at one character that we are given, and we're trying to predict the next character in the sequence. So what characters are likely to follow r? What characters are likely to follow a? And so on. And we're just modeling that kind of a little local structure. And we're forgetting the fact that we may have a lot more information. We're always just looking at the previous character to predict the next one. So it's a very simple and weak language model, but I think it's a great place to start. So now let's begin by looking at these bigrams in our dataset, and what they look like. And these bigrams again are just two characters in a row. So for w in words, each w here is an individual word, a string. We want to iterate this word with consecutive characters. So two characters at a time sliding it through the word. Now an interesting, nice, cute way to do this in Python, by the way, is doing something like this. For character1, character2 in zip of w and w at 1. One column. Print character1, character2. And let's not do all the words. Let's just do the first three words. And I'm going to show you in a second how this works. But for now, basically, as an example, let's just do the very first word alone, emma. You see how we have an emma, and this will just print em, mm, ma. And the reason this works is because w is the string emma, w at one column is the string mma, and zip takes two iterators and it pairs them up and then creates an iterator over the tuples of their consecutive entries. And if any one of these lists is shorter than the other, then it will just halt and return. So basically, that's why we return em, mm, mm, ma. But then because this iterator, the second one here, runs out of elements, zip just ends. And that's why we only get these tuples. So pretty cute. So these are the consecutive elements in the first word. Now, we have to be careful because we actually have more information here than just these three examples. As I mentioned, we know that e is very likely to come first, and we know that a, in this case, is coming last. So one way to do this is basically we're going to create a special array here of characters. And we're going to hallucinate a special start token here. I'm going to call it, like, special start. So this is a list of one element, plus w, and then plus a special end character. And the reason I'm wrapping list of w here is because w is a string emma. List of w will just have the individual characters in the list. And then doing this again now, but not iterating over w's, but over the characters, will give us something like this. So e is likely, so this is a bigram of the start character and e, and this is a bigram of the a and the special end character. And now we can look at, for example, what this looks like for Olivia or Eva. And indeed, we can actually potentially do this for the entire dataset. But we won't print that. That's going to be too much. But these are the individual character bigrams, and we can print them. Now, in order to learn the statistics about which characters are likely to follow other characters, the simplest way in the bigram language models is to simply do it by counting. So we're basically just going to count how often any one of these combinations occurs in the training set in these words. So we're going to need some kind of a dictionary that's going to maintain some counts for every one of these bigrams. So let's use a dictionary b, and this will map these bigrams. So bigram is a tuple of character one, character two. And then b at bigram will be b.get of bigram, which is basically the same as b at bigram. But in the case that bigram is not in the dictionary b, we would like to, by default, return a zero, plus one. So this will basically add up all the bigrams and count how often they occur. Let's get rid of printing. Or rather, let's keep the printing and let's just inspect what b is in this case. And we see that many bigrams occur just a single time. This one allegedly occurred three times. So a was an ending character three times, and that's true for all of these words. All of Emma, Olivia, and Eva end with a. So that's why this occurred three times. Now let's do it for all the words. Oops, I should not have printed. I meant to erase that. Let's kill this. Let's just run. And now b will have the statistics of the entire data set. So these are the counts across all the words of the individual bigrams. And we could, for example, look at some of the most common ones and least common ones. This kind of grows in Python, but the way to do this, the simplest way I like, is we just use b.items. b.items returns the tuples of key value. In this case, the keys are the character bigrams and the values are the counts. And so then what we want to do is we want to do sorted of this. But by default, sort is on the first item of a tuple. But we want to sort by the values, which are the second element of a tuple, that is the key value. So we want to use the key equals lambda that takes the key value and returns the key value at 1. Not at 0, but at 1, which is the count. So we want to sort by the count of these elements. And actually, we want it to go backwards. So here what we have is the bigram Q&R occurs only a single time. DZ occurred only a single time. And when we sort this the other way around, we're going to see the most likely bigrams. So we see that N was very often an ending character, many, many times. And apparently, N almost always follows an A, and that's a very likely combination as well. So this is kind of the individual counts that we achieve over the entire dataset. Now, it's actually going to be significantly more convenient for us to keep this information in a two-dimensional array instead of a Python dictionary. So we're going to store this information in a 2D array. And the rows are going to be the first character of the bigram, and the columns are going to be the second character. And each entry in this two-dimensional array will tell us how often that first character follows the second character in the dataset. So in particular, the array representation that we're going to use, or the library, is that of PyTorch. And PyTorch is a deep learning neural network framework. But part of it is also this torch.tensor, which allows us to create multidimensional arrays and manipulate them very efficiently. So let's import PyTorch, which you can do by import torch. And then we can create arrays. So let's create an array of zeros, and we give it a size of this array. Let's create a 3x5 array as an example. And this is a 3x5 array of zeros. And by default, you'll notice a.dtype, which is short for datatype, is float32. So these are single precision floating point numbers. Because we are going to represent counts, let's actually use dtype as torch.int32. So these are 32-bit integers. So now you see that we have integer data inside this tensor. Now, tensors allow us to really manipulate all the individual entries and do it very efficiently. So for example, if we want to change this bit, we have to index into the tensor. And in particular, here, this is the first row, because it's zero-indexed. So this is row index 1 and column index 0, 1, 2, 3. So a at 1, 3, we can set that to 1. And then a will have a 1 over there. We can, of course, also do things like this. So now a will be 2 over there, or 3. And also we can, for example, say a, 0, 0 is 5. And then a will have a 5 over here. So that's how we can index into the arrays. Now, of course, the array that we are interested in is much, much bigger. So for our purposes, we have 26 letters of the alphabet. And then we have two special characters, s and e. So we want 26 plus 2, or 28 by 28 array. And let's call it the capital N, because it's going to represent the counts. Let me erase this stuff. So that's the array that starts at 0s, 28 by 28. And now let's copy-paste this here. But instead of having a dictionary b, which we're going to erase, we now have an N. Now, the problem here is that we have these characters, which are strings, but we have to now basically index into an array, and we have to index using integers. So we need some kind of a lookup table from characters to integers. So let's construct such a character array. And the way we're going to do this is we're going to take all the words, which is a list of strings. We're going to concatenate all of it into a massive string. So this is just simply the entire data set as a single string. We're going to pass this to the set constructor, which takes this massive string and throws out duplicates, because sets do not allow duplicates. So set of this will just be the set of all the lowercase characters. And there should be a total of 26 of them. And now we actually don't want a set, we want a list. But we don't want a list sorted in some weird arbitrary way. We want it to be sorted from A to Z. So a sorted list. So those are our characters. Now what we want is this lookup table, as I mentioned. So let's create a special S to I, I will call it. S is string or character. And this will be an S to I mapping for IS in enumerate of these characters. So enumerate basically gives us this iterator over the integer, index, and the actual element of the list. And then we are mapping the character to the integer. So S to I is a mapping from A to 0, B to 1, etc., all the way from Z to 25. And that's going to be useful here, but we actually also have to specifically set that S will be 26, and S to I at E will be 27, because Z was 25. So those are the lookups. And now we can come here and we can map both character 1 and character 2 to their integers. So this will be S to I of character 1, and IX2 will be S to I of character 2. And now we should be able to do this line, but using our array. So n at IX1, IX2. This is the two-dimensional array indexing I've shown you before. And honestly, just plus equals 1, because everything starts at 0. So this should work and give us a large 28 by 28 array of all these counts. So if we print n, this is the array, but of course it looks ugly. So let's erase this ugly mess, and let's try to visualize it a bit more nicer. So for that, we're going to use a library called matplotlib. So matplotlib allows us to create figures. So we can do things like plt.imshow of the count array. So this is the 28 by 28 array, and this is the structure. But even this, I would say, is still pretty ugly. So we're going to try to create a much nicer visualization of it, and I wrote a bunch of code for that. The first thing we're going to need is we're going to need to invert this array here, this dictionary. So s2i is a mapping from s to i, and in i2s, we're going to reverse this dictionary. So iterate over all the items and just reverse that array. So i2s maps inversely from 0 to a, 1 to b, etc. So we'll need that. And then here's the code that I came up with to try to make this a little bit nicer. We create a figure. We plot n, and then we visualize a bunch of things later. Let me just run it so you get a sense of what this is. Okay. So you see here that we have the array spaced out, and every one of these is basically like b follows g zero times, b follows h 41 times, so a follows j 175 times. And so what you can see that I'm doing here is first I show that entire array, and then I iterate over all the individual little cells here, and I create a character string here, which is the entire Binding-Free HTTP original coding collection chart, inverse mapping i2s of the integer i and the integer j, so those are the bigrams in a character representation, and then I plot just the bigram text, and then I plot the number of times that this bigram occurs. Now the reason that there's a dot item here is because when you index into these arrays, these are torch tensors, you see that we still get a tensor back. So the type of this thing, you'd think it would be just an integer, 149, but it's actually a torch dot tensor. And so if you do dot item, then it will pop out that individual integer. So it'll just be 149. So that's what's happening there. And these are just some options to make it look nice. So what is the structure of this array? We have all these counts, and we see that some of them occur often, and some of them do not occur often. Now if you scrutinize this carefully, you will notice that we're not actually being very clever. That's because when you come over here, you'll notice that for example we have an entire row of completely zeros, and that's because the end character is never possibly going to be the first character of a bigram, because we're always placing these end tokens at the end of a bigram. Similarly we have entire columns of zeros here, because the s character will never possibly be the second element of a bigram, because we always start with s and we end with e, and we only have the words in between. So we have an entire column of zeros, an entire row of zeros, and in this little 2x2 matrix here as well, the only one that can possibly happen is if s directly follows e. That can be non-zero if we have a word that has no letters. So in that case there's no letters in the word, it's an empty word, and we just have s follows e. But the other ones are just not possible. And so we're basically wasting space, and not only that, but the s and the e are getting very crowded here. I was using these brackets because there's convention in natural language processing to use these kinds of brackets to denote special tokens, but we're going to use something else. So let's fix all this and make it prettier. We're not actually going to have two special tokens, we're only going to have one special token. So we're going to have nxn array of 27x27 instead. Instead of having two, we will just have one, and I will call it a dot. Let me swing this over here. Now one more thing that I would like to do is I would actually like to make this special character have position 0, and I would like to offset all the other letters off. I find that a little bit more pleasing. So we need a plus one here so that the first character, which is a, will start at 1. So s to i will now be a starts at 1 and dot is 0. And i to s, of course, we're not changing this because i to s just creates a reverse mapping and this will work fine. So 1 is a, 2 is b, 0 is dot. So we reverse that here, we have a dot and a dot. This should work fine. Make sure I start at 0s. Count. And then here we don't go up to 28, we go up to 27. And this should just work. So we see that dot dot never happened, it's at 0 because we don't have empty words. And this row here now is just very simply the counts for all the first letters. So j starts a word, h starts a word, i starts a word, etc. And then these are all the ending characters. And in between we have the structure of what characters follow each other. So this is the counts array of our entire dataset. So this array actually has all the information necessary for us to actually sample from this bigram character level language model. And roughly speaking what we're going to do is we're just going to start following these probabilities and these counts and we're going to start sampling from the model. So in the beginning of course we start with the dot, the start token dot. So to sample the first character of a name we're looking at this row here. So we see that we have the counts and those counts are telling us how often any one of these characters is to start a word. So if we take this n and we grab the first row, we can do that by just indexing as 0 and then using this notation colon for the rest of that row. So n, 0, colon is indexing into the 0th row and then it's grabbing all the columns. And so this will give us a one-dimensional array of the first row. So 0, 4, 4, 10, you notice 0, 4, 4, 10, 1, 3, 0, 6, 1, 5, 4, 2, etc. It's just the first row. The shape of this is 27, it's just the row of 27. And the other way that you can do this also is you don't actually do this, you just grab the 0th row like this. This is equivalent. Now these are the counts and now what we'd like to do is we'd like to basically sample from this. Since these are the raw counts, we actually have to convert this to probabilities. So we create a probability vector. So we'll take n of 0 and we'll actually convert this to float first. So these integers are converted to float, floating point numbers. And the reason we're creating floats is because we're about to normalize these counts. So to create a probability distribution here, we want to divide. We basically want to do p, p divide, p dot sum. And now we get a vector of smaller numbers and these are now probabilities. So of course because we divided by the sum, the sum of p now is 1. So this is a nice proper probability distribution, it sums to 1. This is giving us the probability for any single character to be the first character of a word. So now we can try to sample from this distribution. To sample from these distributions, we're going to use tors dot multinomial, which I've pulled up here. So tors dot multinomial returns samples from the multinomial probability distribution, which is a complicated way of saying you give me probabilities and I will give you integers, which are sampled according to the probability distribution. So this is the signature of the method. And to make everything deterministic, we're going to use a generator object in PyTorch. So this makes everything deterministic. So when you run this on your computer, you're going to get the exact same results that I'm getting here on my computer. So let me show you how this works. Here's the deterministic way of creating a torch generator object, seeding it with some number that we can agree on, so that seeds a generator, gives us an object g. And then we can pass that g to a function that creates here random numbers, tors dot rand creates random numbers, three of them. And it's using this generator object as a source of randomness. So without normalizing it, I can just print. This is sort of like numbers between zero and one that are random according to this thing. And whenever I run it again, I'm always going to get the same result because I keep using the same generator object, which I'm seeding here. And then if I divide to normalize, I'm going to get a nice probability distribution of just three elements. And then we can use tors dot multinomial to draw samples from it. So this is what that looks like. Tors dot multinomial will take the torch tensor of probability distributions. Then we can ask for a number of samples, let's say 20. Replacement equals true means that when we draw an element, we can draw it and then we can put it back into the list of eligible indices to draw again. And we have to specify replacement as true because by default, for some reason, it's false. It's just something to be careful with. And the generator is passed in here. So we are going to always get deterministic results, the same results. So if I run these two, we're going to get a bunch of samples from this distribution. Now you'll notice here that the probability for the first element in this tensor is 60%. So in these 20 samples, we'd expect 60% of them to be zero. We'd expect 30% of them to be one. And because the element index two has only 10% probability, very few of these samples should be two. And indeed, we only have a small number of twos. We can sample as many as we like. And the more we sample, the more these numbers should roughly have the distribution here. So we should have lots of zeros, half as many ones, and we should have three times as few ones, and three times as few twos. So you see that we have very few twos, we have some ones, and most of them are zero. So that's what torsion multinomial is doing. For us here, we are interested in this row, we've created this p here, and now we can sample from it. So if we use the same seed, and then we sample from this distribution, let's just get one sample, then we see that the sample is, say, 13. So this will be the index. And you see how it's a tensor that wraps 13? We again have to use.item to pop out that integer. And now index would be just the number 13. And of course, we can map the I2S of IX to figure out exactly which character we're sampling here. We're sampling M. So we're saying that the first character is M in our generation. And just looking at the row here, M was drawn, and we can see that M actually starts a large number of words. M started 2,500 words out of 32,000 words. So almost a bit less than 10% of the words start with M. So this is actually a fairly likely character to draw. So that would be the first character of our word. And now we can continue to sample more characters, because now we know that M is already sampled. So now to draw the next character, we will come back here, and we will look for the row that starts with M. So you see M, and we have a row here. So we see that M. is 516, MA is this many, MB is this many, etc. So these are the counts for the next row, and that's the next character that we are going to now generate. So I think we are ready to actually just write out the loop, because I think you're starting to get a sense of how this is going to go. The, we always begin at index 0, because that's the start token. And then while true, we're going to grab the row corresponding to index that we're currently on. So that's P, so that's N array at IX, converted to float is RP. Then we normalize this P to sum to 1. I accidentally ran the infinite loop. We normalize P to sum to 1. Then we need this generator object. And we're going to initialize up here, and we're going to draw a single sample from this distribution. And then this is going to tell us what index is going to be next. If the index sampled is 0, then that's now the end token. So we will break. Otherwise we are going to print S2I of IX. I2S of IX. And that's pretty much it. We're just, this should work. Okay, more. So that's the name that we've sampled. We started with M, the next step was O, then R, and then dot. And this dot, we print it here as well. So let's now do this a few times. So let's actually create an out list here. And instead of printing, we're going to append. So out.append this character. And then here, let's just print it at the end. So let's just join up all the outs, and we're just going to print more. Now we're always getting the same result because of the generator. So if we want to do this a few times, we can go for I in range 10. We can sample 10 names, and we can just do that 10 times. And these are the names that we're getting out. Let's do 20. I'll be honest with you, this doesn't look right. So I started a few minutes to convince myself that it actually is right. The reason these samples are so terrible is that bigram language model is actually just like really terrible. We can generate a few more here. And you can see that they're kind of like, they're name-like a little bit, like Ianu, O'Reilly, et cetera. But they're just like totally messed up. And I mean, the reason that this is so bad, like we're generating H as a name. But you have to think through it from the model's eyes. It doesn't know that this H is the very first H. All it knows is that H was previously. And now how likely is H the last character? Well, it's somewhat likely. And so it just makes it last character. It doesn't know that there were other things before it or there were not other things before it. And so that's why it's generating all these like nonsense names. Another way to do this is to convince yourself that this is actually doing something reasonable, even though it's so terrible, is these little p's here are 27, right? Like 27. So how about if we did something like this? Instead of p having any structure whatsoever, how about if p was just torch.once of 27? By default, this is a float 32, so this is fine. Divide 27. So what I'm doing here is this is the uniform distribution, which will make everything equally likely. And we can sample from that. So let's see if that does any better. So this is what you have from a model that is completely untrained, where everything is equally likely. So it's obviously garbage. And then if we have a trained model, which is trained on just bigrams, this is what we get. So you can see that it is more name-like. It is actually working. It's just bigram is so terrible and we have to do better. Now next, I would like to fix an inefficiency that we have going on here, because what we're doing here is we're always fetching a row of n from the counts matrix up ahead. And then we're always doing the same things. We're converting to float and we're dividing. And we're doing this every single iteration of this loop. And we just keep renormalizing these rows over and over again. And it's extremely inefficient and wasteful. So what I'd like to do is I'd like to actually prepare a matrix, capital P, that will just have the probabilities in it. So in other words, it's going to be the same as the capital N matrix here of counts, but every single row will have the row of probabilities that is normalized to one, indicating the probability distribution for the next character, given the character before it, as defined by which row we're in. So basically what we'd like to do is we'd like to just do it up front here. And then we would like to just use that row here. So here we would like to just do P equals P of IX instead. The other reason I want to do this is not just for efficiency, but also I would like us to practice these n-dimensional tensors. And I'd like us to practice their manipulation, and especially something that's called broadcasting that we'll go into in a second. We're actually going to have to become very good at these tensor manipulations, because if we're going to build out all the way to transformers, we're going to be doing some pretty complicated array operations for efficiency, and we need to really understand that and be very good at it. So intuitively what we want to do is we first want to grab the floating point copy of N, and I'm mimicking the line here, basically. And then we want to divide all the rows so that they sum to one. So we'd like to do something like this, P divide P dot sum. But now we have to be careful, because P dot sum actually produces a sum, sorry, P equals N dot float copy. P dot sum produces a, sums up all of the counts of this entire matrix N, and gives us a single number of just the summation of everything. So that's not the way we want to divide. We want to simultaneously and in parallel divide all the rows by their respective sums. So what we have to do now is we have to go into documentation for tors dot sum, and we can scroll down here to a definition that is relevant to us, which is where we don't only provide an input array that we want to sum, but we also provide the dimension along which we want to sum. And in particular, we want to sum up over rows, right? Now one more argument that I want you to pay attention to here is the keepdim is false. If keepdim is true, then the output tensor is of the same size as input, except of course the dimension along which you summed, which will become just one. But if you pass in keepdim as false, then this dimension is squeezed out. And so tors dot sum not only does the sum and collapses dimension to be of size one, but in addition it does what's called a squeeze, where it squeezes out that dimension. So basically what we want here is we instead want to do p dot sum of some axis. And in particular, notice that p dot shape is 27 by 27. So when we sum up across axis zero, then we would be taking the zeroth dimension and we would be summing across it. So when keepdim is true, then this thing will not only give us the counts along the columns, but notice that basically the shape of this is one by 27. We just get a row vector. And the reason we get a row vector here again is because we passed in keepdim. in 0 dimension, so this 0th dimension becomes 1. And we've done a sum, and we get a row. And so basically we've done the sum this way, vertically, and arrived at just a single 1 by 27 vector of counts. What happens when you take out keep them is that we just get 27. So it squeezes out that dimension, and we just get a one-dimensional vector of size 27. Now we don't actually want 1 by 27 row vector, because that gives us the counts, or the sums, across the columns. We actually want to sum the other way, along dimension 1. And you'll see that the shape of this is 27 by 1, so it's a column vector. It's a 27 by 1 vector of counts. And that's because what's happened here is that we're going horizontally, and this 27 by 27 matrix becomes a 27 by 1 array. Now you'll notice, by the way, that the actual numbers of these counts are identical. And that's because this special array of counts here comes from bigram statistics. And actually it just so happens, by chance, or because of the way this array is constructed, that the sums along the columns, or along the rows, horizontally or vertically, is identical. And actually what we want to do in this case is we want to sum across the rows, horizontally. So what we want here is p.sum of 1 with keepdim true, 27 by 1 column vector. And now what we want to do is we want to divide by that. Now we have to be careful here again. Is it possible to take what's a p.shape you see here, 27 by 27, is it possible to take a 27 by 27 array and divide it by what is a 27 by 1 array? Is that an operation that you can do? And whether or not you can perform this operation is determined by what's called broadcasting rules. So if you just search broadcasting semantics in Torch, you'll notice that there's a special definition for what's called broadcasting, that for whether or not these two arrays can be combined in a binary operation like division. So the first condition is each tensor has at least one dimension, which is the case for us. And then when iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is one, or one of them does not exist. So let's do that. We need to align the two arrays and their shapes, which is very easy because both of these shapes have two elements, so they're aligned. And we iterate over from the right and going to the left. Each dimension must be either equal, one of them is a one, or one of them does not exist. So in this case, they're not equal, but one of them is a one, so this is fine. And then this dimension, they're both equal, so this is fine. So all the dimensions are fine, and therefore this operation is broadcastable. So that means that this operation is allowed. And what is it that these arrays do when you divide 27 by 27 by 27 by 1? What it does is that it takes this dimension 1 and it stretches it out, it copies it to match 27 here in this case. So in our case, it takes this column vector, which is 27 by 1, and it copies it 27 times to make these both be 27 by 27 internally. You can think of it that way. And so it copies those counts, and then it does an element-wise division, which is what we want, because these counts, we want to divide by them on every single one of these columns in this matrix. So this actually, we expect, will normalize every single row. And we can check that this is true by taking the first row, for example, and taking its sum. We expect this to be 1, because it's now normalized. And then we expect this now, because if we actually correctly normalize all the rows, we expect to get the exact same result here. So let's run this. It's the exact same result. So this is correct. So now I would like to scare you a little bit. You actually have to, like, I basically encourage you very strongly to read through broadcasting semantics, and I encourage you to treat this with respect. And it's not something to play fast and loose with. It's something to really respect, really understand, and look up maybe some tutorials for broadcasting and practice it, and be careful with it, because you can very quickly run into bugs. Let me show you what I mean. You see how here we have p.sum of 1, keep them as true. The shape of this is 27 by 1. Let me take out this line just so we have the n, and then we can see the counts. We can see that this is all the counts across all the rows, and it's a 27 by 1 column vector, right? Now, suppose that I tried to do the following, but I erase keep them as true here. What does that do? If keep them is not true, it's false. Then remember, according to the documentation, it gets rid of this dimension 1. It squeezes it out. So basically we just get all the same counts, the same result, except the shape of it is not 27 by 1. It is just 27. The 1 disappears. But all the counts are the same. So you'd think that this, divide that, would work. First of all, can we even write this? Is it even expected to run? Is it broadcastable? Let's determine if this result is broadcastable. p.sum at 1 is shape. It's 27. This is 27 by 27. So 27 by 27, broadcasting into 27. So now, rules of broadcasting, number 1, align all the dimensions on the right. Done. Now, iteration over all the dimensions starting from the right, going to the left. All the dimensions must either be equal, one of them must be 1, or one of them does not exist. So here they are all equal. Here the dimension does not exist. So internally what broadcasting will do is it will create a 1 here, and then we see that one of them is a 1, and this will get copied, and this will run, this will broadcast. Okay, so you'd expect this to work, because we are, this broadcasts and this, we can divide this. Now if I run this, you'd expect it to work, but it doesn't. You actually get garbage. You get a wrong result, because this is actually a bug. This keepdim equals true makes it work. This is a bug. In both cases, we are doing the correct counts. We are summing up across the rows, but keepdim is saving us and making it work. So in this case, I'd like to encourage you to potentially like pause this video at this point and try to think about why this is buggy, and why the keepdim was necessary here. Okay, so the reason to do for this is I'm trying to hint it here when I was sort of giving you a bit of a hint on how this works. This 27 vector internally inside the broadcasting, this becomes a 1 by 27, and 1 by 27 is a row vector, right? And now we are dividing 27 by 27 by 1 by 27, and torch will replicate this dimension. So basically, it will take this row vector, and it will copy it vertically now 27 times, so the 27 by 27 lines exactly, and element-wise divides. And so basically what's happening here is we're actually normalizing the columns instead of normalizing the rows. So you can check that what's happening here is that p at 0, which is the first row of p, dot sum is not 1, it's 7. It is the first column, as an example, that sums to 1. So to summarize, where does the issue come from? The issue comes from the silent adding of a dimension here, because in broadcasting rules, you align on the right and go from right to left, and if the dimension doesn't exist, you create it. So that's where the problem happens. We still did the counts correctly. We did the counts across the rows, and we got the counts on the right here as a column vector. But because the keepdance was true, this dimension was discarded, and now we just have a vector of 27. And because of broadcasting the way it works, this vector of 27 suddenly becomes a row vector. And then this row vector gets replicated vertically, and at every single point, we are dividing by the count in the opposite direction. So this thing just doesn't work. This needs to be keepdance equals true in this case. So then we have that p at 0 is normalized. And conversely, the first column you'd expect to potentially not be normalized. And this is what makes it work. So pretty subtle, and hopefully this helps to scare you, that you should have respect for broadcasting, be careful, check your work, and understand how it works under the hood, and make sure that it's broadcasting in the direction that you like. Otherwise, you're going to introduce very subtle bugs, very hard to find bugs, and just be careful. One more note on efficiency. We don't want to be doing this here, because this creates a completely new tensor that we store in 2P. We prefer to use in-place operations if possible. So this would be an in-place operation, has the potential to be faster. It doesn't create new memory under the hood. And then let's erase this. We don't need it. And let's also just do fewer, just so I'm not wasting space. OK, so we're actually in a pretty good spot now. We trained a bigram language model. And we trained it really just by counting how frequently any pairing occurs, and then normalizing so that we get a nice probability distribution. So really, these elements of this array p are really the parameters of our bigram language model, giving us and summarizing the statistics of these bigrams. So we trained a model, and then we know how to sample from the model. We just iteratively sample the next character, and feed it in each time, and get a next character. Now what I'd like to do is I'd like to somehow evaluate the quality of this model. We'd like to somehow summarize the quality of this model into a single number. How good is it at predicting the training set as an example? So in the training set, we can evaluate now the training loss. This training loss is telling us about the quality of this model in a single number, just like we saw in micrograd. So let's try to think through the quality of the model and how we would evaluate it. Basically, what we're going to do is we're going to copy-paste this code that we previously used for counting. And let me just print these bigrams first. We're going to use f strings. And I'm going to print character 1 followed by character 2. These are the bigrams. And then I don't want to do it for all the words, just do the first three words. So here we have Emma, Olivia, and Ava bigrams. Now what we'd like to do is we'd like to basically look at the probability that the model assigns to every one of these bigrams. So in other words, we can look at the probability, which is summarized in the matrix P, of IX1, IX2. And then we can print it here as probability. And because these probabilities are way too large, let me percent or colon 0.4f to truncate it a bit. So what do we have here? We're looking at the probabilities that the model assigns to every one of these bigrams in the dataset. And so we can see some of them are 4%, 3%, et cetera. Just to have a measuring stick in our mind, by the way, we have 27 possible characters or tokens. And if everything was equally likely, then you'd expect all these probabilities to be 4% roughly. So anything above 4% means that we've learned something useful from these bigram statistics. And you see that roughly some of these are 4%, but some of them are as high as 40%, 35%, and so on. So you see that the model actually assigned a pretty high probability to whatever's in the training set. And so that's a good thing. Basically if you have a very good model, you'd expect that these probabilities should be near 1, because that means that your model is correctly predicting what's going to come next, especially on the training set where you trained your model. So now we'd like to think about how can we summarize these probabilities into a single number that measures the quality of this model. Now when you look at the literature into maximum likelihood estimation and statistical modeling and so on, you'll see that what's typically used here is something called the likelihood. And the likelihood is the product of all of these probabilities. And so the product of all of these probabilities is the likelihood, and it's really telling us about the probability of the entire data set assigned by the model that we've trained. And that is a measure of quality. So the product of these should be as high as possible. When you are training the model and when you have a good model, your product of these probabilities should be very high. Now because the product of these probabilities is an unwieldy thing to work with, you can see that all of them are between 0 and 1. So your product of these probabilities will be a very tiny number. So for convenience, what people work with usually is not the likelihood, but they work with what's called the log likelihood. So the product of these is the likelihood. To get the log likelihood, we just have to take the log of the probability. And so the log of the probability here, I have the log of x from 0 to 1. The log is a, you see here, monotonic transformation of the probability, where if you pass in 1, you get 0. So probability 1 gets you log probability of 0. And then as you go lower and lower probability, the log will grow more and more negative until all the way to negative infinity at 0. So here we have a log prob, which is really just a torched log of probability. Let's print it out to get a sense of what that looks like. Log prob, also 0.4f. So as you can see, when we plug in numbers that are very close, some of our higher numbers, we get closer and closer to 0. And then if we plug in very bad probabilities, we get more and more negative number. That's bad. And the reason we work with this is for large extent convenience, right? Because we have mathematically that if you have some product, a times b times c, of all these probabilities, right? The likelihood is the product of all these probabilities. Then the log of these is just log of a plus log of b plus log of c. If you remember your logs from your high school or undergrad and so on. So we have that basically, the likelihood is the product of probabilities. The log likelihood is just the sum of the logs of the individual probabilities. So log likelihood starts at 0. And then log likelihood here, we can just accumulate simply. And then the end, we can print this. Print the log likelihood. F strings, maybe you're familiar with this. So log likelihood is negative 38. Okay. Now we actually want, so how high can log likelihood get? It can go to 0. So when all the probabilities are 1, log likelihood will be 0. And then when all the probabilities are lower, this will grow more and more negative. Now we don't actually like this because what we'd like is a loss function. And a loss function has the semantics that low is good because we're trying to minimize the loss. So we actually need to invert this. And that's what gives us something called the negative log likelihood. Negative log likelihood is just negative of the log likelihood. These are F strings, by the way, if you'd like to look this up. Negative log likelihood equals. So negative log likelihood now is just negative of it. And so the negative log likelihood is a very nice loss function because the lowest it can get is 0. And the higher it is, the worse off the predictions are that you're making. And then one more modification to this that sometimes people do is that for convenience, they actually like to normalize by, they like to make it an average instead of a sum. And so here, let's just keep some counts as well. So n plus equals 1 starts at 0. And then here, we can have sort of like a normalized log likelihood. If we just normalize it by the count, then we will sort of get the average log likelihood. So this would be usually our loss function here. This is what we would use. So our loss function for the training set assigned by the model is 2.4. That's the quality of this model. And the lower it is, the better off we are. And the higher it is, the worse off we are. And the job of our training is to find the parameters that minimize the negative log likelihood loss. And that would be like a high quality model. Okay, so to summarize, I actually wrote it out here. So our goal is to maximize likelihood, which is the product of all the probabilities assigned by the model. And we want to maximize this likelihood with respect to the model parameters. And in our case, the model parameters here are defined in the table. These numbers, the probabilities, are the model parameters sort of in our bigram language model so far. But you have to keep in mind that here we are storing everything in a table format, the probabilities. But what's coming up as a brief preview is that these numbers will not be kept explicitly, but these numbers will be calculated by a neural network. So that's coming up. And we want to change and tune the parameters of these neural networks. We want to change these parameters to maximize the likelihood, the product of the probabilities. Now maximizing the likelihood is equivalent to maximizing the log likelihood, because log is a monotonic function. Here's the graph of log. And basically all it is doing is it's just scaling your, you can look at it as just a scaling of the loss function. And so the optimization problem here and here are actually equivalent, because this is just scaling. You can look at it that way. And so these are two identical optimization problems. Maximizing the log likelihood is equivalent to minimizing the negative log likelihood. And then in practice, people actually minimize the average negative log likelihood to get numbers like 2.4. And then this summarizes the quality of your model. And we'd like to minimize it and make it as small as possible. And then the performance was smooth, because even if I use my approximation of higher value probabilities, I actually have a small expected value. the lowest it can get is zero, and the lower it is, the better off your model is, because it's assigning high probabilities to your data. Now let's estimate the probability over the entire training set, just to make sure that we get something around 2.4. Let's run this over the entire, oops, let's take out the print statement as well. Okay, 2.45 over the entire training set. Now what I'd like to show you is that you can actually evaluate the probability for any word that you want, like for example, if we just test a single word, Andre, and bring back the print statement, then you see that Andre is actually kind of like an unlikely word, like on average, we take three log probability to represent it, and roughly that's because EJ apparently is very uncommon as an example. Now think through this. When I take Andre and I append Q, and I test the probability of it, Andre Q, we actually get infinity. And that's because JQ has a 0% probability according to our model, so the log of zero will be negative infinity, and we get infinite loss. So this is kind of undesirable, right, because we plugged in a string that could be like a somewhat reasonable name, but basically what this is saying is that this model is exactly 0% likely to predict this name, and our loss is infinity on this example. And really the reason for that is that J is followed by Q zero times. Where's Q? JQ is zero, and so JQ is 0% likely. So this is actually kind of gross, and people don't like this too much. To fix this, there's a very simple fix that people like to do to sort of like smooth out your model a little bit, and it's called model smoothing. And roughly what's happening is that we will add some fake counts. So imagine adding a count of one to everything. So we add a count of one, like this, and then we recalculate the probabilities. And that's model smoothing, and you can add as much as you like. You can add five, and that will give you a smoother model. And the more you add here, the more uniform model you're going to have. And the less you add, the more peaked model you're going to have, of course. So one is like a pretty decent count to add, and that will ensure that there will be no zeros in our probability matrix P. And so this will of course change the generations a little bit. In this case it didn't, but in principle it could. But what that's going to do now is that nothing will be infinity unlikely. So now our model will predict some other probability, and we see that JQ now has a very small probability, so the model still finds it very surprising that this was a word or a bigram, but we don't get negative infinity. So that's kind of like a nice fix that people like to apply sometimes, and it's called model smoothing. Okay, so we've now trained a respectable bigram character level language model. And we saw that we both sort of trained the model by looking at the counts of all the bigrams and normalizing the rows to get probability distributions. We saw that we can also then use those parameters of this model to perform sampling of new words, so we sample new names according to those distributions. And we also saw that we can evaluate the quality of this model. And the quality of this model is summarized in a single number, which is the negative log likelihood. And the lower this number is, the better the model is, because it is giving high probabilities to the actual next characters in all the bigrams in our training set. So that's all well and good, but we've arrived at this model explicitly by doing something that felt sensible. We were just performing counts, and then we were normalizing those counts. Now what I would like to do is I would like to take an alternative approach. We will end up in a very, very similar position, but the approach will look very different, because I would like to cast the problem of bigram character level language modeling into the neural network framework. And in the neural network framework, we're going to approach things slightly differently, but again, end up in a very similar spot. I'll go into that later. Now our neural network is going to be still a bigram character level language model. So it receives a single character as an input. Then there's neural network with some weights or some parameters w, and it's going to output the probability distribution over the next character in a sequence. It's going to make guesses as to what is likely to follow this character that was input to the model. And then in addition to that, we're going to be able to evaluate any setting of the parameters of the neural net, because we have the loss function, the negative log likelihood. So we're going to take a look at its probability distributions, and we're going to use the labels, which are basically just the identity of the next character in that bigram, the second character. So knowing what second character actually comes next in the bigram allows us to then look at how high of a probability the model assigns to that character. And then we of course want the probability to be very high. And that is another way of saying that the loss is low. So we're going to use gradient-based optimization then to tune the parameters of this network, because we have the loss function and we're going to minimize it. So we're going to tune the weights so that the neural net is correctly predicting the probabilities for the next character. So let's get started. The first thing I want to do is I want to compile the training set of this neural network. So create the training set of all the bigrams. And here I'm going to copy-paste this code, because this code iterates over all the bigrams. So here we start with the words, we iterate over all the bigrams, and previously, as you recall, we did the counts. But now we're not going to do counts. We're just creating a training set. Now this training set will be made up of two lists. We have the inputs and the targets, the labels. And these bigrams will denote x, y. Those are the characters, right? And so we're given the first character of the bigram, and then we're trying to predict the next one. Both of these are going to be integers. So here we'll take xs.append is just x1, ys.append is just x2. And then here, we actually don't want lists of integers. We will create tensors out of these. So xs is tors.tensor of xs, and ys is tors.tensor of ys. And then we don't actually want to take all the words just yet, because I want everything to be manageable. So let's just do the first word, which is Emma. And then it's clear what these xs and ys would be. Here let me print character1, character2, just so you see what's going on here. So the bigrams of these characters is.emmmma. So this single word, as I mentioned, has one, two, three, four, five examples for our neural network. There are five separate examples in Emma. And those examples are summarized here. When the input to the neural network is integer 0, the desired label is integer 5, which corresponds to e. When the input to the neural network is 5, we want its weights to be arranged so that 13 gets a very high probability. When 13 is put in, we want 13 to have a high probability. When 13 is put in, we also want 1 to have a high probability. When 1 is input, we want 0 to have a very high probability. So there are five separate input examples to a neural net in this dataset. I wanted to add a tangent of a note of caution to be careful with a lot of the APIs of some of these frameworks. You saw me silently use torch.tensor with a lowercase t, and the output looked right. But you should be aware that there's actually two ways of constructing a tensor. There's a torch.lowercase tensor, and there's also a torch.capital tensor class, which you can also construct. So you can actually call both. You can also do torch.capital tensor, and you get an Xs and a Ys as well. So that's not confusing at all. There are threads on what is the difference between these two. And unfortunately, the docs are just not clear on the difference. And when you look at the docs of lowercase tensor, construct tensor with no autograd history by copying data. It's just like it doesn't make sense. So the actual difference, as far as I can tell, is explained eventually in this random thread that you can Google. And really it comes down to, I believe, that... Where is this? Torch.tensor infers the D type, the data type, automatically, while torch.tensor just returns a float tensor. I would recommend to stick to torch.lowercase tensor. So indeed, we see that when I construct this with a capital T, the data type here of Xs is float32. But torch.lowercase tensor, you see how it's now X.Dtype is now integer. So it's advised that you use lowercase T, and you can read more about it if you like in some of these threads. But basically, I'm pointing out some of these things because I want to caution you, and I want you to get used to reading a lot of documentation and reading through a lot of Q&As and threads like this. And some of this stuff is unfortunately not easy and not very well documented, and you have to be careful out there. What we want here is integers, because that's what makes sense. And so lowercase tensor is what we are using. Okay, now we want to think through how we're going to feed in these examples into a neural network. Now, it's not quite as straightforward as plugging it in, because these examples right now are integers. So there's like a 0, 5, or 13. It gives us the index of the character, and you can't just plug an integer index into a neural net. These neural nets are made up of these neurons, and these neurons have weights. And as you saw in micrograd, these weights act multiplicatively on the inputs, wx plus b, there's 10 h's, and so on. And so it doesn't really make sense to make an input neuron take on integer values that you feed in and then multiply on with weights. So instead, a common way of encoding integers is what's called one-hot encoding. In one-hot encoding, we take an integer like 13, and we create a vector that is all zeros, except for the 13th dimension, which we turn to a 1. And then that vector can feed into a neural net. Now conveniently, PyTorch actually has something called the one-hot function inside Torch and in Functional. It takes a tensor made up of integers. Long is an integer. And it also takes a number of classes, which is how large you want your vector to be. So here, let's import Torch.nn.functional as f. This is a common way of importing it. And then let's do f.one-hot, and we feed in the integers that we want to encode. So we can actually feed in the entire array of x's, and we can tell it that numClass is 27. So it doesn't have to try to guess it. It may have guessed that it's only 13 and would give us an incorrect result. So this is the one-hot. Let's call this x.inc for xEncoded. And then we see that xEncoded.shape is 5 by 27. And we can also visualize it, plt.imshow of x.inc, to make it a little bit more clear, because this is a little messy. So we see that we've encoded all the five examples into vectors. We have five examples, so we have five rows, and each row here is now an example into a neural net. And we see that the appropriate bit is turned on as a 1, and everything else is 0. So here, for example, the 0th bit is turned on, the 5th bit is turned on, the 13th bits are turned on for both of these examples, and then the first bit here is turned on. So that's how we can encode integers into vectors, and then these vectors can feed in to neural nets. One more issue to be careful with here, by the way, is let's look at the data type of xEncoding. We always want to be careful with data types. What would you expect xEncoding's data type to be? When we're plugging numbers into neural nets, we don't want them to be integers. We want them to be floating-point numbers that can take on various values. But the Dtype here is actually a 64-bit integer. And the reason for that, I suspect, is that one hot received a 64-bit integer here, and it returned the same data type. And when you look at the signature of one hot, it doesn't even take a desired data type off the output tensor. And so we can't, in a lot of functions in Torch, we'd be able to do something like Dtype equals Torch.float32, which is what we want, but one hot does not support that. So instead, we're going to want to cast this to float like this, so that these, everything is the same, everything looks the same, but the Dtype is float32. And floats can feed into neural nets. So now let's construct our first neuron. This neuron will look at these input vectors. And as you remember from micrograd, these neurons basically perform a very simple function, wx plus b, where wx is a dot product. So we can achieve the same thing here. Let's first define the weights of this neuron, basically. What are the initial weights at initialization for this neuron? Let's initialize them with Torch.random. Torch.random fills a tensor with random numbers drawn from a normal distribution. And a normal distribution has a probability density function like this, and so most of the numbers drawn from this distribution will be around zero, but some of them will be as high as almost three and so on, and very few numbers will be above three in magnitude. So we need to take a size as an input here, and I'm going to use size as to be 27 by 1. So 27 by 1, and then let's visualize w. So w is a column vector of 27 numbers. And these weights are then multiplied by the inputs. So now to perform this multiplication, we can take x encoding and we can multiply it with w. This is a matrix multiplication operator in PyTorch. And the output of this operation is 5 by 1. The reason it's 5 by 5 is the following. We took x encoding, which is 5 by 27, and we multiplied it by 27 by 1. And in matrix multiplication, you see that the output will become 5 by 1 because these 27 will multiply and add. So basically what we're seeing here out of this operation is we are seeing the five activations of this neuron on these five inputs, and we've evaluated all of them in parallel. We didn't feed in just a single input to the single neuron. We fed in simultaneously all the five inputs into the same neuron. And in parallel, PyTorch has evaluated the wx plus b, but here it's just wx. There's no bias. It has evaluated w times x for all of them independently. Now instead of a single neuron, though, I would like to have 27 neurons. And I'll show you in a second why I want 27 neurons. So instead of having just a 1 here, which is indicating this presence of one single neuron, we can use 27. And then when w is 27 by 27, this will in parallel evaluate all the 27 neurons on all the five inputs, giving us a much better, much, much bigger result. So now what we've done is 5 by 27 multiplied 27 by 27, and the output of this is now 5 by 27. So we can see that the shape of this is 5 by 27. So what is every element here telling us? It's telling us for every one of 27 neurons that we created, what is the firing rate of those neurons on every one of those five examples? So the element, for example, 3, 13 is giving us the firing rate of the 13th neuron looking at the third input. And the way this was achieved is by a dot product between the third input and the 13th column of this w matrix here. So using matrix multiplication, we can very efficiently evaluate the dot product between lots of input examples in a batch and lots of neurons where all of those neurons have weights in the columns of those w's. And in matrix multiplication, we're just doing those dot products in parallel. Just to show you that this is the case, we can take xnc and we can take the third row, and we can take the w and take its 13th column, and then we can do xnc at 3, element-wise multiply with w at 13, and sum that up. That's wx plus b. Well, there's no plus b, it's just wx dot product, and that's this number. So you see that this is just being done efficiently by the matrix multiplication operation for all the input examples and for all the output neurons of this first layer. Okay, so we fed our 27 dimensional inputs into a first layer of a neural net that has 27 neurons. So we have 27 inputs and now we have 27 neurons. These neurons perform w times x, they don't have a bias, and they don't have a nonlinearity like tanh. We're going to leave them to be a linear layer. In addition to that, we're not going to have any other layers. This is going to be it. It's just going to be the dumbest, smallest, simplest neural net, which is just a single linear layer. And now I'd like to explain what I want those 27 outputs to be. Intuitively, what we're trying to produce here for every single input example is we're trying to produce some kind of a probability distribution for the next character in a sequence. And there's 27 of them. But we have to come up with precise semantics for exactly how we're going to interpret these 27 numbers that these neurons take on. Now intuitively, you see here that these numbers are negative and some of them are positive, etc. And that's because these are coming out of a neural net layer initialized with these normal distribution parameters. But what we want is we want something like we had here. Like each row here told us the character. counts, and then we normalize the counts to get probabilities. And we want something similar to come out of a neural net. But what we just have right now is just some negative and positive numbers. Now we want those numbers to somehow represent the probabilities for the next character. But you see that probabilities, they have a special structure. They're positive numbers, and they sum to one. And so that doesn't just come out of a neural net. And then they can't be counts, because these counts are positive, and counts are integers. So counts are also not really a good thing to output from a neural net. So instead, what the neural net is going to output, and how we are going to interpret the 27 numbers, is that these 27 numbers are giving us log counts, basically. So instead of giving us counts directly, like in this table, they're giving us log counts. And to get the counts, we're going to take the log counts, and we're going to exponentiate them. Now exponentiation takes the following form. It takes numbers that are negative or they are positive. It takes the entire real line. And then if you plug in negative numbers, you're going to get e to the x, which is always below 1. So you're getting numbers lower than 1. And if you plug in numbers greater than 0, you're getting numbers greater than 1, all the way growing to the infinity. And this here grows to 0. So basically, we're going to take these numbers here, and instead of them being positive and negative and all over the place, we're going to interpret them as log counts. And then we're going to element-wise exponentiate these numbers. Exponentiating them now gives us something like this. And you see that these numbers now, because they went through an exponent, all the negative numbers turned into numbers below 1, like 0.338. And all the positive numbers originally turned into even more positive numbers, sort of greater than 1. So like for example, 7 is some positive number over here. And that is greater than 0. But exponentiated outputs here basically give us something that we can use and interpret as the equivalent of counts originally. So you see these counts here, 112, 7, 51, 1, et cetera. The neural net is kind of now predicting counts. And these counts are positive numbers. They can never be below 0. So that makes sense. And they can now take on various values depending on the settings of W. So let me break this down. We're going to interpret these to be the log counts. In other words for this, that is often used, is so-called logits. These are logits, log counts. And these will be sort of the counts. Logits exponentiate it. And this is equivalent to the n matrix, sort of the n array that we used previously. Remember this was the n? This is the array of counts. And each row here are the counts for the next character, sort of. So those are the counts. And now the probabilities are just the counts normalized. And so I'm not going to find the same, but basically I'm not going to scroll all over the place. We've already done this. We want to counts.sum along the first dimension. And we want to keep dims as true. We went over this. And this is how we normalize the rows of our counts matrix to get our probabilities. So now these are the probabilities. And these are the counts that we have currently. And now when I show the probabilities, you see that every row here, of course, will sum to one because they're normalized. And the shape of this is 5 by 27. And so really what we've achieved is for every one of our five examples, we now have a row that came out of a neural net. And because of the transformations here, we made sure that this output of this neural net now are probabilities, or we can interpret to be probabilities. So our WX here gave us logits. And then we interpret those to be log counts. We exponentiate to get something that looks like counts. And then we normalize those counts to get a probability distribution. And all of these are differentiable operations. So what we've done now is we are taking inputs. We have differentiable operations that we can back propagate through. And we're getting out probability distributions. So for example, for the zeroth example that fed in, which was the zeroth example here was a one-half vector of zero. And it basically corresponded to feeding in this example here. So we're feeding in a dot into a neural net. And the way we fed the dot into a neural net is that we first got its index. Then we one-hot encoded it. Then it went into the neural net. And out came this distribution of probabilities. And its shape is 27. There's 27 numbers. And we're going to interpret this as the neural net's assignment for how likely every one of these characters, the 27 characters, are to come next. And as we tune the weights w, we're going to be, of course, getting different probabilities out for any character that you input. And so now the question is just, can we optimize and find a good w such that the probabilities coming out are pretty good? And the way we measure pretty good is by the loss function. OK, so I organized everything into a single summary so that hopefully it's a bit more clear. So it starts here. We have an input data set. We have some inputs to the neural net. And we have some labels for the correct next character in a sequence. These are integers. Here I'm using tors generators now so that you see the same numbers that I see. And I'm generating 27 neurons' weights. And each neuron here receives 27 inputs. Then here we're going to plug in all the input examples x's into a neural net. So here, this is a forward pass. First, we have to encode all of the inputs into one-hot representations. So we have 27 classes. We pass in these integers. And x inc becomes a array that is 5 by 27. Zeros except for a few ones. We then multiply this in the first layer of a neural net to get logits. Exponentiate the logits to get fake counts, sort of. And normalize these counts to get probabilities. So these last two lines, by the way, here, are called the softmax, which I pulled up here. Softmax is a very often used layer in a neural net that takes these z's, which are logits, exponentiates them, and divides and normalizes. It's a way of taking outputs of a neural net layer. And these outputs can be positive or negative. And it outputs probability distributions. It outputs something that always sums to 1 and are positive numbers, just like probabilities. So it's kind of like a normalization function, if you want to think of it that way. And you can put it on top of any other linear layer inside a neural net. And it basically makes a neural net output probabilities. That's very often used. And we used it as well here. So this is the forward pass. And that's how we made a neural net output probability. Now you'll notice that all of these, this entire forward pass is made up of differentiable layers. Everything here we can back propagate through. And we saw some of the back propagation in micrograd. This is just multiplication and addition. All that's happening here is just multiply and add. And we know how to back propagate through them. Exponentiation, we know how to back propagate through. And then here we are summing. And sum is easily back propagatable as well. And division as well. So everything here is a differentiable operation. And we can back propagate through. Now we achieve these probabilities, which are 5 by 27. For every single example, we have a vector of probabilities that sum to 1. And then here I wrote a bunch of stuff to sort of like break down the examples. So we have five examples making up Emma, right? And there are five bigrams inside Emma. So bigram example 1 is that E is the beginning character, right after dot. And the indexes for these are 0 and 5. So then we feed in a 0. That's the input to the neural net. We get probabilities from the neural net that are 27 numbers. And then the label is 5, because E actually comes after dot. So that's the label. And then we use this label 5 to index into the probability distribution here. So this index 5 here is 0, 1, 2, 3, 4, 5. It's this number here, which is here. So that's basically the probability assigned by the neural net to the actual correct character. You see that the network currently thinks that this next character, that E following dot, is only 1% likely, which is, of course, not very good, right? Because this actually is a training example. And the network thinks that this is currently very, very unlikely. But that's just because we didn't get very lucky in generating a good setting of W. So right now this network thinks this is unlikely. And 0.01 is not a good outcome. So the log likelihood, then, is very negative. And the negative log likelihood is very positive. And so 4 is a very high negative log likelihood. And that means we're going to have a high loss. Because what is the loss? The loss is just the average negative log likelihood. So the second character is EM. And you see here that also the network thought that M following E is very unlikely, 1%. For M following M, it thought it was 2%. And for A following M, it actually thought it was 7% likely. So just by chance, this one actually has a pretty good probability, and therefore a pretty low negative log likelihood. And finally here, it thought this was 1% likely. So overall, our average negative log likelihood, which is the loss, the total loss that summarizes basically how well this network currently works, at least on this one word, not on the full data set, just the one word, is 3.76, which is actually a fairly high loss. This is not a very good setting of W's. Now here's what we can do. We're currently getting 3.76. We can actually come here and we can change our W. We can resample it. So let me just add one to have a different seed. And then we get a different W. And then we can rerun this. And with this different setting of W's, we now get 3.37. So this is a much better W, right? And it's better because the probabilities just happen to come out higher for the characters that actually are next. And so you can imagine actually just resampling this. We can try 2. So okay, this was not very good. Let's try one more. We can try 3. Okay, this was a terrible setting because we have a very high loss. So anyway, I'm going to erase this. What I'm doing here, which is just guess and check of randomly assigning parameters and seeing if the network is good, that is amateur hour. That's not how you optimize in neural net. The way you optimize neural net is you start with some random guess, and we're going to commit to this one, even though it's not very good. But now the big deal is we have a loss function. So this loss is made up only of differentiable operations. And we can minimize the loss by tuning Ws by computing the gradients of the loss with respect to these W matrices. And so then we can tune W to minimize the loss and find a good setting of W using gradient-based optimization. So let's see how that will work. Now things are actually going to look almost identical to what we had with micrograd. So here I pulled up the lecture from micrograd, the notebook that's from this repository. And when I scroll all the way to the end where we left off with micrograd, we had something very, very similar. We had a number of input examples. In this case, we had four input examples inside Xs. And we had their targets, desired targets. Just like here, we have our Xs now, but we have five of them. And they're now integers instead of vectors. But we're going to convert our integers to vectors, except our vectors will be 27 large instead of three large. And then here what we did is first we did a forward pass where we ran a neural net on all of the inputs to get predictions. Our neural net at the time, this N of X, was a multilayer perceptron. Our neural net is going to look different because our neural net is just a single layer, single linear layer followed by a softmax. So that's our neural net. And the loss here was the mean squared error. So we simply subtracted the prediction from the ground truth and squared it and summed it all up. And that was the loss. And loss was the single number that summarized the quality of the neural net. And when loss is low, like almost zero, that means the neural net is predicting correctly. So we had a single number that summarized the performance of the neural net. And everything here was differentiable and was stored in a massive compute graph. And then we iterated over all the parameters. We made sure that the gradients are set to zero. And we called loss.backward. And loss.backward initiated backpropagation at the final output node of loss. So remember these expressions? We had loss all the way at the end. We start backpropagation and we went all the way back. And we made sure that we populated all the parameters dot grad. So dot grad started at zero, but backpropagation filled it in. And then in the update, we iterated over all the parameters. And we simply did a parameter update where every single element of our parameters was nudged in the opposite direction of the gradient. And so we're going to do the exact same thing here. So I'm going to pull this up on the side here. So that we have it available. And we're actually going to do the exact same thing. So this was the forward pass. So where we did this. And probs is our wipe read. So now we have to evaluate the loss, but we're not using the mean squared error. We're using the negative log likelihood because we are doing classification. We're not doing regression as it's called. So here we want to calculate loss. Now the way we calculate it is just this average negative log likelihood. Now this probs here has a shape of five by 27. And so to get all the, we basically want to pluck out the probabilities at the correct indices here. So in particular, because the labels are stored here in the array wise, basically what we're after is for the first example, we're looking at probability of five, right? At index five. For the second example, at the second row or row index one, we are interested in the probability assigned to index 13. At the second example, we also have 13. At the third row, we want one. And at the last row, which is four, we want zero. So these are the probabilities we're interested in, right? And you can see that they're not amazing as we saw above. So these are the probabilities we want, but we want like a more efficient way to access these probabilities. Not just listing them out in a tuple like this. So it turns out that the way to do this in PyTorch, one of the ways at least, is we can basically pass in all of these, sorry about that, all of these integers in a vectors. So these ones, you see how they're just 0, 1, 2, 3, 4. We can actually create that using mp, not mp, sorry, torch.arrange of five, 0, 1, 2, 3, 4. So we index here with torch.arrange of five. And here we index with y's. And you see that that gives us exactly these numbers. So that plucks out the probabilities that the neural network assigns to the correct next character. Now we take those probabilities and we actually look at the log probability. So we want to dot log. And then we want to just average that up. So take the mean of all of that. And then it's the negative average log likelihood that is the loss. So the loss here is 3.7 something. And you see that this loss, 3.76, 3.76 is exactly as we've obtained before. But this is a vectorized form of that expression. So we get the same loss. And the same loss we can consider sort of as part of this forward pass. And we've achieved here now loss. Okay, so we made our way all the way to loss. We've defined the forward pass. We've forwarded the network and the loss. Now we're ready to do the backward pass. So backward pass. We want to first make sure that all the gradients are reset. So they're at zero. Now in PyTorch, you can set the gradients to be zero, but you can also just set it to none. And setting it to none is more efficient. And PyTorch will interpret none as like a lack of a gradient and is the same as zeros. So this is a way to set to zero, the gradient. And now we do loss.backward. Before we do loss.backward, we need one more thing. If you remember from micro grad, PyTorch actually requires that we pass in requires grad is true so that we tell PyTorch that we are interested in calculating gradient for this leaf tensor. By default, this is false. So let me recalculate with that. And then set to none and loss.backward. Now something magical happened when loss.backward was run. Because PyTorch, just like micro grad, when we did the forward pass here, it keeps track of all the operations under the hood. It builds a full computational graph. Just like the graphs we produced in micro grad, those graphs exist inside PyTorch. And so it knows all the dependencies and all the mathematical operations of everything. And when you then calculate the loss, we can call a dot backward on it. And dot backward then fills in the gradients of all the intermediates all the way back to W's, which are the parameters of our neural net. So now we can do wlgrad and we see that it has structure. There's stuff inside it. And these gradients, every single element here, so w.shape is 27 by 27. wgradshape is the same, 27 by 27. And every element of w.grad is telling us the influence of that weight on the loss function. So for example, this number all the way here, if this element, the 0, 0 element of W, because the gradient is positive, it's telling us that this has a positive influence on the loss. Slightly nudging W, slightly taking W0, 0 and adding a small h to it would increase the loss mildly because this gradient is positive. Some of these gradients are also negative. So that's telling us about the gradient information. And we can use this gradient information to update the weights of this neural network. So let's now do the update. It's going to be very similar to what we had in micrograd. We need no loop over all the parameters because we only have one parameter tensor, and that is W. So we simply do W.data plus equals. We can actually copy this almost exactly, negative 0.1 times W.grad. And that would be the update to the tensor. So that updates the tensor. And because the tensor is updated, we would expect that now the loss should decrease. So here, if I print loss.item, it was 3.76, right? So we've updated the W here. So if I recalculate forward pass, loss now should be slightly lower. So 3.76 goes to 3.74. And then we can again set grad to none and backward, update. And now the parameter's changed again. So if we recalculate the forward pass, we expect a lower loss again, 3.72. And this is again doing the, we're now doing gradient descent. And when we achieve a low loss, that will mean that the network is assigning high probabilities to the correct next characters. OK, so I rearranged everything and I put it all together from scratch. So here is where we construct our data set of bigrams. You see that we are still iterating only over the first word, Emma. I'm going to change that in a second. I added a number that counts the number of elements in X's so that we explicitly see that number of examples is 5. Because currently we're just working with Emma. There's five bigrams there. And here I added a loop of exactly what we had before. So we had 10 iterations of gradient descent of forward pass, backward pass, and an update. And so running these two cells, initialization and gradient descent, gives us some improvement on the loss function. But now I want to use all the words. And there's not five, but 228,000 bigrams now. However, this should require no modification whatsoever. Everything should just run because all the code we wrote doesn't care if there's five bigrams or 228,000 bigrams. And with everything, it should just work. So you see that this will just run. But now we are optimizing over the entire training set of all the bigrams. And you see now that we are decreasing very slightly. So actually, we can probably afford a larger learning rate. We can probably afford an even larger learning rate. Even 50 seems to work on this very, very simple example. So let me reinitialize. And let's run 100 iterations. See what happens. We seem to be coming up to some pretty good losses here. 2.47. Let me run 100 more. What is the number that we expect, by the way, in the loss? We expect to get something around what we had originally, actually. So all the way back, if you remember, in the beginning of this video, when we optimized just by counting, our loss was roughly 2.47 after we added smoothing. But before smoothing, we had roughly 2.45 likelihood. Sorry, loss. And so that's actually roughly the vicinity of what we expect to achieve. But before, we achieved it by counting. And here we are achieving roughly the same result, but with gradient-based optimization. So we come to about 2.46, 2.45, et cetera. And that makes sense because fundamentally, we're not taking in any additional information. We're still just taking in the previous character and trying to predict the next one. But instead of doing it explicitly by counting and normalizing, we are doing it with gradient-based learning. And it just so happens that the explicit approach happens to very well optimize the loss function without any need for gradient-based optimization because the setup for bigram language models is so straightforward, it's so simple, we can just afford to estimate those probabilities directly and maintain them in a table. But the gradient-based approach is significantly more flexible. So we've actually gained a lot because what we can do now is we can expand this approach and complexify the neural net. So currently, we're just taking a single character and feeding into a neural net, and the neural net is extremely simple. But we're about to iterate on this substantially. We're going to be taking multiple previous characters, and we're going to be feeding them into increasingly more complex neural nets. But fundamentally, the output of the neural net will always just be logits. And those logits will go through the exact same transformation. We are going to take them through a softmax, calculate the loss function and the negative log-likelihood, and do gradient-based optimization. And so actually, as we complexify the neural nets and work all the way up to transformers, none of this will really fundamentally change. None of this will fundamentally change. The only thing that will change is the way we do the forward pass, where we've taken some previous characters and calculated logits for the next character in the sequence. That will become more complex, but we'll use the same machinery to optimize it. And it's not obvious how we would have extended this bigram approach into the case where there are many more characters at the input, because eventually these tables would get way too large, because there's way too many combinations of what previous characters could be. If you only have one previous character, we can just keep everything in a table that counts. But if you have the last 10 characters that are input, we can't actually keep everything in a table anymore. So this is fundamentally an unscalable approach, and the neural network approach is significantly more scalable, and it's something that actually we can improve on over time. So that's where we will be digging next. I wanted to point out two more things. Number one, I want you to notice that this xnk here, this is made up of one-hot vectors, and then those one-hot vectors are multiplied by this w matrix. And we think of this as multiple neurons being forwarded in a fully connected manner. But actually what's happening here is that, for example, if you have a one-hot vector here that has a 1 at, say, the fifth dimension, then because of the way the matrix multiplication works, multiplying that one-hot vector with w actually ends up plucking out the fifth row of w. Logits would become just the fifth row of w, and that's because of the way the matrix multiplication works. So that's actually what ends up happening. But that's actually exactly what happened before, because remember, all the way up here, we have a bigram, we took the first character, and then that first character indexed into a row of this array here. And that row gave us the probability distribution for the next character. So the first character was used as a lookup into a matrix here to get the probability distribution. Well, that's actually exactly what's happening here, because we're taking the index, we're encoding it as one-hot, and multiplying it by w. So logits literally becomes the appropriate row of w, and that gets, just as before, exponentiated to create the counts, and then normalized and becomes probability. So this w here is literally the same as this array here. But w, remember, is the log counts, not the counts. So it's more precise to say that w exponentiated, w.exp, is this array. But this array was filled in by counting, and by basically populating the counts of bigrams, whereas in the gradient-based framework, we initialize it randomly, and then we let the laws guide us to arrive at the exact same array. So this array exactly here is basically the array w at the end of optimization, except we arrived at it piece by piece by following the laws. And that's why we also obtain the same loss function at the end. And the second note is, if I come here, remember the smoothing where we added fake counts to our counts in order to smooth out and make more uniform the distributions of these probabilities. And that prevented us from assigning zero probability to any one bigram. Now, if I increase the count here, what's happening to the probability? As I increase the count, probability becomes more and more uniform, because these counts go only up to like 900 or whatever. So if I'm adding plus a million to every single number here, you can see how the row and its probability then when we divide is just going to become more and more close to exactly even probability, uniform distribution. It turns out that the gradient-based framework has an equivalent to smoothing. In particular, think through these w's here, which we initialized randomly. We could also think about initializing w's to be zero. If all the entries of w are zero, then you'll see that logits will become all zero, and then exponentiating those logits becomes all one, and then the probabilities turn out to be exactly uniform. So basically, when w's are all equal to each other, or say especially zero, then the probabilities come out completely uniform. So trying to incentivize w to be near zero is basically equivalent to label smoothing. And the more you incentivize that in a loss function, the more smooth distribution you're going to achieve. So this brings us to something that's called regularization, where we can actually augment the loss function to have a small component that we call a regularization loss. In particular, what we're going to do is we can take w, and we can, for example, square all of its entries. And then we can take all the entries of w, and we can sum them. And because we're squaring, there will be no signs anymore. Negatives and positives all get squashed to be positive numbers. And then the way this works is you achieve zero loss if w is exactly or zero. But if w has non-zero numbers, you accumulate loss. And so we can actually take this, and we can add it on here. So we can do something like loss plus w square dot sum. Or let's actually, instead of sum, let's take a mean, because otherwise the sum gets too large. So mean is a little bit more manageable. And then we have a regularization loss here, like say 0.01 times, or something like that. You can choose the regularization strength. And then we can just optimize this. And now this optimization actually has two components. Not only is it trying to make all the probabilities work out, but in addition to that, there's an additional component that simultaneously tries to make all w's be zero. Because if w's are non-zero, you feel a loss. And so minimizing this, the only way to achieve that is for w to be zero. And so you can think of this as adding like a spring force, or like a gravity force that pushes w to be zero. So w wants to be zero, and the probabilities want to be uniform. But they also simultaneously want to match up your probabilities, as indicated by the data. And so the strength of this regularization is exactly controlling the amount of counts that you add here. Adding a lot more counts here corresponds to increasing this number. Because the more you increase it, the more this part of the loss function dominates this part. And the more these weights will be unable to grow, because as they grow, they accumulate way too much loss. And so if this is strong enough, then we are not able to overcome the force of this loss. And basically everything will be uniform predictions. So I thought that's kind of cool. Okay, and lastly, before we wrap up, I wanted to show you how you would sample from this neural net model. And I copy-pasted the sampling code from before. Remember that we sampled five times, and all we did was we started at zero, we grabbed the current ix row of p. And that was our probability row, from which we sampled the next index, and just accumulated that, and a break when zero. And running this gave us these results. I still have the p in memory, so this is fine. Now, this p doesn't come from the row of p. Instead, it comes from this neural net. First, we take ix, and we encode it into a one-hot row of xn. This xn multiplies our w, which really just plucks out the row of w corresponding to ix. Really, that's what's happening. And that gets our logits, and then we normalize those logits. Exponentiate to get counts, and then normalize to get the distribution. And then we can sample from the distribution. So if I run this, kind of anticlimactic or climatic, depending how you look at it, but we get the exact same result. And that's because this is the identical model. Not only does it achieve the same loss, but as I mentioned, these are identical models, and this w is the log counts of what we've estimated before. But we came to this answer in a very different way, and it's got a very different interpretation. But fundamentally, this is basically the same model and gives the same samples here. And so that's kind of cool. Okay, so we've actually covered a lot of ground. We introduced the bigram character-level language model. We saw how we can train the model, how we can sample from the model, and how we can evaluate the quality of the model using the negative log likelihood loss. And then we actually trained the model in two completely different ways that actually give the same result and the same model. In the first way, we just counted up the frequency of all the bigrams and normalized. In the second way, we used the negative log likelihood loss as a guide to optimizing the counts matrix or the counts array so that the loss is minimized in a gradient-based framework. And we saw that both of them give the same result, and that's it. Now, the second one of these, the gradient-based framework, is much more flexible. And right now, our neural network is super simple. We're taking a single previous character, and we're taking it through a single linear layer to calculate the logits. This is about to complexify. So in the follow-up videos, we're going to be taking more and more of these characters, and we're going to be feeding them into a neural net. But this neural net will still output the exact same thing. The neural net will output logits. And these logits will still be normalized in the exact same way, and all the loss and everything else in the gradient-based framework, everything stays identical. It's just that this neural net will now complexify all the way to transformers. So that's going to be pretty awesome, and I'm looking forward to it. For now, bye. Hi everyone. Today we are continuing our implementation of Makemore. Now, in the last lecture, we implemented the bigram language model, and we implemented it both using counts and also using a super simple neural network that has a single linear layer. Now, this is the Jupyter Notebook that we built out last lecture, and we saw that the way we approached this is that we looked at only the single previous character, and we predicted the distribution for the character that would go next in the sequence. And we did that by taking counts and normalizing them into probabilities so that each row here sums to 1. Now, this is all well and good if you only have one character of previous context. And this works, and it's approachable. The problem with this model, of course, is that the predictions from this model are not very good because you only take one character of context. So the model didn't produce very name-like sounding things. Now, the problem with this approach, though, is that if we are to take more context into account when predicting the next character in a sequence, things quickly blow up. And this table, the size of this table, grows, and in fact it grows exponentially with the length of the context. Because if we only take a single character at a time, that's 27 possibilities of context. But if we take two characters in the pass and try to predict the third one, suddenly the number of rows in this matrix, you can look at it that way, is 27 times 27. So there's 729 possibilities for what could have come in the context. If we take three characters as the context, suddenly we have 20,000 possibilities of context. And so that's just way too many rows of this matrix. It's way too few counts for each possibility. And the whole thing just kind of explodes and doesn't work very well. So that's why today we're going to move on to this bullet point here. And we're going to implement a multilayer perceptron model to predict the next character in a sequence. And this modeling approach that we're going to adopt follows this paper, Ben-Ju et al., 2003. So I have the paper pulled up here. Now, this isn't the very first paper that proposed the use of multilayer perceptrons or neural networks to predict the next character or token in a sequence. But it's definitely one that was very influential around that time. It is very often cited to stand in for this idea. And I think it's a very nice write-up. And so this is the paper that we're going to first look at and then implement. Now, this paper has 19 pages, so we don't have time to go into the full detail of this paper. But I invite you to read it. It's very readable, interesting, and has a lot of interesting ideas in it as well. In the introduction, they described the exact same problem I just described. And then to address it, they proposed the following model. Now, keep in mind that we are building a character-level language model. So we're working on the level of characters. In this paper, they have a vocabulary of 17,000 possible words, and they instead build a word-level language model. But we're going to still stick with the characters, but we'll take the same modeling approach. Now, what they do is basically they propose to take every one of these words, 17,000 words, and they're going to associate to each word a, say, 30-dimensional feature vector. So every word is now embedded into a 30-dimensional space. You can think of it that way. So we have 17,000 points or vectors in a 30-dimensional space, and you might imagine that's very crowded. That's a lot of points for a very small space. Now, in the beginning, these words are initialized completely randomly, so they're spread out at random. But then we're going to tune these embeddings of these words using backpropagation. So during the course of training of this neural network, these points or vectors are going to basically move around in this space. And you might imagine that, for example, words that have very similar meanings or that are indeed synonyms of each other might end up in a very similar part of the space. And conversely, words that mean very different things would go somewhere else in the space. Now, their modeling approach otherwise is identical to ours. They are using a multilayer neural network to predict the next word, given the previous words, and to train the neural network, they are maximizing the log likelihood of the training data, just like we did. So the modeling approach itself is identical. Now, here they have a concrete example of this intuition. Why does it work? Basically, suppose that, for example, you are trying to predict a dog was running in a blank. Now, suppose that the exact phrase, a dog was running in a, has never occurred in the training data. And here you are at sort of test time later, when the model is deployed somewhere, and it's trying to make a sentence, and it's saying, a dog was running in a blank. And because it's never encountered this exact phrase in the training set, you're out of distribution, as we say. Like, you don't have fundamentally any reason to suspect what might come next. But this approach actually allows you to get around that, because maybe you didn't see the exact phrase, a dog was running in a something, and maybe you've seen similar phrases. Maybe you've seen the phrase, the dog was running in a blank. And maybe your network has learned that a and the are, like, frequently are interchangeable with each other. And so maybe it took the embedding for a and the embedding for the, and it actually put them, like, nearby each other in the space. And so you can transfer knowledge through that embedding, and you can generalize in that way. Similarly, the network could know that cats and dogs are animals, and they co-occur in lots of very similar contexts. And so even though you haven't seen this exact phrase, or you haven't seen exactly walking or running, you can, through the embedding space, transfer knowledge, and you can generalize to novel scenarios. So let's now scroll down to the diagram of the neural network. They have a nice diagram here. And in this example, we are taking three previous words, and we are trying to predict the fourth word in a sequence. Now, these three previous words, as I mentioned, we have a vocabulary of 17,000 possible words. So every one of these basically are the index of the incoming word. And because there are 17,000 words, this is an integer between 0 and 16,999. Now, there's also a lookup table that they call C. This lookup table is a matrix that is 17,000 by, say, 30. And basically what we're doing here is we're treating this as a lookup table. And so every index is plucking out a row of this embedding matrix so that each index is converted to the 30-dimensional vector that corresponds to the embedding vector for that word. So here we have the input layer of 30 neurons for three words, making up 90 neurons in total. And here they're saying that this matrix C is shared across all the words. So we're always indexing into the same matrix C over and over for each one of these words. Next up is the hidden layer of this neural network. The size of this hidden neural layer, this neural net, is a hyperparameter. So we use the word hyperparameter when it's kind of like a design choice up to the designer of the neural net. And this can be as large as you'd like or as small as you'd like. So, for example, the size could be 100. And we are going to go over multiple choices of the size of this hidden layer, and we're going to evaluate how well they work. So say there were 100 neurons here. All of them would be fully connected to the 90 words or 90 numbers that make up these three words. So this is a fully connected layer. Then there's a 10-inch-long linearity. And then there's this output layer. And because there are 17,000 possible words that could come next, this layer has 17,000 neurons, and all of them are fully connected to all of these neurons in the hidden layer. So there's a lot of parameters here because there's a lot of words. So most computation is here. This is the expensive layer. Now, there are 17,000 logits here. So on top of there, we have the softmax layer, which we've seen in our previous video as well. So every one of these logits is exponentiated, and then everything is normalized to sum to 1 and we have a nice probability distribution for the next word in the sequence. Now, of course, during training, we actually have the label. We have the identity of the next word in the sequence. That word or its index is used to pluck out the probability of that word, and then we are maximizing the probability of that word with respect to the parameters of this neural net. So the parameters are the weights and biases of this output layer. The weights and biases of this hidden layer, and the embedding lookup table C, and all of that is optimized using backpropagation. And these dashed arrows, ignore those. That represents a variation of a neural net that we are not going to explore in this video. So that's the setup, and now let's implement it. Okay, so I started a brand new notebook for this lecture. We are importing PyTorch, and we are importing Matplotlib so we can create figures. Then I am reading all the names into a list of words like I did before, and I'm showing the first eight right here. Keep in mind that we have 32,000 in total. These are just the first eight. And then here I'm building out the vocabulary of characters and all the mappings from the characters as strings to integers and vice versa. Now, the first thing we want to do is we want to compile the dataset for the neural network, and I had to rewrite this code. I'll show you in a second what it looks like. So this is the code that I created for the dataset creation. So let me first run it, and then I'll briefly explain how this works. So first we're going to define something called block size, and this is basically the context length of how many characters do we take to predict the next one. So here in this example, we're taking three characters to predict the fourth one, so we have a block size of three. That's the size of the block that supports the prediction. Then here I'm building out the x and y. The x are the input to the neural net, and the y are the labels for each example inside x. Then I'm iterating over the first five words. I'm doing the first five just for efficiency while we are developing all the code, but then later we're going to come here and erase this so that we use the entire training set. So here I'm printing the word, Emma, and here I'm basically showing the examples that we can generate, the five examples that we can generate out of the single word, Emma. So when we are given the context of just dot, dot, dot, the first character in a sequence is E. In this context, the label is M. When the context is this, the label is M, and so forth. So the way I build this out is first I start with a padded context of just zero tokens. Then I iterate over all the characters. I get the character in the sequence, and I basically build out the array y of this current character, and the array x, which stores the current running context. Then here, see, I print everything, and here I crop the context and enter the new character in the sequence. So this is kind of like a rolling window of context. Now we can change the block size here to, for example, four, and in that case we would be predicting the fifth character in the previous four, or it can be five, and then it would look like this. Or it can be, say, ten, and then it would look something like this. We're taking ten characters to predict the eleventh one, and we're always padding with dots. So let me bring this back to three just so that we have what we have here in the paper. And finally, the data set right now looks as follows. From these five words, we have created a data set of 32 examples, and each input to the neural net is three integers, and we have a label that is also an integer, y. So x looks like this. These are the individual examples, and then y are the labels. So given this, let's now write a neural network that takes these x's and predicts the y's. First, let's build the embedding lookup table C. So we have 27 possible characters, and we're going to embed them in a lower-dimensional space. In the paper, they have 17,000 words, and they embed them in spaces as small-dimensional as 30. So they cram 17,000 words into 30-dimensional space. In our case, we have only 27 possible characters, so let's cram them in something as small as, to start with, for example, a two-dimensional space. So this lookup table will be random numbers, and we'll have 27 rows, and we'll have two columns. So each one of 27 characters will have a two-dimensional embedding. So that's our matrix C of embeddings, in the beginning, initialized randomly. Now, before we embed all of the integers inside the input x using this lookup table C, let me actually just try to embed a single individual integer, like, say, 5, so we get a sense of how this works. Now, one way this works, of course, is we can just take the C, and we can index into row 5, and that gives us a vector, the fifth row of C. And this is one way to do it. The other way that I presented in the previous lecture is actually seemingly different, but actually identical. So in the previous lecture, what we did is we took these integers, and we used the one-hot encoding to first encode them. So f.oneHot, we want to encode integer 5, and we want to tell it that the number of classes is 27. So that's the 26-dimensional vector of all zeros, except the fifth bit is turned on. Now, this actually doesn't work. The reason is that this input actually must be a torsdot tensor. And I'm making some of these errors intentionally, just so you get to see some errors and how to fix them. So this must be a tensor, not an int. Fairly straightforward to fix. We get a one-hot vector. The fifth dimension is 1, and the shape of this is 27. And now notice that, just as I briefly alluded to in a previous video, if we take this one-hot vector and we multiply it by C, then what would you expect? Well, number one, first you'd expect an error, because expected scalar type long, but found float. So a little bit confusing, but the problem here is that one-hot, the data type of it, is long. It's a 64-bit integer, but this is a float tensor. And so PyTorch doesn't know how to multiply an int with a float, and that's why we had to explicitly cast this to a float, so that we can multiply. Now, the output actually here is identical. And it's identical because of the way the matrix multiplication here works. We have the one-hot vector multiplying columns of C, and because of all the zeros, they actually end up masking out everything in C except for the fifth row, which is plucked out. And so we actually arrive at the same result. And that tells you that here we can interpret this first piece here, this embedding of the integer, we can either think of it as the integer indexing into lookup table C, but equivalently we can also think of this little piece here as a first layer of this bigger neural net. This layer here has neurons that have no nonlinearity. There's no tanh. They're just linear neurons. And their weight matrix is C. And then we are encoding integers into one-hot and feeding those into a neural net. And this first layer basically embeds them. Those are two equivalent ways of doing the same thing. We're just going to index because it's much, much faster, and we're going to discard this interpretation of one-hot inputs into neural nets, and we're just going to index integers and use embedding tables. Now, embedding a single integer like 5 is easy enough. We can simply ask PyTorch to retrieve the fifth row of C, or the row index 5 of C. But how do we simultaneously embed all of these 32 by 3 integers stored in array X? Luckily, PyTorch indexing is fairly flexible and quite powerful. So it doesn't just work to ask for a single element 5 like this. You can actually index using lists. So, for example, we can get the rows 5, 6, and 7, and this will just work like this. We can index with a list. It doesn't just have to be a list. It can also be actually a tensor of integers. And we can index with that. So this is an integer tensor of 5, 6, 7, and this will just work as well. In fact, we can also, for example, repeat row 7 and retrieve it multiple times, and that same index will just get embedded multiple times here. So here we are indexing with a one-dimensional tensor of integers, but it turns out that you can also index with multi-dimensional tensors of integers. Here we have a two-dimensional tensor of integers. So we can simply just do C at x, and this just works. And the shape of this is 32 by 3, which is the original shape. And now for every one of those 32 by 3 integers, we've retrieved the embedding vector here. So basically we have that as an example. The 13th or example index 13, the second dimension, is the integer 1, as an example. And so here, if we do C of x, which gives us that array, and then we index into 13 by 2 of that array, then we get the embedding here. And you can verify that C at 1, which is the integer at that location, is indeed equal to this. You see they're equal. So basically, long story short, PyTorch indexing is awesome, and to embed simultaneously all of the integers in x, we can simply do C of x, and that is our embedding, and that just works. Now let's construct this layer here, the hidden layer. So we have that w1, as I'll call it, are these weights, which we will initialize randomly. Now the number of inputs to this layer is going to be 3 times 2, right? Because we have two-dimensional embeddings, and we have three of them. So the number of inputs is 6, and the number of neurons in this layer is a variable up to us. Let's use 100 neurons as an example. And then biases will be also initialized randomly, as an example, and we just need 100 of them. Now the problem with this is we can't simply... Normally we would take the input, in this case that's embedding, and we'd like to multiply it with these weights, and then we would like to add the bias. This is roughly what we want to do. But the problem here is that these embeddings are stacked up in the dimensions of this input tensor. So this will not work, this matrix multiplication, because this is a shape 32 by 3 by 2, and I can't multiply that by 6 by 100. So somehow we need to concatenate these inputs here together so that we can do something along these lines, which currently does not work. So how do we transform this 32 by 3 by 2 into a 32 by 6 so that we can actually perform this multiplication over here? I'd like to show you that there are usually many ways of implementing what you'd like to do in Torus. and some of them will be faster, better, shorter, etc. And that's because Torch is a very large library and it's got lots and lots of functions. So if we just go to the documentation and click on Torch, you'll see that my slider here is very tiny and that's because there are so many functions that you can call on these tensors to transform them, create them, multiply them, add them, perform all kinds of different operations on them. And so this is kind of like the space of possibility, if you will. Now, one of the things that you can do is if we can control here, Ctrl-F for concatenate, and we see that there's a function, Torch.cat, short for concatenate. And this concatenates the given sequence of tensors in a given dimension. And these tensors must have the same shape, etc. So we can use the concatenate operation to, in a naive way, concatenate these three embeddings for each input. So in this case, we have m of the shape. And really what we want to do is we want to retrieve these three parts and concatenate them. So we want to grab all the examples. We want to grab first the 0th index and then all of this. And so this plucks out the 32x2 embeddings of just the first word here. And so basically we want this guy, we want the first dimension, and we want the second dimension. And these are the three pieces individually. And then we want to treat this as a sequence, and we want to Torch.cat on that sequence. So this is the list. Torch.cat takes a sequence of tensors, and then we have to tell it along which dimension to concatenate. So in this case, all of these are 32x2, and we want to concatenate not across dimension 0 but across dimension 1. So passing in 1 gives us the result that the shape of this is 32x6, exactly as we'd like. So that basically took 32 and squashed these by concatenating them into 32x6. Now this is kind of ugly because this code would not generalize if we want to later change the block size. Right now we have three inputs, three words. But what if we had five? Then here we would have to change the code because I'm indexing directly. Well, Torch comes to rescue again because there turns out to be a function called unbind, and it removes a tensor dimension. So it removes a tensor dimension, and it returns a tuple of all slices along a given dimension without it. So this is exactly what we need. And basically when we call Torch.unbind of m and pass in dimension 1, index 1, this gives us a list of tensors exactly equivalent to this. So running this gives us a lang 3, and it's exactly this list. So we can call Torch.cat on it and along the first dimension. And this works, and this shape is the same. But now it doesn't matter if we have block size 3 or 5 or 10. This will just work. So this is one way to do it. But it turns out that in this case, there's actually a significantly better and more efficient way. This gives me an opportunity to hint at some of the internals of Torch.tensor. So let's create an array here of elements from 0 to 17, and the shape of this is just 18. It's a single vector of 18 numbers. It turns out that we can very quickly re-represent this as different-sized n-dimensional tensors. We do this by calling a view, and we can say that actually this is not a single vector of 18. This is a 2x9 tensor, or alternatively, this is a 9x2 tensor, or this is actually a 3x3x2 tensor. As long as the total number of elements here multiply to be the same, this will just work. And in PyTorch, this operation, calling that view, is extremely efficient. And the reason for that is that in each tensor, there's something called the underlying storage. And the storage is just the numbers, always as a one-dimensional vector. And this is how this tensor is represented in the computer memory. It's always a one-dimensional vector. But when we call that view, we are manipulating some of the attributes of that tensor that dictate how this one-dimensional sequence is interpreted to be an n-dimensional tensor. And so what's happening here is that no memory is being changed, copied, moved, or created when we call that view. The storage is identical. But when you call that view, some of the internal attributes of the view of this tensor are being manipulated and changed. In particular, there's something called storage offset, strides, and shapes. And those are manipulated so that this one-dimensional sequence of bytes is seen as different n-dimensional arrays. There's a blog post here from Eric called PyTorch Internals where he goes into some of this with respect to tensor and how the view of a tensor is represented. And this is really just like a logical construct of representing the physical memory. And so this is a pretty good blog post that you can go into. I might also create an entire video on the internals of TorchTensor and how this works. For here, we just note that this is an extremely efficient operation. And if I delete this and come back to our emb, we see that the shape of our emb is 32 by 3 by 2. But we can simply ask for PyTorch to view this instead as a 32 by 6. And the way this gets flattened into a 32 by 6 array just happens that these two get stacked up in a single row. And so that's basically the concatenation operation that we're after. And you can verify that this actually gives the exact same result as what we had before. So this is an element y equals, and you can see that all the elements of these two tensors are the same. And so we get the exact same result. So long story short, we can actually just come here. And if we just view this as a 32 by 6 instead, then this multiplication will work and give us the hidden states that we're after. So if this is h, then h-shaped is now the 100-dimensional activations for every one of our 32 examples. And this gives the desired result. Let me do two things here. Number one, let's not use 32. We can, for example, do something like emb.shape at 0 so that we don't hard-code these numbers, and this would work for any size of this emb. Or alternatively, we can also do negative 1. When we do negative 1, PyTorch will infer what this should be. Because the number of elements must be the same, and we're saying that this is 6, PyTorch will derive that this must be 32, or whatever else it is if emb is of different size. The other thing is here, one more thing I'd like to point out is here when we do the concatenation, this actually is much less efficient because this concatenation would create a whole new tensor with a whole new storage, so new memory is being created because there's no way to concatenate tensors just by manipulating the view attributes. So this is inefficient and creates all kinds of new memory. So let me delete this now. We don't need this. And here to calculate h, we want to also dot 10h of this to get our h. So these are now numbers between negative 1 and 1 because of the 10h, and we have that the shape is 32 by 100, and that is basically this hidden layer of activations here for every one of our 32 examples. Now there's one more thing I've lost over that we have to be very careful with, and that's this plus here. In particular, we want to make sure that the broadcasting will do what we like. The shape of this is 32 by 100, and b1's shape is 100. So we see that the addition here will broadcast these two, and in particular, we have 32 by 100 broadcasting to 100. So broadcasting will align on the right, create a fake dimension here, so this will become a 1 by 100 row vector, and then it will copy vertically for every one of these rows of 32 and do an element-wise addition. So in this case, the correct thing will be happening because the same bias vector will be added to all the rows of this matrix. So that is correct. That's what we'd like. And it's always good practice to just make sure so that you don't shoot yourself in the foot. And finally, let's create the final layer here. So let's create w2 and b2. The input now is 100, and the output number of neurons will be for us 27 because we have 27 possible characters that come next. So the biases will be 27 as well. So therefore, the logits, which are the outputs of this neural net, are going to be h multiplied by w2 plus b2. Logits.shape is 32 by 27, and the logits look good. Now, exactly as we saw in the previous video, we want to take these logits, and we want to first exponentiate them to get our fake counts, and then we want to normalize them into a probability. So prob is counts.divide, and now counts.sum along the first dimension and keep them as true, exactly as in the previous video. And so prob.shape now is 32 by 27. And you'll see that every row of prob sums to 1, so it's normalized. So that gives us the probabilities. Now, of course, we have the actual error that comes next, and that comes from this array y, which we created during the dataset creation. So y is this last piece here, which is the identity of the next character in the sequence that we'd like to now predict. So what we'd like to do now is, just as in the previous video, we'd like to index into the rows of prob, and in each row, we'd like to pluck out the probability assigned to the correct character, as given here. So first, we have torch.arrange of 32, which is kind of like an iterator over numbers from 0 to 31, and then we can index into prob in the following way. prob in torch.arrange of 32, which iterates the rows, and then in each row, we'd like to grab this column, as given by y. So this gives the current probabilities, as assigned by this neural network with this setting of its weights, to the correct character in the sequence. And you can see here that this looks okay for some of these characters, like this is basically 0.2, but it doesn't look very good at all for many other characters. Like this is 0.0701 probability, and so the network thinks that some of these are extremely unlikely. But of course, we haven't trained the neural network yet, so this will improve. And ideally, all of these numbers here, of course, are 1, because then we are correctly predicting the next character. Now, just as in the previous video, we want to take these probabilities, we want to look at the log probability, and then we want to look at the average log probability, and the negative of it to create the negative log likelihood loss. So the loss here is 17, and this is the loss that we'd like to minimize to get the network to predict the correct character in the sequence. Okay, so I rewrote everything here and made it a bit more respectable. So here's our data set. Here's all the parameters that we defined. I'm now using a generator to make it reproducible. I clustered all the parameters into a single list of parameters so that, for example, it's easy to count them and see that in total we currently have about 3,400 parameters. And this is the forward pass as we developed it, and we arrive at a single number here, the loss, that is currently expressing how well this neural network works with the current setting of parameters. And for that, I would like to make it even more respectable. So in particular, see these lines here where we take the logits and we calculate the loss. We're not actually reinventing the wheel here. This is just classification, and many people use classification, and that's why there is a functional.crossentropy function in PyTorch to calculate this much more efficiently. So we could just simply call f.crossentropy, and we can pass in the logits, and we can pass in the array of targets, y, and this calculates the exact same loss. So in fact, we can simply put this here and erase these three lines, and we're going to get the exact same result. Now, there are actually many good reasons to prefer f.crossentropy over rolling your own implementation like this. I did this for educational reasons, but you'd never use this in practice. Why is that? Number one, when you use f.crossentropy, PyTorch will not actually create all these intermediate tensors because these are all new tensors in memory, and all this is fairly inefficient to run like this. Instead, PyTorch will cluster up all these operations and very often have fused kernels that very efficiently evaluate these expressions that are sort of like clustered mathematical operations. Number two, the backward pass can be made much more efficient, and not just because it's a fused kernel, but also analytically and mathematically, it's often a very much simpler backward pass to implement. We actually saw this with micrograd. You see here when we implemented 10h, the forward pass of this operation to calculate the 10h was actually a fairly complicated mathematical expression. But because it's a clustered mathematical expression, when we did the backward pass, we didn't individually backward through the exp and the 2 times and the minus 1 and division, etc. We just said it's 1 minus t squared, and that's a much simpler mathematical expression. And we were able to do this because we're able to reuse calculations and because we are able to mathematically and analytically derive the derivative, and often that expression simplifies mathematically, and so there's much less to implement. So not only can it be made more efficient because it runs in a fused kernel, but also because the expressions can take a much simpler form mathematically. So that's number one. Number two, under the hood, F dot cross entropy can also be significantly more numerically well-behaved. Let me show you an example of how this works. Suppose we have a logit of negative 2, 3, negative 3, 0, and 5, and then we are taking the exponent of it and normalizing it to sum to 1. So when logits take on this values, everything is well and good, and we get a nice probability distribution. Now consider what happens when some of these logits take on more extreme values, and that can happen during optimization of a neural network. Suppose that some of these numbers grow very negative, like say negative 100, then actually everything will come out fine. We still get probabilities that are well-behaved, and they sum to 1, and everything is great. But because of the way the X works, if you have very positive logits, like say positive 100 in here, you actually start to run into trouble, and we get not a number here. The reason for that is that these counts have an inf here. So if you pass in a very negative number to exp, you just get a very small number, very near 0, and that's fine. But if you pass in a very positive number, suddenly we run out of range in our floating-point number that represents these counts. So basically we're taking e and we're raising it to the power of 100, and that gives us inf because we've run out of dynamic range on this floating-point number that is count. And so we cannot pass very large logits through this expression. Now let me reset these numbers to something reasonable. The way PyTorch solves this is that you see how we have a well-behaved result here. It turns out that because of the normalization here, you can actually offset logits by any arbitrary constant value that you want. So if I add 1 here, you actually get the exact same result. Or if I add 2, or if I subtract 3, any offset will produce the exact same probabilities. So because negative numbers are okay, but positive numbers can actually overflow this exp, what PyTorch does is it internally calculates the maximum value that occurs in the logits and it subtracts it. So in this case it would subtract 5. And so therefore the greatest number in logits will become 0, and all the other numbers will become some negative numbers. And then the result of this is always well-behaved. So even if we have 100 here previously, not good. But because PyTorch will subtract 100, this will work. And so there's many good reasons to call cross-entropy. Number one, the forward pass can be much more efficient, the backward pass can be much more efficient, and also things can be much more numerically well-behaved. Okay, so let's now set up the training of this neural net. We have the forward pass. We don't need these. Instead we have that loss is equal to f dot cross-entropy. That's the forward pass. Then we need the backward pass. First we want to set the gradients to be 0. So for p in parameters, we want to make sure that p dot grad is none, which is the same as setting it to 0 in PyTorch. And then loss dot backward to populate those gradients. Once we have the gradients, we can do the parameter update. So for p in parameters, we want to take all the data, and we want to nudge it learning rate times p dot grad. And then we want to repeat this a few times. And let's print the loss here as well. Now this won't suffice, and it will create an error, because we also have to go for p in parameters, and we have to make sure that p dot requires grad is set to true in PyTorch. And this should just work. Okay, so we started off with loss of 17, and we're decreasing it. Let's run longer. And you see how the loss decreases a lot here. If we just run for 1,000 times, we get a very, very low loss, and that means that we're making very good predictions. Now the reason that this is so straightforward right now is because we're only overfitting 32 examples. So we only have 32 examples of the first five words, and therefore it's very easy to make this neural net fit only these 32 examples, because we have 3,400 parameters and only 32 examples. So we're doing what's called overfitting a single batch of the data and getting a very low loss and good predictions, but that's just because we have so many parameters for so few examples, so it's easy to make this be very low. Now we're not able to achieve exactly zero. And the reason for that is, we can for example look at logits, which are being predicted. And we can look at the max along the first dimension. And in PyTorch, max reports both the actual values that take on the maximum number, but also the indices of these. And you'll see that the indices are very close to the labels. But in some cases, they differ. For example, in this very first example, the predicted index is 19, but the label is 5. And we're not able to make loss be zero. And fundamentally, that's because here, the very first or the zeroth index is the example where dot dot dot is supposed to predict E. But you see how dot dot dot is also supposed to predict an O. And dot dot dot is also supposed to predict an I, and then S as well. And so basically, E, O, A, or S are all possible outcomes in a training set for the exact same input. So we're not able to completely overfit and make the loss be exactly zero. But we're getting very close in the cases where there's a unique input for a unique output. In those cases, we do what's called overfit. And we basically get the exact same and the exact correct result. So now all we have to do is we just need to make sure that we read in the full dataset and optimize the neural net. Okay, so let's swing back up where we created the dataset. And we see that here we only use the first five words. So let me now erase this. And let me erase the print statements. Otherwise, we'd be printing way too much. And so when we process the full dataset of all the words, we now had 228,000 examples instead of just 32. So let's now scroll back down to the dataset that's much larger, reinitialize the weights, the same number of parameters, they all require gradients. And then let's push this print.loss.item to be here. And let's just see how the optimization goes if we run this. Okay, so we started with a fairly high loss. And then as we're optimizing, the loss is coming down. But you'll notice that it takes quite a bit of time for every single iteration. So let's actually address that. Because we're doing way too much work forwarding and backwarding 228,000 examples. In practice, what people usually do is they perform forward and backward pass and update on many batches of the data. So what we will want to do is we want to randomly select some portion of the dataset. And that's a mini batch, and then only forward backward and update on that little mini batch. And then we integrate on those mini batches. So in PyTorch, we can, for example, use torch.randint. We can generate numbers between zero and five and make 32 of them. I believe the size has to be a tuple in PyTorch. So we can have a tuple 32 of numbers between zero and five. But actually, we want x.shape of zero here. And so this creates integers that index into our dataset, and there's 32 of them. So if our mini batch size is 32, then we can come here and we can first do a mini batch construct. So in the integers that we want to optimize in this single iteration are in the ix. And then we want to index into x with ix to only grab those rows. So we're only getting 32 rows of x, and therefore embeddings will again be 32 by 3 by 2, not 200,000 by 3 by 2. And then this ix has to be used not just to index into x, but also to index into y. And now this should be mini batches, and this should be much, much faster. So okay, so it's instant almost. So this way, we can run many, many examples nearly instantly and decrease the loss much, much faster. Now, because we're only dealing with mini batches, the quality of our gradient is lower. So the direction is not as reliable. It's not the actual gradient direction. But the gradient direction is good enough, even when it's estimating on only 32 examples, that it is useful. And so it's much better to have an approximate gradient and just make more steps than it is to evaluate the exact gradient and take fewer steps. So that's why in practice, this works quite well. So let's now continue the optimization. Let me take out this lost.item from here and place it over here at the end. Okay, so we're hovering around 2.5 or so. However, this is only the loss for that mini batch. So let's actually evaluate the loss here for all of x and for all of y, just so we have a full sense of exactly how well the model is doing right now. So right now we're about 2.7 on the entire training set. So let's run the optimization for a while. Okay, we're at 2.6, 2.57, 2.53. Okay, so one issue, of course, is we don't know if we're stepping too slow or too fast. So this point one, I just guessed it. So one question is, how do you determine this learning rate? And how do we gain confidence that we're stepping in the right sort of speed? So I'll show you one way to determine a reasonable learning rate. It works as follows. Let's reset our parameters to the initial settings. And now let's print in every step, but let's only do 10 steps or so, or maybe 100 steps. We want to find a very reasonable search range, if you will. So for example, if this is very low, then we see that the loss is barely decreasing. So that's not, that's too low, basically. So let's try this one. Okay, so we're decreasing the loss, but not very quickly. So that's a pretty good low range. Now let's reset it again. And now let's try to find the place at which the loss kind of explodes. So maybe at negative one. Okay, we see that we're minimizing the loss, but you see how it's kind of unstable. It goes up and down quite a bit. So negative one is probably like a fast learning rate. Let's try negative 10. Okay, so this isn't optimizing. This is not working very well. So negative 10 is way too big. Negative one was already kind of big. So therefore, negative one was like somewhat reasonable if I reset. So I'm thinking that the right learning rate is somewhere between negative 0.001 and negative one. So the way we can do this here is we can use torch.lenspace. And we want to basically do something like this, between zero and one. But number of steps is one more parameter that's required. Let's do a thousand steps. This creates 1,000 numbers between 0.001 and one. But it doesn't really make sense to step between these linearly. So instead, let me create learning rate exponent. And instead of 0.001, this will be a negative three, and this will be a zero. And then the actual LRs that we want to search over are going to be 10 to the power of LRE. So now what we're doing is we're stepping linearly between the exponents of these learning rates. This is 0.001, and this is one, because 10 to the power of zero is one. And therefore, we are spaced exponentially in this interval. So these are the candidate learning rates that we want to sort of like search over, roughly. So now what we're going to do is, here, we are going to run the optimization for 1,000 steps. And instead of using a fixed number, we are going to use learning rate indexing into here, LRs of i, and make this i. So basically, let me reset this to be, again, starting from random, creating these learning rates between 0.001 and one, but exponentially stepped. And here, what we're doing is we're iterating 1,000 times. We're going to use the learning rate that's, in the beginning, very, very low. In the beginning, it's going to be 0.001, but by the end, it's going to be one. And then we're going to step with that learning rate. And now what we want to do is we want to keep track of the learning rates that we used, and we want to look at the losses that resulted. And so here, let me track stats. So LRI.append LR, and LOSI.append LOS, that item. So again, reset everything, and then run. And so basically, we started with a very low learning rate, and we went all the way up to a learning rate of negative one. And now what we can do is we can plt.plot, and we can plot the two. So we can plot the learning rates on the x-axis and the losses we saw on the y-axis. And often, you're going to find that your plot looks something like this, where in the beginning, you had very low learning rates. So basically, barely anything happened. Then we got to a nice spot here. And then as we increased the learning rate enough, we basically started to be unstable here. So a good learning rate turns out to be somewhere around here. And because we have LRI here, we actually may want to do not the learning rate, but the exponent. So that would be the LRE at i is maybe what we want to log. So let me reset this and redo that calculation. But now on the x-axis, we have the exponent of the learning rate. And so we can see the exponent of the learning rate that is good to use. It would be sort of like roughly in the valley here, because here, the learning rates are just way too low. And then here, we expect relatively good learning rates somewhere here. And then here, things are starting to explode. So somewhere around negative 1 as the exponent of the learning rate is a pretty good setting. And 10 to the negative 1 is 0.1. So 0.1 was actually a fairly good learning rate around here. And that's what we had in the initial setting. But that's roughly how you would determine it. And so here now, we can take out the tracking of these. And we can just simply set LR to be 10 to the negative 1, or basically otherwise 0.1, as it was before. And now we have some confidence that this is actually a fairly good learning rate. And so now what we can do is we can crank up the iterations. We can reset our optimization. And we can run for a pretty long time using this learning rate. Oops. And we don't want to print. That's way too much printing. So let me again reset and run 10,000 steps. OK. So we're at 2.48, roughly. Let's run another 10,000 steps. 2.46. And now let's do one learning rate decay. What this means is we're going to take our learning rate, and we're going to 10x lower it. And so we're at the late stages of training, potentially. And we may want to go a bit slower. Let's do one more, actually, at 0.1, just to see if we're making a dent here. OK. We're still making a dent. And by the way, the bigram loss that we achieved last video was 2.45. So we've already surpassed the bigram model. And once I get a sense that this is actually kind of starting to plateau off, people like to do, as I mentioned, this learning rate decay. So let's try to decay the loss, the learning rate, I mean. And we achieve at about 2.3 now. Obviously, this is janky and not exactly how you would train it in production. But this is roughly what you're going through. You first find a decent learning rate using the approach that I showed you. Then you start with that learning rate, and you train for a while. And then at the end, people like to do a learning rate decay, where you decay the learning rate by, say, a factor of 10, and you do a few more steps, and then you get a trained network, roughly speaking. So we've achieved 2.3 and dramatically improved on the bigram language model using this simple neural net, as described here, using these 3,400 parameters. Now there's something we have to be careful with. I said that we have a better model because we are achieving a lower loss, 2.3, much lower than 2.45 with the bigram model previously. Now that's not exactly true. And the reason that's not true is that this is actually a fairly small model, but these models can get larger and larger if you keep adding neurons and parameters. So you can imagine that we don't potentially have 1,000 parameters. We could have 10,000 or 100,000 or millions of parameters. And as the capacity of the neural network grows, it becomes more and more capable of overfitting your training set. What that means is that the loss on the training set, on the data that you're training on, will become very, very low, as low as zero. But all that the model is doing is memorizing your training set verbatim. So if you take that model and it looks like it's working really well, but you try to sample from it, you will basically only get examples exactly as they are in the training set. You won't get any new data. In addition to that, if you try to evaluate the loss on some withheld names or other words, you will actually see that the loss on those can be very high. And so basically it's not a good model. So the standard in the field is to split up your data set into three splits, as we call them. We have the training split, the dev split or the validation split, and the test split. So training split, dev or validation split, and test split. And typically this would be, say, 80% of your data set. This could be 10% and this 10%, roughly. So you have these three splits of the data. Now these 80% of your trainings of the data set, the training set, is used to optimize the parameters of the model, just like we're doing here using gradient descent. These 10% of the examples, the dev or validation split, they're used for development over all the hyperparameters of your model. So hyperparameters are, for example, the size of this hidden layer, the size of the embedding. So this is 100 or a two for us, but we could try different things. The strength of the regularization, which we aren't using yet so far. So there's lots of different hyperparameters and settings that go into defining a neural net, and you can try many different variations of them and see whichever one works best on your validation split. So this is used to train the parameters. This is used to train the hyperparameters. And test split is used to evaluate basically the performance of the model at the end. So we're only evaluating the loss on the test split very, very sparingly and very few times, because every single time you evaluate your test loss and you learn something from it, you are basically starting to also train on the test split. So you are only allowed to test the loss on the test set very, very few times. Otherwise you risk overfitting to it as well as you experiment on your model. So let's also split up our training data into train, dev, and test. And then we are going to train on train and only evaluate on test very, very sparingly. Okay, so here we go. Here is where we took all the words and put them into X and Y tensors. So instead, let me create a new cell here and let me just copy paste some code here, because I don't think it's that complex, but we're going to try to save a little bit of time. So I'm going to create this to be a function now. And this function takes some list of words and builds the arrays X and Y for those words only. And then here I am shuffling up all the words. So these are the input words that we get. We are randomly shuffling them all up. And then we're going to set n1 to be the number of examples that is 80% of the words and n2 to be 90% of the way of the words. So basically if length of words is 32,000, n1 is... Well, sorry, I should probably run this. n1 is 25,000 and n2 is 28,000. And so here we see that I'm calling buildDataset to build a training set X and Y by indexing into up to n1. So we're going to have only 25,000 training words. And then we're going to have roughly n2 minus n1, 3,000 validation examples or dev examples. And we're going to have length of words basically minus n2 or 3,204 examples here for the test set. So now we have Xs and Ys for all those three splits. Oh yeah, I'm printing their size here inside the function as well. But here we don't have words, but these are already the individual examples made from those words. So let's now scroll down here. And the dataset now for training is more like this. And then when we reset the network, when we're training, we're only going to be training using X train, X train, and Y train. So that's the only thing we're training on. Let's see where we are on the single batch. Let's now train maybe a few more steps. Training neural networks can take a while. Usually you don't do it inline. You launch a bunch of jobs and you wait for them to finish. It can take multiple days and so on. Luckily, this is a very small network. So the loss is pretty good. We accidentally used a learning rate that is way too low. So let me actually come back. We used the decay learning rate of 0.01. So this will train much faster. And then here, when we evaluate, let's use the dev set here. X dev and Y dev to evaluate the loss. And let's now decay the learning rate and only do, say, 10,000 examples. And let's evaluate the dev loss once here. OK, so we're getting there. about 2.3 on dev. And so the neural network running most training did not see these dev examples. It hasn't optimized on them. And yet when we evaluate the loss on these dev, we actually get a pretty decent loss. And so we can also look at what the loss is on all of training set. Oops. And so we see that the training and the dev loss are about equal. So we're not overfitting. This model is not powerful enough to just be purely memorizing the data. And so far we are what's called underfitting because the training loss and the dev or test losses are roughly equal. So what that typically means is that our network is very tiny, very small. And we expect to make performance improvements by scaling up the size of this neural net. So let's do that now. So let's come over here and let's increase the size of the neural net. The easiest way to do this is we can come here to the hidden layer, which currently has a hundred neurons, and let's just bump this up. So let's do 300 neurons. And then this is also 300 biases. And here we have 300 inputs into the final layer. So let's initialize our neural net. We now have 10,000 parameters instead of 3000 parameters. And then we're not using this. And then here, what I'd like to do is I'd like to actually keep track of that. So, okay, let's just do this. Let's keep stats again. And here, when we're keeping track of the loss, let's just also keep track of the steps. And let's just have a I here. And let's train on 30,000, or rather say, okay, let's try 30,000. And we are at 0.1. And we should be able to run this and optimize the neural net. And then here, basically, I want to plt.plot the steps against the loss. So these are the Xs and the Ys. And this is the loss function and how it's being optimized. Now, you see that there's quite a bit of thickness to this, and that's because we are optimizing over these mini-batches and the mini-batches create a little bit of noise in this. Where are we in the dev set? We are at 2.5. So we still haven't optimized this neural net very well. And that's probably because we made it bigger. It might take longer for this neural net to converge. And so let's continue training. Yeah, let's just continue training. One possibility is that the batch size is so low that we just have way too much noise in the training. And we may want to increase the batch size so that we have a bit more correct gradient, and we're not thrashing too much, and we can actually optimize more properly. Okay, this will now become meaningless because we've reinitialized these. So yeah, this looks not pleasing right now. But there probably is a tiny improvement, but it's so hard to tell. Let's go again, 2.52. Let's try to decrease the learning rate by a factor of two. Okay, we're at 2.32. Let's continue training. We basically expect to see a lower loss than what we had before, because now we have a much, much bigger model, and we were underfitting. So we'd expect that increasing the size of the model should help the neural net. 2.32, okay, so that's not happening too well. Now, one other concern is that even though we've made the 10H layer here, or the hidden layer, much, much bigger, it could be that the bottleneck of the network right now are these embeddings that are two-dimensional. It can be that we're just cramming way too many characters into just two dimensions, and the neural net is not able to really use that space effectively, and that is sort of like the bottleneck to our network's performance. Okay, 2.23, so just by decreasing the learning rate, I was able to make quite a bit of progress. Let's run this one more time, and then evaluate the training and the dev loss. Now, one more thing after training that I'd like to do is I'd like to visualize the embedding vectors for these characters before we scale up the embedding size from two, because we'd like to make this bottleneck potentially go away. But once I make this greater than two, we won't be able to visualize them. So here, okay, we're at 2.23 and 2.24, so we're not improving much more, and maybe the bottleneck now is the character embedding size, which is two. So here I have a bunch of code that will create a figure, and then we're going to visualize the embeddings that were trained by the neural net on these characters, because right now the embedding size is just two, so we can visualize all the characters with the X and the Y coordinates as the two embedding locations for each of these characters. And so here are the X coordinates and the Y coordinates, which are the columns of C, and then for each one, I also include the text of the little character. So here what we see is actually kind of interesting. The network has basically learned to separate out the characters and cluster them a little bit. So for example, you see how the vowels, A, E, I, O, U, are clustered up here? So what that's telling us is that the neural net treats these as very similar, because when they feed into the neural net, the embedding for all of these characters is very similar, and so the neural net thinks that they're very similar and kind of interchangeable, if that makes sense. Then the points that are really far away are, for example, Q. Q is kind of treated as an exception, and Q has a very special embedding vector, so to speak. Similarly, Dot, which is a special character, is all the way out here, and a lot of the other letters are sort of clustered up here. And so it's kind of interesting that there's a little bit of structure here after the training, and it's definitely not random, and these embeddings make sense. So we're now going to scale up the embedding size and won't be able to visualize it directly, but we expect that because we're underfitting and we made this layer much bigger and did not sufficiently improve the loss, we're thinking that the constraint to better performance right now could be these embedding vectors. So let's make them bigger. Okay, so let's scroll up here, and now we don't have two-dimensional embeddings. We are going to have, say, 10-dimensional embeddings for each word. Then this layer will receive three times 10, so 30 inputs will go into the hidden layer. Let's also make the hidden layer a bit smaller. So instead of 300, let's just do 200 neurons in that hidden layer. So now the total number of elements will be slightly bigger at 11,000. And then here we have to be a bit careful because, okay, the learning rate, we set to 0.1. Here we are hard-coding six, and obviously if you're working in production, you don't want to be hard-coding magic numbers, but instead of six, this should now be 30. And let's run for 50,000 iterations, and let me split out the initialization here outside so that when we run this cell multiple times, it's not going to wipe out our loss. In addition to that, here, let's, instead of logging the lost item, let's actually log the, let's do log 10. I believe that's a function of the loss, and I'll show you why in a second. Let's optimize this. Basically, I'd like to plot the log loss instead of the loss, because when you plot the loss, many times it can have this hockey stick appearance, and log squashes it in, so it just kind of looks nicer. So the x-axis is step i, and the y-axis will be the loss i. And then here, this is 30. Ideally, we wouldn't be hard-coding these. Okay, so let's look at the loss. Okay, it's, again, very thick because the mini-batch size is very small, but the total loss over the training set is 2.3, and the test, or the dev set, is 2.38 as well. So, so far, so good. Let's try to now decrease the learning rate by a factor of 10, and train for another 50,000 iterations. We'd hope that we would be able to beat 2.32. But again, we're just kind of doing this very haphazardly, so I don't actually have confidence that our learning rate is set very well, that our learning rate decay, which we just do at random, is set very well. And so the optimization here is kind of suspect, to be honest, and this is not how you would do it, typically, in production. In production, you would create parameters, or hyperparameters, out of all these settings, and then you would run lots of experiments and see whichever ones are working well for you. Okay, so we have 2.17 now, and 2.2. Okay, so you see how the training and the validation performance are starting to slightly, slowly depart. So maybe we're getting the sense that the neural net is getting good enough, or that number of parameters is large enough that we are slowly starting to overfit. Let's maybe run one more iteration of this and see where we get. But yeah, basically, you would be running lots of experiments, and then you are slowly scrutinizing whichever ones give you the best depth performance. And then once you find all the hyperparameters that make your depth performance good, you take that model and you evaluate the test set performance a single time. And that's the number that you report in your paper, or wherever else you want to talk about and brag about your model. So let's then rerun the plot and rerun the train and dev. And because we're getting lower loss now, it is the case that the embedding size of these was holding us back very lightly. Okay, so 2.16, 2.19 is what we're roughly getting. So there's many ways to go from here. We can continue tuning the optimization. We can continue, for example, playing with the size of the neural net, or we can increase the number of words or characters in our case that we are taking as an input. So instead of just three characters, we could be taking more characters than as an input. And that could further improve the loss. Okay, so I changed the code slightly. So we have here 200,000 steps of the optimization. And in the first 100,000, we're using a learning rate of 0.1. And then in the next 100,000, we're using a learning rate of 0.01. This is the loss that I achieve. And these are the performance on the training and validation loss. And in particular, the best validation loss I've been able to obtain in the last 30 minutes or so is 2.17. So now I invite you to beat this number. And you have quite a few knobs available to you to I think surpass this number. So number one, you can of course change the number of neurons in the hidden layer of this model. You can change the dimensionality of the embedding lookup table. You can change the number of characters that are feeding in as an input, as the context into this model. And then of course, you can change the details of the optimization. How long are we running? Where's the learning rate? How does it change over time? How does it decay? You can change the batch size and you may be able to actually achieve a much better convergence rate or a much better convergence speed in terms of how many seconds or minutes it takes to train the model and get your result in terms of really good loss. And then of course, I actually invite you to read this paper. It is 19 pages, but at this point, you should actually be able to read a good chunk of this paper and understand pretty good chunks of it. And this paper also has quite a few ideas for improvements that you can play with. So all of those are knobs available to you and you should be able to beat this number. I'm leaving that as an exercise to the reader and that's it for now. And I'll see you next time. Before we wrap up, I also wanted to show how you would sample from the model. So we're going to generate 20 samples. At first we begin with all dots. So that's the context. And then until we generate the zeroth character again, we're going to embed the current context using the embedding table C. Now, usually here, the first dimension was the size of the training set, but here we're only working with a single example that we're generating. So this is just dimension one, just for simplicity. And so this embedding then gets projected into the end state, you get the logits. Now we calculate the probabilities. For that, you can use F.softmax of logits and that just basically exponentiates logits and makes them sum to one. And similar to cross entropy, it is careful that there's no overflows. Once we have the probabilities, we sample from them using torsion multinomial to get our next index. And then we shift the context window to append the index and record it. And then we can just decode all the integers to strings and print them out. And so these are some example samples and you can see that the model now works much better. So the words here are much more word like or name like. So we have things like ham, Joe's, Lila, you know, it's starting to sound a little bit more name like so we're definitely making progress, but we can still improve on this model quite a lot. Okay, sorry, there's some bonus content. I wanted to mention that I want to make these notebooks more accessible. And so I don't want you to have to like install Jupyter notebooks and torch and everything else. So I will be sharing a link to a Google Colab and Google Colab will look like a notebook in your browser. And you can just go to a URL and you'll be able to execute all of the code that you saw in the Google Colab. And so this is me executing the code in this lecture and I shortened it a little bit, but basically you're able to train the exact same network and then plot and sample from the model. And everything is ready for you to like tinker with the numbers right there in your browser, no installation necessary. So I just wanted to point that out and the link to this will be in the video description. Hi everyone. Today we are continuing our implementation of Makemore. Now in the last lecture we implemented the multilayer perceptron along the lines of Benji Oettel 2003 for character level language modeling. So we followed this paper, took in a few characters in the past, and used an MLP to predict the next character in a sequence. So what we'd like to do now is we'd like to move on to more complex and larger neural networks, like recurrent neural networks and their variations like the GRU, LSTM, and so on. Now, before we do that though, we have to stick around the level of multilayer perceptron for a bit longer. And I'd like to do this because I would like us to have a very good intuitive understanding of the activations in the neural net during training, and especially the gradients that are flowing backwards, and how they behave and what they look like. And this is going to be very important to understand the history of the development of these architectures. Because we'll see that recurrent neural networks, while they are very expressive in that they are a universal approximator and can in principle implement all the algorithms, we'll see that they are not very easily optimizable with the first-order gradient-based techniques that we have available to us and that we use all the time. And the key to understanding why they are not optimizable easily is to understand the activations and the gradients and how they behave during training. And we'll see that a lot of the variants since recurrent neural networks have tried to improve that situation. And so that's the path that we have to take, and let's get started. So the starting code for this lecture is largely the code from before, but I've cleaned it up a little bit. So you'll see that we are importing all the Torch and Matplotlib utilities. We're reading in the words just like before. These are eight example words. There's a total of 32,000 of them. Here's a vocabulary of all the lowercase letters and the special dot token. Here we are reading the dataset and processing it and creating three splits, the train, dev, and the test split. Now in the MLP, this is the identical same MLP, except you see that I removed a bunch of magic numbers that we had here. And instead we have the dimensionality of the embedding space of the characters and the number of hidden units in the hidden layer. And so I've pulled them outside here so that we don't have to go in and change all these magic numbers all the time. With the same neural net with 11,000 parameters that we optimize now over 200,000 steps with a batch size of 32. And you'll see that I refactored the code here a little bit, but there are no functional changes. I just created a few extra variables, a few more comments, and I removed all the magic numbers. And otherwise it's the exact same thing. Then when we optimize, we saw that our loss looked something like this. We saw that the train and val loss were about 2.16 and so on. Here I refactored the code a little bit for the evaluation of arbitrary splits. So you pass in a string of which split you'd like to evaluate. And then here, depending on train, val, or test, I index in and I get the correct split. And then this is the forward pass of the network and evaluation of the loss and printing it. So just making it nicer. One thing that you'll notice here is I'm using a decorator torch.nograd, which you can also look up and read documentation of. Basically what this decorator does on top of a function is that whatever happens in this function is assumed by Torch to never require any gradients. So it will not do any of the bookkeeping that it does to keep track of all the gradients in anticipation of an eventual backward pass. It's almost as if all the tensors that get created here have a requires grad of false. And so it just makes everything much more efficient because you're telling Torch that I will not call.backward on any of this computation and you don't need to maintain the graph under the hood. So that's what this does. And you can also use a context manager with torch.nograd and you can look those up. Then here we have the sampling from a model just as before. Just a forward pass of a neural net, getting the distribution, sampling from it, adjusting the context window and repeating until we get the special end token. And we see that we are starting to get much nicer looking words sampled from the model. It's still not amazing and they're still not fully name-like, but it's much better than when we had to work with the bigram model. So that's our starting point. Now the first thing I would like to scrutinize is the initialization. I can tell that our network is very improperly configured at initialization and there's multiple things wrong with it, but let's just start with the first one. Look here on the zeroth iteration, the very first iteration, we are recording a loss of 27 and this rapidly comes down to roughly one or two or so. So I can tell that the initialization is all messed up because this is way too high. In training of neural nets, it is almost always the case that you will have a rough idea for what loss to expect at initialization, and that just depends on the loss function and the problem setup. In this case, I do not expect 27. I expect a much lower number and we can calculate it together. Basically at initialization, what we'd like is that there's 27 characters that could come next for any one training example. At initialization, we have no reason to believe any characters to be much more likely than others. And so we'd expect that the probability distribution that comes out initially is a uniform distribution assigning about equal probability to all the 27 characters. So basically what we'd like is the probability for any character would be roughly 1 over 27. That is the probability we should record. And then the loss is the negative log probability. So let's wrap this in a tensor and then we can take the log of it. And then the negative log probability is the loss we would expect, which is 3.29, much, much lower than 27. And so what's happening right now is that at initialization, the neural net is creating probability distributions that are all messed up. Some characters are very confident and some characters are very not confident. And then basically what's happening is that the network is very confidently wrong and that's what makes it record very high loss. So here's a smaller four-dimensional example of the issue. Let's say we only have four characters and then we have logits that come out of the neural net and they are very, very close to zero. Then when we take the softmax of all zeros, we get probabilities that are a diffuse distribution. So sums to one and is exactly uniform. And then in this case, if the label is say two, it doesn't actually matter if the label is two or three or one or zero because it's a uniform distribution. We're recording the exact same loss in this case, 1.38. So this is the loss we would expect for a four-dimensional example. And I can see of course that as we start to manipulate these logits, we're going to be changing the loss here. So it could be that we luck out and by chance this could be a very high number like five or something like that. Then in that case, we'll record a very low loss because we're assigning the correct probability at initialization by chance to the correct label. Much more likely it is that some other dimension will have a high logit. And then what will happen is we start to record a much higher loss. And what can happen is basically the logits come out like something like this, and they take on extreme values and we record really high loss. For example, if we have torq.random of four, so these are uniform, sorry, these are normally distributed numbers, four of them. And here we can also print the logits, probabilities that come out of it, and loss. And so because these logits are near zero, for the most part, the loss that comes out is okay. But suppose this is like times 10 now. You see how because these are more extreme values, it's very unlikely that you're going to be guessing the correct bucket, and then you're confidently wrong and recording very high loss. If your logits are coming out even more extreme, you might get extremely insane losses like infinity even at initialization. So basically this is not good, and we want the logits to be roughly zero when the network is initialized. In fact, the logits don't have to be just zero, they just have to be equal. So for example, if all the logits are one, then because of the normalization inside the softmax, this will actually come out okay. But by symmetry, we don't want it to be any arbitrary positive or negative number, we just want it to be all zeros and record the loss that we expect at initialization. So let's now concretely see where things go wrong in our example. Here we have the initialization. Let me reinitialize the neural net. And here, let me break after the very first iteration, so we only see the initial loss, which is 27. So that's way too high. And intuitively, now we can expect the variables involved. And we see that the logits here, if we just print some of these, if we just print the first row, we see that the logits take on quite extreme values. And that's what's creating the fake confidence and incorrect answers and makes the loss get very, very high. So these logits should be much, much closer to zero. So now let's think through how we can achieve logits coming out of this neural net to be more closer to zero. You see here that logits are calculated as the hidden states multiplied by w2 plus b2. So first of all, currently we're initializing b2 as random values of the right size. But because we want roughly zero, we don't actually want to be adding a bias of random numbers. So in fact, I'm going to add a times a zero here to make sure that b2 is just basically zero at initialization. And second, this is h multiplied by w2. So if we want logits to be very, very small, then we would be multiplying w2 and making that smaller. So for example, if we scale down w2 by 0.1, all the elements, then if I do again just the very first iteration, you see that we are getting much closer to what we expect. So roughly what we want is about 3.29. This is 4.2. I can make this maybe even smaller. 3.32. Okay, so we're getting closer and closer. Now you're probably wondering, can we just set this to zero? Then we get, of course, exactly what we're looking for at initialization. And the reason I don't usually do this is because I'm very nervous. And I'll show you in a second why you don't want to be setting w's or weights of a neural net exactly to zero. You usually want it to be small numbers instead of exactly zero. For this output layer in this specific case, I think it would be fine, but I'll show you in a second where things go wrong very quickly if you do that. So let's just go with 0.01. In that case, our loss is close enough, but has some entropy. It's not exactly zero. It's got some little entropy, and that's used for symmetry breaking, as we'll see in a second. Logits are now coming out much closer to zero, and everything is well and good. So if I just erase these, and I now take away the break statement, we can run the optimization with this new initialization. And let's just see what losses we record. Okay, so I let it run, and you see that we started off good, and then we came down a bit. The plot of the loss now doesn't have this hockey-shape appearance, because basically what's happening in the hockey stick, the very first few iterations of the loss, what's happening during the optimization is the optimization is just squashing down the logits, and then it's rearranging the logits. So basically we took away this easy part of the loss function, where just the weights were just being shrunk down. And so therefore, we don't get these easy gains in the beginning, and we're just getting some of the hard gains of training the actual neural net. And so there's no hockey stick appearance. So good things are happening in that both, number one, loss at initialization is what we expect, and the loss doesn't look like a hockey stick. And this is true for any neural net you might train, and something to look out for. And second, the loss that came out is actually quite a bit improved. Unfortunately, I erased what we had here before. I believe this was 2.12, and this was 2.16. So we get a slightly improved result. And the reason for that is because we're spending more cycles, more time, optimizing the neural net actually, instead of just spending the first several thousand iterations probably just squashing down the weights, because they are so way too high in the beginning of the initialization. So something to look out for, and that's number one. Now let's look at the second problem. Let me reinitialize our neural net, and let me reintroduce the break statement. So we have a reasonable initial loss. So even though everything is looking good on the level of the loss, and we get something that we expect, there's still a deeper problem lurking inside this neural net and its initialization. So the logits are now okay. The problem now is with the values of h, the activations of the hidden states. Now if we just visualize this vector, sorry, this tensor h, it's kind of hard to see, but the problem here, roughly speaking, is you see how many of the elements are 1 or negative 1? Now recall that torch.10h, the 10h function, is a squashing function. It takes arbitrary numbers and it squashes them into a range of negative 1 and 1, and it does so smoothly. So let's look at the histogram of h to get a better idea of the distribution of the values inside this tensor. We can do this first. Well we can see that h is 32 examples and 200 activations in each example. We can view it as negative 1 to stretch it out into one large vector, and we can then call toList to convert this into one large Python list of floats. And then we can pass this into plt.hist for histogram, and we say we want 50 bins, and a semicolon to suppress a bunch of output we don't want. So we see this histogram, and we see that most of the values by far take on value of negative 1 and 1. So this 10h is very, very active. And we can also look at basically why that is. We can look at the preactivations that feed into the 10h. And we can see that the distribution of the preactivations is very, very broad. These take numbers between negative 15 and 15, and that's why in a torsion 10h everything is being squashed and capped to be in the range of negative 1 and 1, and lots of numbers here take on very extreme values. Now if you are new to neural networks, you might not actually see this as an issue. But if you're well-versed in the dark arts of backpropagation and have an intuitive sense of how these gradients flow through a neural net, you are looking at your distribution of 10h activations here, and you are sweating. So let me show you why. We have to keep in mind that during backpropagation, just like we saw in micrograd, we are doing backward pass starting at the loss and flowing through the network backwards. In particular, we're going to backpropagate through this torsion.10h. And this layer here is made up of 200 neurons for each one of these examples, and it implements an element-wise 10h. So let's look at what happens in 10h in the backward pass. We can actually go back to our previous micrograd code in the very first lecture and see how we implemented 10h. We saw that the input here was x, and then we calculate t, which is the 10h of x. So that's t, and t is between negative 1 and 1. It's the output of the 10h. And then in the backward pass, how do we backpropagate through a 10h? We take out.grad, and then we multiply it, this is the chain rule, with the local gradient, which took the form of 1 minus t squared. So what happens if the outputs of your 10h are very close to negative 1 or 1? If you plug in t equals 1 here, you're going to get a 0, multiplying out.grad. No matter what out.grad is, we are killing the gradient, and we're stopping effectively the backpropagation through this 10h unit. Similarly, when t is negative 1, this will again become 0, and out.grad just stops. And intuitively, this makes sense, because this is a 10h neuron, and what's happening is if its output is very close to 1, then we are in the tail of this 10h. And so changing basically the input is not going to impact the output of the 10h too much, because it's in a flat region of the 10h, and so therefore there's no impact on the loss. And so indeed, the weights and the biases along with this 10h neuron do not impact the loss, because the output of this 10h unit is in the flat region of the 10h, and there's no influence. We can be changing them however we want, and the loss is not impacted. So that's another way to justify that indeed, the gradient would be basically 0, it vanishes. Indeed, when t equals 0, we get 1 times out.grad. So when the 10h takes on exactly value of 0, then out.grad is just passed through. So basically what this is doing, right, is if t is equal to 0, then the 10h unit is sort of inactive, and gradient just passes through. But the more you are in the flat tails, the more the gradient is squashed. So in fact, you'll see that the gradient flowing through 10h can only ever decrease, and the amount that it decreases is proportional through a square here, depending on how far you are in the flat tails of this 10h. And so that's kind of what's happening here. And the concern here is that if all of these outputs h are in the flat regions of negative 1 and 1, then the gradients that are flowing through the network will just get destroyed at this layer. Now there is some redeeming quality here, and that we can actually get a sense of the problem here as follows. I wrote some code here. And basically what we want to do here is we want to take a look at h, take the absolute value, and see how often it is in a flat region, so say, greater than 0.99. And what you get is the following. And this is a Boolean tensor. So in the Boolean tensor, you get a white if this is true and a black if this is false. And so basically what we have here is the 32 examples and the 200 hidden neurons. And we see that a lot of this is white. And what that's telling us is that all these 10h neurons were very, very active, and they're in a flat tail. And so in all these cases, the backward gradient would get destroyed. Now we would be in a lot of trouble if, for any one of these 200 neurons, if it was the case that the entire column is white. Because in that case, we have what's called a dead neuron. And this could be a 10h neuron where the initialization of the weights and the biases could be such that no single example ever activates this 10h in the active part of the 10h. If all the examples land in the tail, then this neuron will never learn. It is a dead neuron. And so just scrutinizing this and looking for columns of completely white, we see that this is not the case. So I don't see a single neuron that is all of white. And so therefore... it is the case that for every one of these 10H neurons, we do have some examples that activate them in the active part of the 10H. And so some gradients will flow through and this neuron will learn and neuron will change and it will move and it will do something. But you can sometimes get yourself in cases where you have dead neurons. And the way this manifests is that for a 10H neuron, this would be when no matter what inputs you plug in from your dataset, this 10H neuron always fires completely one or completely negative one, and then it will just not learn because all the gradients will be just zeroed out. This is true not just for 10H, but for a lot of other nonlinearities that people use in neural networks. So we certainly use 10H a lot, but sigmoid will have the exact same issue because it is a squashing neuron. And so the same will be true for sigmoid, but basically the same will actually apply to sigmoid. Basically the same will actually apply to sigmoid. The same will also apply to a ReLU. So ReLU has a completely flat region here below zero. So if you have a ReLU neuron, then it is a pass-through if it is positive. And if the pre-activation is negative, it will just shut it off. Since the region here is completely flat, then during backpropagation, this would be exactly zeroing out the gradient. Like all of the gradient would be set exactly to zero instead of just like a very, very small number depending on how positive or negative T is. And so you can get, for example, a dead ReLU neuron, and a dead ReLU neuron would basically look like, basically what it is is if a neuron with a ReLU nonlinearity never activates, so for any examples that you plug in in the dataset, it never turns on, it's always in this flat region, then this ReLU neuron is a dead neuron. Its weights and bias will never learn. They will never get a gradient because the neuron never activated. And this can sometimes happen at initialization because the weights and the biases just make it so that by chance, some neurons are just forever dead. But it can also happen during optimization. If you have like a too high of a learning rate, for example, sometimes you have these neurons that gets too much of a gradient and they get knocked out of the data manifold. And what happens is that from then on, no example ever activates this neuron, so this neuron remains dead forever. So it's kind of like a permanent brain damage in a mind of a network. And so sometimes what can happen is if your learning rate is very high, for example, and you have a neural net with ReLU neurons, you train the neural net and you get some last loss, but then actually what you do is you go through the entire training set and you forward your examples and you can find neurons that never activate. They are dead neurons in your network. And so those neurons will never turn on. And usually what happens is that during training, these ReLU neurons are changing, moving, et cetera, and then because of a high gradient, somewhere by chance, they get knocked off and then nothing ever activates them. And from then on, they are just dead. So that's kind of like a permanent brain damage that can happen to some of these neurons. These other nonlinearities like Leaky-Bare-LU will not suffer from this issue as much because you can see that it doesn't have flat tails. You'll almost always get gradients. And ELU is also fairly frequently used. It also might suffer from this issue because it has flat parts. So that's just something to be aware of and something to be concerned about. And in this case, we have way too many activations H that take on extreme values. And because there's no column of white, I think we will be okay. And indeed the network optimizes and gives us a pretty decent loss, but it's just not optimal. And this is not something you want, especially during initialization. And so basically what's happening is that this H pre-activation that's flowing to 10H, it's too extreme, it's too large. It's creating a distribution that is too saturated in both sides of the 10H. And it's not something you want because it means that there's less training for these neurons because they update less frequently. So how do we fix this? Well, H pre-activation is MCAT, which comes from C. So these are uniform Gaussian, but then it's multiplied by W1 plus B1. And H pre-act is too far off from zero and that's causing the issue. So we want this pre-activation to be closer to zero, very similar to what we had with logits. So here we want actually something very, very similar. Now it's okay to set the biases to very small number. We can either multiply it by 001 to get like a little bit of entropy. I sometimes like to do that just so that there's like a little bit of variation and diversity in the original initialization of these 10H neurons. And I find in practice that that can help optimization a little bit. And then the weights, we can also just like squash. So let's multiply everything by 0.1. Let's rerun the first batch. And now let's look at this. And well, first let's look at here. You see now, because we multiply W by 0.1, we have a much better histogram. And that's because the pre-activations are now between negative 1.5 and 1.5. And this we expect much, much less white. Okay, there's no white. So basically that's because there are no neurons that saturated above 0.99 in either direction. So it's actually a pretty decent place to be. Maybe we can go up a little bit. Sorry, am I changing W1 here? So maybe we can go to 0.2. Okay, so maybe something like this is a nice distribution. So maybe this is what our initialization should be. So let me now erase these. And let me, starting with initialization, let me run the full optimization without the break. And let's see what we got. Okay, so the optimization finished. And I rerun the loss. And this is the result that we get. And then just as a reminder, I put down all the losses that we saw previously in this lecture. So we see that we actually do get an improvement here. And just as a reminder, we started off with a validation loss of 2.17 when we started. By fixing the softmax being confidently wrong, we came down to 2.13. And by fixing the 10H layer being way too saturated, we came down to 2.10. And the reason this is happening, of course, is because our initialization is better. And so we're spending more time doing productive training instead of not very productive training because our gradients are set to zero. And we have to learn very simple things like the overconfidence of the softmax in the beginning. And we're spending cycles just like squashing down the weight matrix. So this is illustrating basically initialization and its impact on performance just by being aware of the internals of these neural nets and their activations and their gradients. Now, we're working with a very small network. This is just a one-layer, multi-layer perceptron. So because the network is so shallow, the optimization problem is actually quite easy and very forgiving. So even though our initialization was terrible, the network still learned eventually. It just got a bit worse result. This is not the case in general, though. Once we actually start working with much deeper networks that have, say, 50 layers, things can get much more complicated and these problems stack up. And so you can actually get into a place where the network is basically not training at all if your initialization is bad enough. And the deeper your network is and the more complex it is, the less forgiving it is to some of these errors. And so something to definitely be aware of and something to scrutinize, something to plot, and something to be careful with. And yeah. Okay, so that's great that that worked for us. But what we have here now is all these magic numbers, like 0.2, like where do I come up with this? And how am I supposed to set these if I have a large neural net with lots and lots of layers? And so obviously no one does this by hand. There's actually some relatively principled ways of setting these scales that I would like to introduce to you now. So let me paste some code here that I prepared just to motivate the discussion of this. So what I'm doing here is we have some random input here, X, that is drawn from a Gaussian, and there's 1,000 examples that are 10-dimensional. And then we have a weighting layer here that is also initialized using Gaussian, just like we did here. And these neurons in the hidden layer look at 10 inputs, and there are 200 neurons in this hidden layer. And then we have here, just like here in this case, the multiplication, X multiplied by W, to get the preactivations of these neurons. And basically the analysis here looks at, okay, suppose these are uniform Gaussian, and these weights are uniform Gaussian. If I do X times W, and we forget for now the bias and the nonlinearity, then what is the mean and the standard deviation of these Gaussians? So in the beginning here, the input is just a normal Gaussian distribution, mean is zero, and the standard deviation is one. And the standard deviation, again, is just a measure of a spread of this Gaussian. But then once we multiply here, and we look at the histogram of Y, we see that the mean, of course, stays the same. It's about zero, because this is a symmetric operation. But we see here that the standard deviation has expanded to three. So the input standard deviation was one, but now we've grown to three. And so what you're seeing in the histogram is that this Gaussian is expanding. And so we're expanding this Gaussian from the input. And we don't want that. We want most of the neural nets to have relatively similar activations. So unit Gaussian roughly throughout the neural net. And so the question is, how do we scale these Ws to preserve this distribution to preserve this distribution to remain a Gaussian? And so intuitively, if I multiply here these elements of W by a larger number, let's say by five, then this Gaussian grows and grows in standard deviation. So now we're at 15. So basically these numbers here in the output Y take on more and more extreme values. But if we scale it down, let's say 0.2, then conversely, this Gaussian is getting smaller and smaller and it's shrinking. And you can see that the standard deviation is 0.6. And so the question is, what do I multiply by here to exactly preserve the standard deviation to be one? And it turns out that the correct answer mathematically when you work out through the variance of this multiplication here is that you are supposed to divide by the square root of the fan in. The fan in is basically the number of input elements here, 10. So we are supposed to divide by 10 square root. And this is one way to do the square root. You raise it to a power of 0.5. That's the same as doing a square root. So when you divide by the square root of 10, then we see that the output Gaussian, it has exactly standard deviation of one. Now, unsurprisingly, a number of papers have looked into how to best initialize neural networks. And in the case of multilayer perceptrons, we can have fairly deep networks that have these nonlinearities in between. And we want to make sure that the activations are well-behaved and they don't expand to infinity or shrink all the way to zero. And the question is, how do we initialize the weights so that these activations take on reasonable values throughout the network? Now, one paper that has studied this in quite a bit of detail that is often referenced is this paper by Kaiming He et al called Delving Deep Interactive Fires. Now, in this case, they actually study convolutional neural networks, and they study especially the ReLU nonlinearity and the pReLU nonlinearity instead of a 10H nonlinearity. But the analysis is very similar. And basically what happens here is, for them, the ReLU nonlinearity that they care about quite a bit here is a squashing function where all the negative numbers are simply clamped to zero. So the positive numbers are a path through, but everything negative is just set to zero. And because you are basically throwing away half of the distribution, they find in their analysis of the forward activations in the neural net that you have to compensate for that with a gain. And so here, they find that basically when they initialize their weights, they have to do it with a zero-mean Gaussian whose standard deviation is square root of two over the fan-in. What we have here is we are initializing the Gaussian with the square root of fan-in. This nL here is the fan-in. So what we have is square root of one over the fan-in because we have the division here. Now, they have to add this factor of two because of the ReLU, which basically discards half of the distribution and clamps it at zero. And so that's where you get an initial factor. Now, in addition to that, this paper also studies not just the sort of behavior of the activations in the forward paths of the neural net, but it also studies the backpropagation. And we have to make sure that the gradients also are well-behaved. And so, because ultimately, they end up updating our parameters. And what they find here through a lot of the analysis that I invite you to read through, but it's not exactly approachable, what they find is basically if you properly initialize the forward paths, the backward paths is also approximately initialized up to a constant factor that has to do with the size of the number of hidden neurons in an early and late layer. But basically, they find empirically that this is not a choice that matters too much. Now, this kyming initialization is also implemented in PyTorch. So if you go to torch.nn.init documentation, you'll find kyming normal. And in my opinion, this is probably the most common way of initializing neural networks now. And it takes a few keyword arguments here. So number one, it wants to know the mode. Would you like to normalize the activations? Or would you like to normalize the gradients to be always Gaussian with zero mean and a unit or one standard deviation? And because they find in the paper that this doesn't matter too much, most of the people just leave it as the default, which is fan-in. And then second, passing the nonlinearity that you are using. Because depending on the nonlinearity, we need to calculate a slightly different gain. And so if your nonlinearity is just linear, so there's no nonlinearity, then the gain here will be one. And we have the exact same kind of formula that we've got up here. But if the nonlinearity is something else, we're going to get a slightly different gain. And so if we come up here to the top, we see that, for example, in the case of ReLU, this gain is a square root of two. And the reason it's a square root, because in this paper, you see how the two is inside of the square root. So the gain is a square root of two. In the case of linear or identity, we just get a gain of one. In the case of tanh, which is what we're using here, the advised gain is a five over three. And intuitively, why do we need a gain on top of the initialization? It's because tanh, just like ReLU, is a contractive transformation. So what that means is you're taking the output distribution from this matrix multiplication, and then you are squashing it in some way. Now, ReLU squashes it by taking everything below zero and clamping it to zero. Tanh also squashes it because it's a contractive operation. It will take the tails and it will squeeze them in. And so in order to fight the squeezing in, we need to boost the weights a little bit so that we renormalize everything back to unit standard deviation. So that's why there's a little bit of a gain that comes out. Now, I'm skipping through this section a little bit quickly, and I'm doing that actually intentionally. And the reason for that is because about seven years ago, when this paper was written, you had to actually be extremely careful with the activations and the gradients and their ranges and their histograms. And you had to be very careful with the precise setting of gains and the scrutinizing of the nonlinearities used and so on. And everything was very finicky and very fragile and very properly arranged for the neural net to train, especially if your neural net was very deep. But there are a number of modern innovations that have made everything significantly more stable and more well-behaved. And it's become less important to initialize these networks exactly right. And some of those modern innovations, for example, are residual connections, which we will cover in the future, the use of a number of normalization layers, like for example, batch normalization, layer normalization, group normalization. We're going to go into a lot of these as well. And number three, much better optimizers, not just stochastic gradient descent, the simple optimizer we're basically using here, but slightly more complex optimizers like RMSProp and especially Adam. And so all of these modern innovations make it less important for you to precisely calibrate the initialization of the neural net. All that being said, in practice, what should we do? In practice, when I initialize these neural nets, I basically just normalize my weights by the square root of the fan-in. So basically, roughly what we did here is what I do. Now, if we want to be exactly accurate here, and go by init of timing normal, this is how we would implement it. We want to set the standard deviation to be gain over the square root of fan-in, right? So to set the standard deviation of our weights, we will proceed as follows. Basically, when we have a torsdotrandon, and let's say I just create a thousand numbers, we can look at the standard deviation of this, and of course, that's one. That's the amount of spread. Let's make this a bit bigger so it's closer to one. So that's the spread of the Gaussian of zero mean and unit standard deviation. Now, basically, when you take these and you multiply by, say, 0.2, that basically scales down the Gaussian, and that makes its standard deviation 0.2. So basically, the number that you multiply by here ends up being the standard deviation of this Gaussian. So here, this is a standard deviation 0.2 Gaussian here when we sample RW1. But we want to set the standard deviation to gain over square root of fan-mode, which is fan-in. So in other words, we want to multiply by gain, which for 10H is 5 over 3. 5 over 3 is the gain. And then times... Or I guess, sorry, divide square root of the fan-in. And in this example here, the fan-in was 10. And I just noticed that actually here, the fan-in for W1 is actually nmbtimes block size, which, as you will recall, is actually 30. And that's because each character is 10-dimensional, but then we have three of them and we concatenate them. So actually, the fan-in here was 30, and I should have used 30 here probably. But basically, we want 30 square root. So this is the number. This is what our standard deviation we want to be. And this number turns out to be 0.3. Whereas here, just by fiddling with it and looking at the distribution and making sure it looks okay, we came up with 0.2. And so instead, what we want to do here is we want to make the standard deviation be 5 over 3, which is our gain, divide. This amount times 0.2 square root. And these brackets here are not that necessary, but I'll just put them here for clarity. This is basically what we want. This is the kyming init in our case for 10H nonlinearity. And this is how we would initialize the neural net. And so we're multiplying by 0.3 instead of multiplying by 0.2. And so we can initialize this way, and then we can train the neural net and see what we get. Okay, so I trained the neural net and we end up in roughly the same spot. So looking at the validation loss, we now get 2.10, and previously we also had 2.10. And there's a little bit of a difference, but that's just the randomness of the process, I suspect. But the big deal, of course, is we get to the same spot, but we did not have to introduce any magic numbers that we got from just looking at histograms and guessing, checking. We have something that is semi-principled and will scale us to much bigger networks and something that we can sort of use as a guide. So I mentioned that the precise setting of these initializations is not as important today due to some modern innovations. And I think now is a pretty good time to introduce one of those modern innovations, and that is batch normalization. So batch normalization came out in 2015 from a team at Google, and it was an extremely impactful paper because it made it possible to train very deep neural nets quite reliably, and it basically just worked. So here's what batch normalization does and what's implemented. Basically, we have these hidden states, H-preact, right? And we were talking about how we don't want these preactivation states to be way too small because then the 10H is not doing anything, but we don't want them to be too large because then the 10H is saturated. In fact, we want them to be roughly Gaussian, so zero mean and a unit or a one standard deviation, at least at initialization. So the insight from the batch normalization paper is, okay, you have these hidden states and you'd like them to be roughly Gaussian, then why not take the hidden states and just normalize them to be Gaussian? And it sounds kind of crazy, but you can just do that because standardizing hidden states so that they're unit Gaussian is a perfectly differentiable operation as we'll soon see. And so that was kind of like the big insight in this paper. And when I first read it, my mind was blown because you can just normalize these hidden states. And if you'd like unit Gaussian states in your network, at least initialization, you can just normalize them to be unit Gaussian. So let's see how that works. So we're going to scroll to our preactivations here just before they enter into the 10H. Now, the idea again is, remember, we're trying to make these roughly Gaussian, and that's because if these are way too small numbers, then the 10H here is kind of inactive. But if these are very large numbers, then the 10H is way too saturated and gradient flow. So we'd like this to be roughly Gaussian. So the insight in batch normalization again is that we can just standardize these activations so they are exactly Gaussian. So here, HPreact has a shape of 32 by 200, 32 examples by 200 neurons in the hidden layer. So basically what we can do is we can take HPreact and we can just calculate the mean. And the mean we want to calculate across the zero dimension. And we want to also keep them as true so that we can easily broadcast this. So the shape of this is one by 200. In other words, we are doing the mean over all the elements in the batch. And similarly, we can calculate the standard deviation of these activations. And that will also be one by 200. Now in this paper, they have the sort of prescription here. And see here, we are calculating the mean, which is just taking the average value of any neurons activation. And then their standard deviation is basically kind of like the measure of the spread that we've been using, which is the distance of every one of these values away from the mean and that squared and averaged. That's the variance. And then if you want to take the standard deviation, you would square root the variance to get the standard deviation. So these are the two that we're calculating. And now we're going to normalize or standardize these Xs by subtracting the mean and dividing by the standard deviation. So basically we're taking H preact and we subtract the mean. And then we divide by the standard deviation. This is exactly what these two, STD and mean are calculating. Oops, sorry. This is the mean and this is the variance. You see how the sigma is the standard deviation usually. So this is sigma square, which is variance is the square of the standard deviation. So this is how you standardize these values. And what this will do is that every single neuron now and its firing rate will be exactly unit Gaussian on these 32 examples, at least of this batch. That's why it's called batch normalization. We are normalizing these batches. And then we could in principle train this. Notice that calculating the mean and their standard deviation, these are just mathematical formulas. They're perfectly differentiable. All of this is perfectly differentiable. And we can just train this. The problem is you actually won't achieve a very good result with this. And the reason for that is we want these to be roughly Gaussian, but only at initialization. But we don't want these to be forced to be Gaussian always. We'd like to allow the neural net to move this around to potentially make it more diffuse, to make it more sharp, to make some 10H neurons maybe be more trigger happy or less trigger happy. So we'd like this distribution to move around and we'd like the back propagation to tell us how the distribution should move around. And so in addition to this idea of standardizing the activations at any point in the network, we have to also introduce this additional component in the paper here described as scale and shift. And so basically what we're doing is we're taking these normalized inputs and we are additionally scaling them by some gain and offsetting them by some bias to get our final output from this layer. And so what that amounts to is the following. We are going to allow a batch normalization gain to be initialized at just a once, and the once will be in the shape of one by N hidden. And then we also will have a BN bias, which will be torched at zeros. And it will also be of the shape N by one by N hidden. And then here, the BN gain will multiply this and the BN bias will offset it here. So because this is initialized to one and this to zero, at initialization, each neuron's firing values in this batch will be exactly unit Gaussian and will have nice numbers. No matter what the distribution of the H preact is coming in, coming out, it will be unit Gaussian for each neuron. And that's roughly what we want, at least at initialization. And then during optimization, we'll be able to back propagate to BN gain and BN bias and change them so the network is given the full ability to do with this whatever it wants internally. Here, we just have to make sure that we include these in the parameters of the neural net because they will be trained with back propagation. So let's initialize this, and then we should be able to train. And then we're going to also copy this line, which is the best normalization layer here on a single line of code. And we're going to swing down here, and we're also going to do the exact same thing at test time here. So similar to train time, we're going to normalize and then scale, and that's going to give us our train and validation loss. And we'll see in a second that we're actually going to change this a little bit, but for now, I'm going to keep it this way. So I'm just going to wait for this to converge. Okay, so I allowed the neural nets to converge here. And when we scroll down, we see that our validation loss here is 2.10, roughly, which I wrote down here. And we see that this is actually kind of comparable to some of the results that we've achieved previously. Now, I'm not actually expecting an improvement in this case. And that's because we are dealing with a very simple neural net that has just a single hidden layer. So in fact, in this very simple case of just one hidden layer, we were able to actually calculate what the scale of W should be to make these preactivations already have a roughly Gaussian shape. So the best normalization is not doing much here. But you might imagine that once you have a much deeper neural net that has lots of different types of operations, and there's also, for example, residual connections, which we'll cover, and so on, it will become basically very, very difficult to tune the scales of your weight matrices, such that all the activations throughout the neural net are roughly Gaussian. And so that's going to become very quickly intractable. But compared to that, it's going to be much, much easier to sprinkle best normalization layers throughout the neural net. So in particular, it's common to look at every single linear layer, like this one. This is a linear layer multiplying by a weight matrix and adding a bias. Or for example, convolutions, which we'll cover later, and also perform basically a multiplication with a weight matrix, but in a more spatially structured format. It's customary to take these linear layer or convolutional layer and append a best normalization layer right after it to control the scale of these activations at every point in the neural net. So we'd be adding these best normal layers throughout the neural net, and then this controls the scale of these activations throughout the neural net. It doesn't require us to do perfect mathematics and care about the activation distributions for all these different types of neural network, Lego building blocks that you might want to introduce into your neural net. And it significantly stabilizes the training. And that's why these layers are quite popular. Now, the stability offered by best normalization actually comes at a terrible cost. And that cost is that if you think about what's happening here, something terribly strange and unnatural is happening. It used to be that we have a single example feeding into a neural net, and then we calculate this activations and its logits. And this is a deterministic sort of process. So you arrive at some logits for this example. And then because of efficiency of training, we suddenly started to use batches of examples, but those batches of examples were processed independently and it was just an efficiency thing. But now suddenly in batch normalization, because of the normalization through the batch, we are coupling these examples mathematically and in the forward pass and the backward pass of the neural net. So now the hidden state activations, HPREACT, and your logits for any one input example are not just a function of that example and its input, but they're also a function of all the other examples that happen to come for a ride in that batch. And these examples are sampled randomly. And so what's happening is, for example, when you look at HPREACT, that's gonna feed into H, the hidden state activations, for example, for any one of these input examples is going to actually change slightly depending on what other examples there are in the batch. And depending on what other examples happen to come for a ride, H is going to change suddenly and it's going to like jitter if you imagine sampling different examples, because the statistics of the mean and the standard deviation are going to be impacted. And so you'll get a jitter for H and you'll get a jitter for logits. And you'd think that this would be a bug or something undesirable, but in a very strange way, this actually turns out to be good in neural network training as a side effect. And the reason for that is that you can think of this as kind of like a regularizer, because what's happening is you have your input and you get your H, and then depending on the other examples, this is jittering a bit. And so what that does is that it's effectively padding out any one of these input examples, and it's introducing a little bit of entropy. And because of the padding out, it's actually kind of like a form of data augmentation, which we'll cover in the future. And it's kind of like augmenting the input a little bit and it's jittering it. And that makes it harder for the neural nets to overfit to these concrete specific examples. So by introducing all this noise, it actually like pads out the examples and it regularizes the neural net. And that's one of the reasons why, deceivingly as a second order effect, this is actually a regularizer, and that has made it harder for us to remove the use of batch normalization. Because basically no one likes this property that the examples in the batch are coupled mathematically and in the forward pass. And at least all kinds of like strange results, we'll go into some of that in a second as well. And at least to a lot of bugs and so on. And so no one likes this property. And so people have tried to deprecate the use of batch normalization and move to other normalization techniques that do not couple the examples of a batch. Examples are layer normalization, instance normalization, group normalization, and so on. And we'll cover some of these later. But basically long story short, batch normalization was the first kind of normalization layer to be introduced. It worked extremely well. It happens to have this regularizing effect. It stabilized training. And people have been trying to remove it and move to some of the other normalization techniques. But it's been hard because it just works quite well. And some of the reason that it works quite well is again, because of this regularizing effect and because it is quite effective at controlling the activations and their distributions. So that's kind of like the brief story of batch normalization. And I'd like to show you one of the other weird sort of outcomes of this coupling. So here's one of the strange outcomes that I only glossed over previously, when I was evaluating the loss on the validation set. Basically, once we've trained a neural net, we'd like to deploy it in some kind of a setting. And we'd like to be able to feed in a single individual example and get a prediction out from our neural net. But how do we do that when our neural net now in a forward pass estimates the statistics of the mean and standard deviation of a batch? The neural net expects batches as an input now. So how do we feed in a single example and get sensible results out? And so the proposal in the batch normalization paper is the following. What we would like to do here is we would like to basically have a step after training that calculates and sets the batch norm mean and standard deviation a single time over the training set. And so I wrote this code here in the interest of time, and we're going to call what's called calibrate the batch norm statistics. And basically what we do is torch.nograd telling PyTorch that none of this, we will call a dot backward on, and it's going to be a bit more efficient. We're going to take the training set, get the preactivations for every single training example, and then one single time estimate the mean and standard deviation over the entire training set. And then we're going to get B and mean and B and standard deviation. And now these are fixed numbers estimating over the entire training set. And here, instead of estimating it dynamically, we are going to instead here use B and mean, and here we're just going to use B and standard deviation. And so at test time, we are going to fix these, clamp them and use them during inference. And now you see that we get basically identical result, but the benefit that we've gained is that we can now also forward a single example because the mean and standard deviation are now fixed sort of tensors. That said, nobody actually wants to estimate this mean and standard deviation as a second stage after neural network training because everyone is lazy. And so this batch normalization paper actually introduced one more idea, which is that we can estimate the mean and standard deviation in a running manner during training of the neural net. And then we can simply just have a single stage of training and on the side of that training, we are estimating the running mean and standard deviation. So let's see what that would look like. Let me basically take the mean here that we are estimating on the batch and let me call this B and mean on the ith iteration. And then here, this is B and STD. B and STD at i, okay? And the mean comes here and the STD comes here. So, so far I've done nothing. I've just moved around and I created these extra variables for the mean and standard deviation and I've put them here. So, so far nothing has changed, but what we're going to do now is we're going to keep a running mean of both of these values during training. So let me swing up here and let me create a BN mean underscore running. And I'm going to initialize it at zeros and then BNSTD running, which I'll initialize at ones. Because in the beginning, because of the way we initialized W1 and B1, HPreact will be roughly unit Gaussian. So the mean will be roughly zero and the standard deviation roughly one. So I'm going to initialize these that way, but then here I'm going to update these. And in PyTorch, these mean and standard deviation that are running, they're not actually part of the gradient based optimization. We're never going to derive gradients with respect to them. They're updated on the side of training. And so what we're going to do here is we're going to say, with torch.nograd, telling PyTorch that the update here is not supposed to be building out a graph because there will be no dot backward, but this running mean is basically going to be 0.999 times the current value plus 0.001 times this value, this new mean. And in the same way, BNSTD running will be mostly what it used to be, but it will receive a small update in the direction of what the current standard deviation is. And as you're seeing here, this update is outside and on the side of the gradient based optimization. And it's simply being updated, not using gradient descent, it's just being updated using a janky, like smooth sort of running mean manner. And so while the network is training and these pre-activations are sort of changing and shifting around during back propagation, we are keeping track of the typical mean and standard deviation and we're estimating them once. And when I run this, now I'm keeping track of this in a running manner. And what we're hoping for, of course, is that the BNMean underscore running and BNMean underscore STD are going to be very similar to the ones that we calculated here before. And that way we don't need a second stage because we've sort of combined the two stages and we've put them on the side of each other, if you want to look at it that way. And this is how this is also implemented in the batch normalization layer in PyTorch. So during training, the exact same thing will happen. And then later when you're using inference, it will use the estimated running mean of both the mean and standard deviation of those hidden states. So let's wait for the optimization to converge and hopefully the running mean and standard deviation are roughly equal to these two. And then we can simply use it here and we don't need this stage of explicit calibration at the end. Okay, so the optimization. I'll rerun the explicit estimation, and then the bnmean from the explicit estimation is here, and bnmean from the running estimation during the optimization you can see is very, very similar. It's not identical, but it's pretty close. And in the same way, bnstd is this, and bnstd running is this. As you can see, once again, they are fairly similar values, not identical, but pretty close. And so then here, instead of bnmean, we can use the bnmean running. Instead of bnstd, we can use bnstd running. And hopefully, the validation loss will not be impacted too much. So basically identical. And this way, we've eliminated the need for this explicit stage of calibration, because we are doing it inline over here. Okay, so we're almost done with batch normalization. There are only two more notes that I'd like to make. Number one, I've skipped a discussion over what is this plus epsilon doing here. This epsilon is usually like some small fixed number, for example, 1e-5 by default. And what it's doing is that it's basically preventing a division by zero in the case that the variance over your batch is exactly zero. In that case, here we'd normally have a division by zero, but because of the plus epsilon, this is going to become a small number in the denominator instead, and things will be more well-behaved. So feel free to also add a plus epsilon here of a very small number. It doesn't actually substantially change the result. I'm going to skip it in our case just because this is unlikely to happen in our very simple example here. And the second thing I want you to notice is that we're being wasteful here, and it's very subtle, but right here where we are adding the bias into HPREACT, these biases now are actually useless because we're adding them to the HPREACT. But then we are calculating the mean for every one of these neurons and subtracting it. So whatever bias you add here is going to get subtracted right here. And so these biases are not doing anything. In fact, they're being subtracted out, and they don't impact the rest of the calculation. So if you look at B1.grad, it's actually going to be zero because it's being subtracted out and doesn't actually have any effect. And so whenever you're using batch normalization layers, then if you have any weight layers before, like a linear or a conv or something like that, you're better off coming here and just not using bias. So you don't want to use bias, and then here you don't want to add it because that's spurious. Instead, we have this batch normalization bias here, and that batch normalization bias is now in charge of the biasing of this distribution instead of this B1 that we had here originally. And so basically, the batch normalization layer has its own bias, and there's no need to have a bias in the layer before it because that bias is going to be subtracted out anyway. So that's the other small detail to be careful with. Sometimes it's not going to do anything catastrophic. This B1 will just be useless. It will never get any gradient. It will not learn. It will stay constant, and it's just wasteful, but it doesn't actually really impact anything otherwise. Okay, so I rearranged the code a little bit with comments, and I just wanted to give a very quick summary of the batch normalization layer. We are using batch normalization to control the statistics of activations in the neural net. It is common to sprinkle batch normalization layer across the neural net, and usually we will place it after layers that have multiplications, like, for example, a linear layer or a convolutional layer, which we may cover in the future. Now, the batch normalization internally has parameters for the gain and the bias, and these are trained using backpropagation. It also has two buffers. The buffers are the mean and the standard deviation, the running mean and the running mean of the standard deviation. And these are not trained using backpropagation. These are trained using this Janky update of kind of like a running mean update. So these are sort of the parameters and the buffers of batch normalization layer. And then really what it's doing is it's calculating the mean and the standard deviation of the activations that are feeding into the batch normalization layer over that batch. Then it's centering that batch to be unit Gaussian, and then it's offsetting and scaling it by the learned bias and gain. And then on top of that, it's keeping track of the mean and standard deviation of the inputs, and it's maintaining this running mean and standard deviation. And this will later be used at inference so that we don't have to re-estimate the mean and standard deviation all the time. And in addition, that allows us to basically forward individual examples at test time. So that's the batch normalization layer. It's a fairly complicated layer, but this is what it's doing internally. Now I wanted to show you a little bit of a real example. So you can search ResNet, which is a residual neural network, and these are contacts of neural networks used for image classification. And of course, we haven't covered ResNets in detail, so I'm not going to explain all the pieces of it. But for now, just note that the image feeds into a ResNet on the top here, and there's many, many layers with repeating structure all the way to predictions of what's inside that image. This repeating structure is made up of these blocks, and these blocks are just sequentially stacked up in this deep neural network. Now, the code for this, the block basically that's used and repeated sequentially in series, is called this bottleneck block. And there's a lot here. This is all PyTorch. And of course, we haven't covered all of it, but I want to point out some small pieces of it. Here in the init is where we initialize the neural net. So this code of block here is basically the kind of stuff we're doing here. We're initializing all the layers. And in the forward, we are specifying how the neural net acts once you actually have the input. So this code here is along the lines of what we're doing here. And now these blocks are replicated and stacked up serially, and that's what a residual network would be. And so notice what's happening here. Conv1, these are convolution layers. And these convolution layers, basically, they're the same thing as a linear layer, except convolution layers don't apply. Convolution layers are used for images, and so they have spatial structure. And basically, this linear multiplication and bias offset are done on patches instead of the full input. So because these images have structure, spatial structure, convolutions just basically do WX plus B, but they do it on overlapping patches of the input. But otherwise, it's WX plus B. Then we have the normal layer, which by default here is initialized to be a batch norm in 2D, so a two-dimensional batch normalization layer. And then we have a nonlinearity like ReLU. So instead of, here they use ReLU, we are using tanh in this case. But both are just nonlinearities, and you can just use them relatively interchangeably. For very deep networks, ReLUs typically empirically work a bit better. So see the motif that's being repeated here. We have convolution, batch normalization, ReLU, convolution, batch normalization, ReLU, etc. And then here, this is a residual connection that we haven't covered yet. But basically, that's the exact same pattern we have here. We have a weight layer, like a convolution or like a linear layer, batch normalization, and then tanh, which is a nonlinearity. But basically, a weight layer, a normalization layer, and nonlinearity. And that's the motif that you would be stacking up when you create these deep neural networks. Exactly as is done here. And one more thing I'd like you to notice is that here, when they are initializing the conv layers, like conv1x1, the depth for that is right here. And so it's initializing an nn.conv2d, which is a convolution layer in PyTorch. And there's a bunch of keyword arguments here that I'm not going to explain yet. But you see how there's bias equals false? The bias equals false is exactly for the same reason as bias is not used in our case. You see how I erased the use of bias? And the use of bias is spurious because after this weight layer, there's a batch normalization. And the batch normalization subtracts that bias and then has its own bias. So there's no need to introduce these spurious parameters. It wouldn't hurt performance. It's just useless. And so because they have this motif of conv, batch, and relu, they don't need a bias here because there's a bias inside here. By the way, this example here is very easy to find. Just do ResNet PyTorch, and it's this example here. So this is kind of like the stock implementation of a residual neural network in PyTorch. And you can find that here. But of course, I haven't covered many of these parts yet. And I would also like to briefly descend into the definitions of these PyTorch layers and the parameters that they take. Now, instead of a convolutional layer, we're going to look at a linear layer because that's the one that we're using here. This is a linear layer, and I haven't covered convolutions yet. But as I mentioned, convolutions are basically linear layers except on patches. So a linear layer performs a Wx plus b, except here they're calling the W a transpose. So it calculates Wx plus b very much like we did here. To initialize this layer, you need to know the fan in, the fan out, and that's so that they can initialize this W. This is the fan in and the fan out, so they know how big the weight matrix should be. You need to also pass in whether or not you want a bias. And if you set it to false, then no bias will be inside this layer. And you may want to do that exactly like in our case if your layer is followed by a normalization layer such as batch norm. So this allows you to basically disable bias. Now, in terms of the initialization, if we swing down here, this is reporting the variables used inside this linear layer. And our linear layer here has two parameters, the weight and the bias. In the same way, they have a weight and a bias, and they're talking about how they initialize it by default. So by default, PyTorch will initialize your weights by taking the fan in and then doing 1 over fan in square root. And then instead of a normal distribution, they are using a uniform distribution. So it's very much the same thing, but they are using a 1 instead of 5 over 3, so there's no gain being calculated here. The gain is just 1. But otherwise, it's exactly 1 over the square root of fan in, exactly as we have here. So 1 over the square root of k is the scale of the weights. But when they are drawing the numbers, they're not using a Gaussian by default. They're using a uniform distribution by default. And so they draw uniformly from negative square root of k to square root of k. But it's the exact same thing and the same motivation with respect to what we've seen in this lecture. And the reason they're doing this is if you have a roughly Gaussian input, this will ensure that out of this layer, you will have a roughly Gaussian output. And you basically achieve that by scaling the weights by 1 over the square root of fan in. So that's what this is doing. And then the second thing is the batch normalization layer. So let's look at what that looks like in PyTorch. So here we have a one-dimensional batch normalization layer, exactly as we are using here. And there are a number of keyword arguments going into it as well. So we need to know the number of features. For us, that is 200. And that is needed so that we can initialize these parameters here, the gain, the bias, and the buffers for the running mean and standard deviation. Then they need to know the value of epsilon here. And by default, this is 1 and negative 5. You don't typically change this too much. Then they need to know the momentum. And the momentum here, as they explain, is basically used for these running mean and running standard deviation. So by default, the momentum here is 0.1. The momentum we are using here in this example is 0.001. And basically, you may want to change this sometimes. And roughly speaking, if you have a very large batch size, then typically what you'll see is that when you estimate the mean and standard deviation for every single batch size, if it's large enough, you're going to get roughly the same result. And so therefore, you can use slightly higher momentum like 0.1. But for a batch size as small as 32, the mean and standard deviation here might take on slightly different numbers because there's only 32 examples we are using to estimate the mean and standard deviation. So the value is changing around a lot. And if your momentum is 0.1, that might not be good enough for this value to settle and converge to the actual mean and standard deviation over the entire training set. And so basically, if your batch size is very small, momentum of 0.1 is potentially dangerous, and it might make it so that the running mean and standard deviation is thrashing too much during training, and it's not actually converging properly. Affine equals true determines whether this batch normalization layer has these learnable affine parameters. The gain and the bias. And this is almost always kept to true. I'm not actually sure why you would want to change this to false. Then track running stats is determining whether or not batch normalization layer of PyTorch will be doing this. And one reason you may want to skip the running stats is because you may want to, for example, estimate them at the end as a stage two like this. And in that case, you don't want the batch normalization layer to be doing all this extra compute that you're not going to use. And finally, we need to know which device we're going to run this batch normalization on, a CPU or a GPU, and what the data type should be, half precision, single precision, double precision, and so on. So that's the batch normalization layer. Otherwise, they link to the paper. It's the same formula we've implemented, and everything is the same, exactly as we've done here. Okay, so that's everything that I wanted to cover for this lecture. Really what I wanted to talk about is the importance of understanding the activations and the gradients and their statistics in neural networks. And this becomes increasingly important, especially as you make your neural networks bigger, larger, and deeper. We looked at the distributions basically at the output layer, and we saw that if you have too confident mispredictions because the activations are too messed up at the last layer, you can end up with these hockey stick losses. And if you fix this, you get a better loss at the end of training because your training is not doing wasteful work. Then we also saw that we need to control the activations. We don't want them to squash to zero or explode to infinity, and because of that, you can run into a lot of trouble with all of these nonlinearities in these neural nets. And basically, you want everything to be fairly homogeneous throughout the neural net. You want roughly Gaussian activations throughout the neural net. Then we talked about, okay, if we want roughly Gaussian activations, how do we scale these weight matrices and biases during initialization of the neural net so that we don't get, you know, so everything is as controlled as possible? So that gave us a large boost in improvement. And then I talked about how that strategy is not actually possible for much, much deeper neural nets, because when you have much deeper neural nets with lots of different types of layers, it becomes really, really hard to precisely set the weights and biases in such a way that the activations are roughly uniform throughout the neural net. So then I introduced the notion of a normalization layer. Now, there are many normalization layers that people use in practice. Batch normalization, layer normalization, instance normalization, group normalization. We haven't covered most of them, but I've introduced the first one, and also the one that I believe came out first, and that's called batch normalization. And we saw how batch normalization works. This is a layer that you can sprinkle throughout your deep neural net. And the basic idea is if you want roughly Gaussian activations, well, then take your activations and take the mean and the standard deviation and center your data. And you can do that because the centering operation is differentiable. But on top of that, we actually had to add a lot of bells and whistles, and that gave you a sense of the complexities of the batch normalization layer, because now we're centering the data. That's great. But suddenly, we need the gain and the bias, and now those are trainable. And then because we are coupling all the training examples, now suddenly the question is how do you do the inference? To do the inference, we need to now estimate these mean and standard deviation once over the entire training set, and then use those at inference. But then no one likes to do stage two, so instead we fold everything into the batch normalization layer during training and try to estimate these in a running manner so that everything is a bit simpler. And that gives us the batch normalization layer. And as I mentioned, no one likes this layer. It causes a huge amount of bugs, and intuitively it's because it is coupling examples in the forward pass of the neural net. And I've shot myself in the foot with this layer over and over again in my life, and I don't want you to suffer the same. So basically try to avoid it as much as possible. Some of the other alternatives to these layers are, for example, group normalization or layer normalization, and those have become more common in more recent deep learning, but we haven't covered those yet. But definitely batch normalization was very influential at the time when it came out in roughly 2015, because it was kind of the first time that you could train reliably much deeper neural nets. And fundamentally the reason for that is because this layer was very effective at controlling the statistics of the activations in the neural net. So that's the story so far, and that's all I wanted to cover. And in the future lectures, hopefully we can start going into recurrent neural nets. And recurrent neural nets, as we'll see, are just very, very deep networks, because you unroll the loop when you actually optimize these neural nets. And that's where a lot of this analysis around the activation statistics and all these normalization layers will become very, very important for good performance. So we'll see that next time. Bye. Okay, so I lied. I would like us to do one more summary here as a bonus, and I think it's useful as to have one more summary of everything I've presented in this lecture. But also I would like us to start PyTorch-ifying our code a little bit so it looks much more like what you would encounter in PyTorch. So you'll see that I will structure our code into these modules, like a linear module and a batch form module. And I'm putting the code inside these modules so that we can construct neural networks very much like we would construct them in PyTorch. And I will go through this in detail. So we'll create our neural net. Then we will do the optimization loop as we did before. And then the one more thing that I want to do here is I want to look at the activation statistics both in the forward pass and in the backward pass. And then here we have the evaluation and sampling just like before. So let me rewind all the way up here and go a little bit slower. So here I am creating a linear layer. You'll notice that torch.nn has lots of different types of layers, and one of those layers is the linear layer. Torch.nn.linear takes a number of input features, output features, whether or not we should have bias, and then the device that we want to place this layer on, and the data type. So I will omit these two, but otherwise we have the exact same thing. We have the fan-in, which is the number of inputs, the fan-out, the number of outputs, and whether or not we want to use a bias. And internally inside this layer, there's a weight and a bias, if you like it. It is typical to initialize the weight using, say, random numbers drawn from a Gaussian, and then here's the kyming initialization that we discussed already in this lecture. And that's a good default, and also the default that I believe PyTorch uses. And by default, the bias is usually initialized to zeros. Now, when you call this module, this will basically calculate w times x plus b, if you have nb. And then when you also call dot parameters on this module, it will return the tensors that are the parameters of this layer. Now next, we have the batch normalization layer. So I've written that here, and this is very similar to PyTorch's nn.batchnorm1d layer, as shown here. So I'm kind of taking these three parameters here, the dimensionality, the epsilon that we'll use in the division, and the momentum that we will use in keeping track of these running stats, the running mean and the running variance. Now, PyTorch actually takes quite a few more things, but I'm assuming some of their settings. So for us, affine will be true. That means that we will be using a gamma and beta after the normalization. The track running stats will be true, so we will be keeping track of the running mean and the running variance in the batch norm. Our device by default is the CPU, and the data type by default is float, float32. So those are the defaults, otherwise we are taking all the same parameters in this batch norm layer. So first, I'm just saving them. Now here's something new. There's a.training, which by default is true. And PyTorch nn modules also have this attribute,.training. And that's because many modules, and batch norm is included in that, have a different behavior whether you are training your neural net or whether you are running it in an evaluation mode and calculating your evaluation loss or using it for inference on some test examples. And batch norm is an example of this because when we are training, we are going to be using the mean and the variance estimated from the current batch. But during inference, we are using the running mean and running variance. And so also if we are training, we are updating mean and variance. But if we are testing, then these are not being updated. They're kept fixed. And so this flag is necessary and by default true, just like in PyTorch. Now the parameters of batch norm 1D are the gamma and the beta here. And then the running mean and the running variance are called buffers in PyTorch nomenclature. And these buffers are trained using exponential moving average here explicitly. And they are not part of the backpropagation and stochastic gradient descent. So they are not sort of like parameters of this layer. And that's why when we have parameters here, we only return gamma and beta. We do not return the mean and the variance. This is trained sort of like internally here, every forward pass using exponential moving average. So that's the initialization. Now in a forward pass, if we are training, then we use the mean and the variance estimated by the batch. Let me pull up the paper here. We calculate the mean and the variance. Now up above, I was estimating the standard deviation and keeping track of the standard deviation here in the running standard deviation instead of running variance. But let's follow the paper exactly. Here they calculate the variance, which is the standard deviation squared. And that's what's kept track of in the running variance instead of a running standard deviation. But those two would be very, very similar, I believe. If we are not training, then we use the running mean and variance. We normalize. And then here I'm calculating the output of this layer. And I'm also assigning it to an attribute called dot out. Now dot out is something that I'm using in our modules here. This is not what you would find in PyTorch. We are slightly deviating from it. I'm creating a dot out because I would like to very easily maintain all those variables so that we can create statistics of them and plot them. But PyTorch and modules will not have a dot out attribute. And finally here we are updating the buffers using, again, as I mentioned, exponential moving average, given the provided momentum. And importantly, you'll notice that I'm using the torch.nograd context manager. And I'm doing this because if we don't use this, then PyTorch will start building out an entire computational graph out of these tensors because it is expecting that we will eventually call a dot backward. But we are never going to be calling dot backward on anything that includes running mean and running variance. So that's why we need to use this context manager so that we are not sort of maintaining them using all this additional memory. So this will make it more efficient. And it's just telling PyTorch that there will be no backward. We just have a bunch of tensors. We want to update them. That's it. And then we return. Okay, now scrolling down, we have the 10H layer. This is very, very similar to torch.10H. And it doesn't do too much. It just calculates 10H, as you might expect. So that's torch.10H. And there's no parameters in this layer. But because these are layers, it now becomes very easy to sort of like stack them up into basically just a list. And we can do all the initializations that we're used to. So we have the initial sort of embedding matrix. We have our layers, and we can call them sequentially. And then again, with torch.nograd, there's some initializations here. So we want to make the output softmax a bit less confident, like we saw. And in addition to that, because we are using a six-layer multilayer perceptron here, so you see how I'm stacking linear, 10H, linear, 10H, et cetera, I'm going to be using the gain here. And I'm going to play with this in a second. So you'll see how when we change this, what happens to the statistics. Finally, the parameters are basically the embedding matrix and all the parameters in all the layers. And notice here, I'm using a double list comprehension, if you want to call it that. But for every layer in layers, and for every parameter in each of those layers, we are just stacking up all those p's, all those parameters. Now in total, we have 46,000 parameters. And I'm telling PyTorch that all of them require gradient. Then here, we have everything here we are actually mostly used to. We are sampling batch, we are doing forward pass. The forward pass now is just a linear application of all the layers in order, followed by the cross entropy. And then in the backward pass, you'll notice that for every single layer, I now iterate over all the outputs, and I'm telling PyTorch to retain the gradient of them. And then here, we are already used to all the gradients set to none, do the backward to fill in the gradients, do an update using stochastic gradient send, and then track some statistics. And then I am going to break after a single iteration. Now here in this cell, in this diagram, I'm visualizing the histograms of the four pass activations, and I'm specifically doing it at the 10-H layers. So iterating over all the layers, except for the very last one, which is basically just the softmax layer. If it is a 10-H layer, and I'm using a 10-H layer just because they have a finite output, negative one to one, and so it's very easy to visualize here. So you see negative one to one, and it's a finite range, and easy to work with. I take the out tensor from that layer into T, and then I'm calculating the mean, the standard deviation, and the percent saturation of T. And the way I define the percent saturation is that T dot absolute value is greater than 0.97. So that means we are here at the tails of the 10-H. And remember that when we are in the tails of the 10-H, that will actually stop gradients. So we don't want this to be too high. Now here, I'm calling torch dot histogram, and then I am plotting this histogram. So basically what this is doing is that every different type of layer, and they all have a different color, we are looking at how many values in these tensors take on any of the values below on this axis here. So the first layer is fairly saturated here at 20%, so you can see that it's got tails here. But then everything sort of stabilizes. And if we had more layers here, it would actually just stabilize at around the standard deviation of about 0.65, and the saturation would be roughly 5%. And the reason that this stabilizes and gives us a nice distribution here is because gain is set to 5 over 3. Now here, this gain, you see that by default we initialize with 1 over square root of fan in. But then here during initialization, I come in and I iterate over all the layers. And if it's a linear layer, I boost that by the gain. Now we saw that 1, so basically if we just do not use a gain, then what happens? If I redraw this, you will see that the standard deviation is shrinking and the saturation is coming to 0. And basically what's happening is the first layer is pretty decent, but then further layers are just kind of like shrinking down to 0. And it's happening slowly, but it's shrinking to 0. And the reason for that is when you just have a sandwich of linear layers alone, then initializing our weights in this manner we saw previously would have conserved the standard deviation of 1. But because we have this interspersed tanh layers in there, these tanh layers are squashing functions. And so they take your distribution and they slightly squash it. And so some gain is necessary to keep expanding it to fight the squashing. So it just turns out that 5 over 3 is a good value. So if we have something too small like 1, we saw that things will come towards 0. But if it's something too high, let's do 2. Then here we see that, well, let me do something a bit more extreme so it's a bit more visible. Let's try 3. Okay, so we see here that the saturations are starting to be way too large. So 3 would create way too saturated activations. So 5 over 3 is a good setting for a sandwich of linear layers with tanh activations. And it roughly stabilizes the standard deviation at a reasonable point. Now honestly, I have no idea where 5 over 3 came from in PyTorch when we were looking at the counting initialization. I see empirically that it stabilizes this sandwich of linear and tanh and that the saturation is in a good range. But I don't actually know if this came out of some math formula. I tried searching briefly for where this comes from, but I wasn't able to find anything. But certainly we see that empirically these are very nice ranges. Our saturation is roughly 5%, which is a pretty good number. And this is a good setting of the gain in this context. Similarly, we can do the exact same thing with the gradients. So here is a very same loop if it's a tanh, but instead of taking the layer dot out, I'm taking the grad. And then I'm also showing the mean and the standard deviation. And I'm plotting the histogram of these values. And so you'll see that the gradient distribution is fairly reasonable. And in particular, what we're looking for is that all the different layers in this sandwich has roughly the same gradient. Things are not shrinking or exploding. So we can, for example, come here and we can take a look at what happens if this gain was way too small. So this was 0.5. Then you see the, first of all, the activations are shrinking to zero, but also the gradients are doing something weird. The gradient started out here and then now they're like expanding out. And similarly, if we, for example, have a too high of a gain, so like three, then we see that also the gradients have, there's some asymmetry going on where as you go into deeper and deeper layers, the activations are also changing. And so that's not what we want. And in this case, we saw that without the use of batch norm, as we are going through right now, we have to very carefully set those gains to get nice activations in both the forward pass and the backward pass. Now, before we move on to batch normalization, I would also like to take a look at what happens when we have no 10H units here. So erasing all the 10H nonlinearities, but keeping the gain at five over three, we now have just a giant linear sandwich. So let's see what happens to the activations. As we saw before, the correct gain here is one, that is the standard deviation preserving gain. So 1.667 is too high. And so what's going to happen now is the following. I have to change this to be linear, because there's no more 10H layers. And let me change this to linear as well. So what we're seeing is the activations started out on the blue and have, by layer four, become very diffuse. So what's happening to the activations is this. And with the gradients on the top layer, the activation, the gradient statistics are the purple, and then they diminish as you go down deeper in the layers. And so basically you have an asymmetry in the neural net. And you might imagine that if you have very deep neural networks, say like 50 layers or something like that, this is not a good place to be. So that's why before batch normalization, this was incredibly tricky to set. In particular, if this is too large of a gain, this happens, and if it's too little of a gain, then this happens. So the opposite of that basically happens. Here we have a shrinking and a diffusion, depending on which direction you look at it from. And so certainly this is not what you want. And in this case, the correct setting of the gain is exactly 1, just like we're doing at initialization. And then we see that the statistics for the forward and the backward paths are well-behaved. And so the reason I want to show you this is that basically getting neural nets to train before these normalization layers and before the use of advanced optimizers like Adam, which we still have to cover, and residual connections and so on, training neural nets basically look like this. It's like a total balancing act. You have to make sure that everything is precisely orchestrated, and you have to care about the activations and the gradients and their statistics, and then maybe you can train something. But it was basically impossible to train very deep networks, and this is fundamentally the reason for that. You'd have to be very, very careful with your initialization. The other point here is, you might be asking yourself, by the way, I'm not sure if I covered this, why do we need these 10H layers at all? Why do we include them and then have to worry about the gain? And the reason for that, of course, is that if you just have a stack of linear layers, then certainly we're getting very easily nice activations and so on, but this is just a massive linear sandwich, and it turns out that it collapses to a single linear layer in terms of its representation power. So if you were to plot the output as a function of the input, you're just getting a linear function. No matter how many linear layers you stack up, you still just end up with a linear transformation. All the WX plus Bs just collapse into a large WX plus B with slightly different Ws and slightly different B. But interestingly, even though the forward pass collapses to just a linear layer, because of back propagation and the dynamics of the backward pass, the optimization actually is not identical. You actually end up with all kinds of interesting dynamics in the backward pass because of the way the chain rule is calculating it. And so optimizing a linear layer by itself and optimizing a sandwich of 10 linear layers, in both cases, those are just a linear transformation in the forward pass, but the training dynamics would be different. And there's entire papers that analyze, in fact, infinitely layered linear layers and so on. And so there's a lot of things that you can play with there. But basically the tanh nonlinearities allow us to turn this sandwich from just a linear chain into a neural network that can, in principle, approximate any arbitrary function. Okay, so now I've reset the code to use the linear tanh sandwich like before, and I've reset everything. So the gain is 5 over 3. We can run a single step of optimization, and we can look at the activation statistics of the forward pass and the backward pass. But I've added one more plot here that I think is really important to look at when you're training your neural nets and to consider. And ultimately what we're doing is we're updating the parameters of the neural net. So we care about the parameters and their values and their gradients. So here what I'm doing is I'm actually iterating over all the parameters available, and then I'm only restricting it to the two-dimensional parameters, which are basically the weights of these linear layers. And I'm skipping the biases, and I'm skipping the gammas and the betas and the bastrom just for simplicity. But you can also take a look at those as well. But what's happening with the weights is instructive by itself. So here we have all the different weights, their shapes. So this is the embedding layer, the first linear layer, all the way to the very last linear layer. And then we have the mean, the standard deviation of all these parameters. The histogram, and you can see that it actually doesn't look that amazing. So there's some trouble in paradise. Even though these gradients looked okay, there's something weird going on here. I'll get to that in a second. And the last thing here is the gradient to data ratio. So sometimes I like to visualize this as well, because what this gives you a sense of is what is the scale of the gradient compared to the scale of the actual values. And this is important because we're going to end up taking a step update that is the learning rate times the gradient onto the data. And so if the gradient has too large of a magnitude, if the numbers in there are too large compared to the numbers in data, then you'd be in trouble. But in this case, the gradient to data is our low numbers. So the values inside grad are 1000 times smaller than the values inside data in these weights, most of them. Now notably, that is not true about the last layer. And so the last layer actually here, the output layer, is a bit of a troublemaker in the way that this is currently arranged. Because you can see that the last layer here in pink takes on values that are much larger than some of the values inside the neural net. So the standard deviations are roughly 1 and negative 3 throughout, except for the last layer, which actually has roughly 1 and negative 2 standard deviation of gradients. And so the gradients on the last layer are currently about 100 times greater, sorry, 10 times greater than all the other weights inside the neural net. And so that's problematic, because in the simple stochastic gradient descent setup, you would be training this last layer about 10 times faster than you would be training the other layers at initialization. Now this actually kind of fixes itself a little bit if you train for a bit longer. So for example, if I greater than 1000, only then do a break. Let me reinitialize, and then let me do it 1000 steps. And after 1000 steps, we can look at the forward pass. Okay, so you see how the neurons are saturating a bit. And we can also look at the backward pass. But otherwise, they look good. They're about equal, and there's no shrinking to zero or exploding to infinities. And you can see that here in the weights, things are also stabilizing a little bit. So the tails of the last pink layer are actually coming in during the optimization. But certainly this is like a little bit troubling, especially if you are using a very simple update rule like stochastic gradient descent, instead of a modern optimizer like Adam. Now I'd like to show you one more plot that I usually look at when I train neural networks. And basically... Basically, the gradient-to-data ratio is not actually that informative, because what matters at the end is not the gradient-to-data ratio, but the update-to-data ratio, because that is the amount by which we will actually change the data in these tensors. So coming up here, what I'd like to do is I'd like to introduce a new update-to-data ratio. It's going to be a list, and we're going to build it out every single iteration. And here, I'd like to keep track of basically the ratio every single iteration. So without any gradients, I'm comparing the update, which is learning rate times the gradient. That is the update that we're going to apply to every parameter. So see, I'm iterating over all the parameters. And then I'm taking the basically standard deviation of the update we're going to apply and divide it by the actual content, the data of that parameter and its standard deviation. So this is the ratio of basically how great are the updates to the values in these tensors. Then we're going to take a log of it, and actually I'd like to take a log10, just so it's a nicer visualization. So we're going to be basically looking at the exponents of this division here, and then that item to pop out the float. And we're going to be keeping track of this for all the parameters and adding it to this UD tensor. So now let me reinitialize and run a thousand iterations. We can look at the activations, the gradients, and the parameter gradients as we did before. But now I have one more plot here to introduce. And what's happening here is we're iterating over all the parameters, and I'm constraining it again like I did here to just the weights. So the number of dimensions in these sensors is two. And then I'm basically plotting all of these update ratios over time. So when I plot this, I plot those ratios and you can see that they evolve over time during initialization to take on certain values. And then these updates are like start stabilizing usually during training. Then the other thing that I'm plotting here is I'm plotting here like an approximate value that is a rough guide for what it roughly should be. And it should be like roughly 1 and negative 3. And so that means that basically there's some values in this tensor and they take on certain values and the updates to them at every single iteration are no more than roughly one thousandth of the actual magnitude in those tensors. If this was much larger, like for example, if the log of this was like say negative 1, this is actually updating those values quite a lot. They're undergoing a lot of change. But the reason that the final layer here is an outlier is because this layer was artificially shrunk down to keep the softmax unconfident. So here you see how we multiply the weight by 0.1 in the initialization to make the last layer prediction less confident. That artificially made the values inside that tensor way too low. And that's why we're getting temporarily a very high ratio. And you see that that stabilizes over time once that weight starts to learn. But basically I like to look at the evolution of this update ratio for all my parameters usually. And I like to make sure that it's not too much above 1 and negative 3 roughly. So around negative 3 on this log plot. If it's below negative 3, usually that means that the parameters are not training fast enough. So if our learning weight was very low, let's do that experiment. Let's initialize. And then let's actually do a learning rate of say 1 and negative 3 here. So 0.001. If your learning rate is way too low, this plot will typically reveal it. So you see how all of these updates are way too small. So the size of the update is basically 10,000 times in magnitude to the size of the numbers in that tensor in the first place. So this is a symptom of training way too slow. So this is another way to sometimes set the learning rate and to get a sense of what that learning rate should be. And ultimately this is something that you would keep track of. If anything, the learning rate here is a little bit on the higher side because you see that we're above the black line of negative 3. We're somewhere around negative 2.5. It's like, okay. But everything is somewhat stabilizing. And so this looks like a pretty decent setting of learning rates and so on. But this is something to look at. And when things are miscalibrated, you will see very quickly. So for example, everything looks pretty well behaved, right? But just as a comparison, when things are not properly calibrated, what does that look like? Let me come up here and let's say that, for example, what do we do? Let's say that we forgot to apply this fan-in normalization. So the weights inside the linear layers are just a sample from a Gaussian in all the stages. What happens to our... how do we notice that something's off? Well, the activation plot will tell you, whoa, your neurons are way too saturated. The gradients are going to be all messed up. The histogram for these weights are going to be all messed up as well. And there's a lot of asymmetry. And then if we look here, I suspect it's all going to be also pretty messed up. So you see there's a lot of discrepancy in how fast these layers are learning. And some of them are learning way too fast. So negative 1, negative 1.5, those are very large numbers in terms of this ratio. Again, you should be somewhere around negative 3 and not much more above that. So this is how miscalibrations of your neural nets are going to manifest. And these kinds of plots here are a good way of sort of bringing those miscalibrations sort of to your attention and so you can address them. Okay, so so far we've seen that when we have this linear tanh sandwich, we can actually precisely calibrate the gains and make the activations, the gradients, and the parameters and the updates all look pretty decent. But it definitely feels a little bit like balancing of a pencil on your finger. And that's because this gain has to be very precisely calibrated. So now let's introduce batch normalization layers into the mix. Let's see how that helps fix the problem. So here I'm going to take the BatchNormalization1D class and I'm going to start placing it inside. And as I mentioned before, the standard typical place you would place it is between the linear layer, so right after it, but before the nonlinearity. But people have definitely played with that. And in fact, you can get very similar results, even if you place it after the nonlinearity. And the other thing that I wanted to mention is it's totally fine to also place it at the end after the last linear layer and before the loss function. So this is potentially fine as well. And in this case, this would be output, would be vocab size. Now because the last layer is batch norm, we would not be changing the weight to make the softmax less confident. We'd be changing the gamma. Because gamma, remember, in the batch norm, is the variable that multiplicatively interacts with the output of that normalization. So we can initialize this sandwich now. We can train. And we can see that the activations are going to, of course, look very good. And they are going to necessarily look good, because now before every single tanh layer, there is a normalization in the batch norm. So this is, unsurprisingly, all looks pretty good. It's going to be standard deviation of roughly 0.65, 2%, and roughly equal standard deviation throughout the entire layers. So everything looks very homogeneous. The gradients look good. The weights look good in their distributions. And then the updates also look pretty reasonable. We're going above negative 3 a little bit, but not by too much. So all the parameters are training at roughly the same rate here. But now what we've gained is we are going to be slightly less brittle with respect to the gain of these. So for example, I can make the gain be, say, 0.2 here, which is much, much slower than what we had with the tanh. But as we'll see, the activations will actually be exactly unaffected. And that's because of, again, this explicit normalization. The gradients are going to look OK. The weight gradients are going to look OK. But actually, the updates will change. And so even though the forward and backward paths, to a very large extent, look OK, because of the backward paths of the batch norm and how the scale of the incoming activations interacts in the batch norm and its backward paths, this is actually changing the scale of the updates on these parameters. So the gradients of these weights are affected. So we still don't get a completely free path to pass in arbitrary weights here. But everything else is significantly more robust in terms of the forward, backward, and the weight gradients. It's just that you may have to retune your learning rate if you are changing sufficiently the scale of the activations that are coming into the batch norms. So here, for example, we changed the gains of these linear layers to be greater. And we're seeing that the updates are coming out lower as a result. And then finally, we can also, if we are using batch norms, we don't actually need to necessarily  let me reset this to 1 so there's no gain  we don't necessarily even have to normalize by fan-in sometimes. So if I take out the fan-in, so these are just now random Gaussian, we'll see that because of batch norm, this will actually be relatively well-behaved. So this looks, of course, in the forward path, looks good. The gradients look good. The weight updates look okay. A little bit of fat tails in some of the layers. And this looks okay as well. But as you can see, we're significantly below negative 3, so we'd have to bump up the learning rate of this batch norm so that we are training more properly. And in particular, looking at this, roughly looks like we have to 10x the learning rate to get to about 1e negative 3. So we'd come here and we would change this to be update of 1.0. And if I reinitialize, then we'll see that everything still, of course, looks good. And now we are roughly here. And we expect this to be an okay training run. So long story short, we are significantly more robust to the gain of these linear layers, whether or not we have to apply the fan-in. And then we can change the gain, but we actually do have to worry a little bit about the update scales and making sure that the learning rate is properly calibrated here. But the activations of the forward, backward paths and the updates are looking significantly more well-behaved, except for the global scale that is potentially being adjusted here. Okay, so now let me summarize. There are three things I was hoping to achieve with this section. Number one, I wanted to introduce you to batch normalization, which is one of the first modern innovations that we're looking into that helped stabilize very deep neural networks and their training. And I hope you understand how the batch normalization works and how it would be used in a neural network. Number two, I was hoping to PyTorchify some of our code and wrap it up into these modules. So like linear, BashNorm1D, 10H, et cetera. These are layers or modules, and they can be stacked up into neural nets like Lego building blocks. And these layers actually exist in PyTorch. And if you import TorchNN, then you can actually, the way I've constructed it, you can simply just use PyTorch by prepending NN. to all these different layers, and actually everything will just work because the API that I've developed here is identical to the API that PyTorch uses. And the implementation also is basically, as far as I'm aware, identical to the one in PyTorch. And number three, I tried to introduce you to the diagnostic tools that you would use to understand whether your neural network is in a good state dynamically. So we are looking at the statistics and histograms and activation of the forward pass activations, the backward pass gradients. And then also we're looking at the weights that are going to be updated as part of stochastic gradient ascent. And we're looking at their means, standard deviations, and also the ratio of gradients to data, or even better, the updates to data. And we saw that typically, we don't actually look at it as a single snapshot frozen in time at some particular iteration. Typically people look at this as over time, just like I've done here. And they look at these update to data ratios and they make sure everything looks okay. And in particular, I said that 1 in negative 3, or basically negative 3 on the log scale, is a good rough heuristic for what you want this ratio to be. And if it's way too high, then probably the learning rate or the updates are a little too big. And if it's way too small, then the learning rate is probably too small. So that's just some of the things that you may want to play with when you try to get your neural network to work very well. Now, there's a number of things I did not try to achieve. I did not try to beat our previous performance, as an example, by introducing the BatchNorm layer. Actually, I did try, and I found that I used the learning rate finding mechanism that I've described before. I tried to train the BatchNorm layer, a BatchNorm neural net, and I actually ended up with results that are very, very similar to what we've obtained before. And that's because our performance now is not bottlenecked by the optimization, which is what BatchNorm is helping with. The performance at this stage is bottlenecked by what I suspect is the context length of our context. So currently, we are taking three characters to predict the fourth one, and I think we need to go beyond that, and we need to look at more powerful architectures, like recurrent neural networks and transformers, in order to further push the log probabilities that we're achieving on this dataset. And I also did not try to have a full explanation of all of these activations, the gradients, and the backward pass, and the statistics of all these gradients. And so you may have found some of the parts here unintuitive, and maybe you were slightly confused about, okay, if I change the gain here, how come that we need a different learning rate? And I didn't go into the full detail, because you'd have to actually look at the backward pass of all these different layers and get an intuitive understanding of how all that works. And I did not go into that in this lecture. The purpose really was just to introduce you to the diagnostic tools and what they look like, but there's still a lot of work remaining on the intuitive level to understand the initialization, the backward pass, and how all of that interacts. But you shouldn't feel too bad, because honestly, we are getting to the cutting edge of where the field is. We certainly haven't, I would say, solved initialization, and we haven't solved backpropagation. And these are still very much an active area of research. People are still trying to figure out what is the best way to initialize these networks, what is the best update rule to use, and so on. So none of this is really solved, and we don't really have all the answers to all these cases. But at least we're making progress, and at least we have some tools to tell us whether or not things are on the right track for now. So I think we've made positive progress in this lecture, and I hope you enjoyed that, and I will see you next time. Hi everyone. So today we are once again continuing our implementation of Makemore. Now so far we've come up to here, Montalio perceptrons, and our neural net looked like this, and we were implementing this over the last few lectures. Now I'm sure everyone is very excited to go into recurrent neural networks and all of their variants and how they work, and the diagrams look cool and it's very exciting and interesting and we're going to get a better result, but unfortunately I think we have to remain here for one more lecture. And the reason for that is we've already trained this Montalio perceptron, right, and we are getting pretty good loss, and I think we have a pretty decent understanding of the architecture and how it works, but the line of code here that I take an issue with is here, lost at backward. That is, we are taking PyTorch autograd and using it to calculate all of our gradients along the way, and I would like to remove the use of lost at backward, and I would like us to write our backward pass manually on the level of tensors. And I think that this is a very useful exercise for the following reasons. I actually have an entire blog post on this topic, but I like to call backpropagation a leaky abstraction. And what I mean by that is backpropagation doesn't just make your neural networks just work magically. It's not the case that you can just stack up arbitrary Lego blocks of differentiable functions and just cross your fingers and backpropagate and everything is great. Things don't just work automatically. It is a leaky abstraction in the sense that you can shoot yourself in the foot if you do not understand its internals. It will magically not work or not work optimally, and you will need to understand how it works under the hood if you're hoping to debug it and if you are hoping to address it in your neural net. So this blog post here from a while ago goes into some of those examples. So for example, we've already covered them, some of them already. For example, the flat tails of these functions and how you do not want to saturate them too much because your gradients will die. The case of dead neurons, which I've already covered as well. The case of exploding or vanishing gradients in the case of recurrent neural networks, which we are about to cover. And then also you will often come across some examples in the wild. This is a snippet that I found in a random code base on the internet where they actually have a very subtle but pretty major bug in their implementation. And the bug points at the fact that the author of this code does not actually understand backpropagation. So what they're trying to do here is they're trying to clip the loss at a certain maximum value. But actually what they're trying to do is they're trying to clip the gradients to have a maximum value instead of trying to clip the loss at a maximum value. And indirectly, they're basically causing some of the outliers to be actually ignored because when you clip a loss of an outlier, you are setting its gradient to zero. And so have a look through this and read through it. But there's basically a bunch of subtle issues that you're going to avoid if you actually know what you're doing. And that's why I don't think it's the case that because PyTorch or other frameworks offer autograd, it is okay for us to ignore how it works. Now, we've actually already covered autograd and we wrote micrograd. But micrograd was an autograd engine only on the level of individual scalars. So the atoms were single individual numbers. And I don't think it's enough and I'd like us to basically think about backpropagation on the level of tensors as well. And so in a summary, I think it's a good exercise. I think it is very, very valuable. You're going to become better at debugging neural networks and making sure that you understand what you're doing. It is going to make everything fully explicit. So you're not going to be nervous about what is hidden away from you. And basically, in general, we're going to emerge stronger. And so let's get into it. A bit of a fun historical note here is that today, writing your backward pass by hand and manually is not recommended, and no one does it except for the purposes of exercise. But about 10 years ago in deep learning, this was fairly standard and in fact pervasive. So at the time, everyone used to write their backward pass by hand manually, including myself, and it's just what you would do. So we used to write backward pass by hand and now everyone just calls lost a backward. We've lost something. I wanted to give you a few examples of this. So here's a 2006 paper from Jeff Hinton and Roslyn Slavkinov in science that was influential at the time. And this was training some architectures called restricted Boltzmann machines. And basically, it's an autoencoder trained here. And this is from roughly 2010. I had a library for training restricted Boltzmann machines. And this was at the time written in MATLAB. So Python was not used for deep learning pervasively. It was all MATLAB. And MATLAB was this scientific computing package that everyone would use. So we would write MATLAB, which is barely a programming language as well, but it had a very convenient tensor class. And it was this computing environment and you would run here. It would all run on the CPU, of course, but you would have very nice plots to go with it and a built in debugger. And it was pretty nice. Now, the code in this package in 2010 that I wrote for fitting restricted Boltzmann machines to a large extent is recognizable, but I wanted to show you how you would, well, I'm creating the data and the XY batches. I'm initializing the neural net. So it's got weights and biases, just like we're used to. And then this is the training loop where we actually do the forward pass. And then here at this time, they didn't even necessarily use back propagation to train neural networks. So this in particular implements contrastive divergence, which estimates a gradient. And then here we take that gradient and use it for a parameter update along the lines that we're used to. Yeah, here. But you can see that basically people are meddling with these gradients directly and inline and themselves. It wasn't that common to use an autograd engine. Here's one more example from a paper of mine from 2014 called Deep Fragment Embeddings. And here what I was doing is I was aligning images and text. And so it's kind of like a clip if you're familiar with it. But instead of working on the level of entire images and entire sentences, it was working on the level of individual objects and little pieces of sentences. And I was embedding them and then calculating a very much like a clip like loss. And I dug up the code from 2014 of how I implemented this. And it was already in NumPy and Python. And here I'm implementing the cost function. And it was standard to implement not just the cost, but also the backward pass manually. So here I'm calculating the image embeddings, sentence embeddings, the loss function. I calculate the scores. This is the loss function. And then once I have the loss function, I do the backward pass right here. So I backward through the loss function and through the neural net and I append regularization. So everything was done by hand manually. And you would just write out the backward pass. And then you would use a gradient checker to make sure that your numerical estimate of the gradient agrees with the one you calculated during the back propagation. So this was very standard for a long time. But today, of course, it is standard to use an AutoGrad engine. But it was definitely useful. And I think people sort of understood how these neural networks work on a very intuitive level. And so I think it's a good exercise again. And this is where we want to be. Okay, so just as a reminder from our previous lecture, this is the Jupyter notebook that we implemented at the time. And we're going to keep everything the same. So we're still going to have a two-layer multi-layer perceptron with a batch normalization layer. So the forward pass will be basically identical to this lecture. But here we're going to get rid of lost at backward. And instead, we're going to write the backward pass manually. Now, here's the starter code for this lecture. We are becoming a backprop ninja in this notebook. And the first few cells here are identical to what we are used to. So we are doing some imports, loading the dataset, and processing the dataset. None of this changed. Now, here I'm introducing a utility function that we're going to use later to compare the gradients. So in particular, we are going to have the gradients that we estimate manually ourselves. And we're going to have gradients that PyTorch calculates. And we're going to be checking for correctness, assuming, of course, that PyTorch is correct. Then here we have the initialization that we are quite used to. So we have our embedding table for the characters, the first layer, second layer, and a batch normalization in between. And here's where we create all the parameters. Now, you will note that I changed the initialization a little bit to be small numbers. So normally, you would set the biases to be all zero. Here I am setting them to be small random numbers. And I'm doing this because if your variables are initialized to exactly zero, sometimes what can happen is that can mask an incorrect implementation of a gradient. Because when everything is zero, it simplifies and gives you a much simpler expression of the gradient than you would otherwise get. And so by making it small numbers, I'm trying to unmask those potential errors in these calculations. You also notice that I'm using b1 in the first layer. I'm using a bias despite batch normalization right afterwards. So this would typically not be what you do because we talked about the fact that you don't need a bias. But I'm doing this here just for fun because we're going to have a gradient with respect to it and we can check that we are still calculating it correctly even though this bias is spurious. So here I'm calculating a single batch. And then here I am doing a forward pass. Now, you'll notice that the forward pass is significantly expanded from what we are used to. Here the forward pass was just here. Now, the reason that the forward pass is longer is for two reasons. Number one, here we just had an f dot cross entropy. But here I am bringing back an explicit implementation of the loss function. And number two, I've broken up the implementation into manageable chunks. So we have a lot more intermediate tensors along the way in the forward pass. And that's because we are about to go backwards and calculate the gradients in this back propagation from the bottom to the top. So we're going to go upwards. And just like we have, for example, the logprobs tensor in a forward pass, in a backward pass we're going to have a dlogprobs which is going to store the derivative of the loss with respect to the logprobs tensor. And so we're going to be prepending d to every one of these tensors and calculating it along the way of this back propagation. So as an example, we have a b in raw here. We're going to be calculating a d b in raw. So here I'm telling PyTorch that we want to retain the grad of all these intermediate values because here in exercise one, we're going to calculate the backward pass. So we're going to calculate all these d variables and use the CMP function I've introduced above to check our correctness with respect to what PyTorch is telling us. This is going to be exercise one where we sort of back propagate through this entire graph. Now, just to give you a very quick preview of what's going to happen in exercise two and below, here we have fully broken up the loss and back propagated through it manually in all the little atomic pieces that make it up. But here we're going to collapse the loss into a single cross entropy cull and instead we're going to analytically derive using math and paper and pencil the gradient of the loss with respect to the logits. And instead of back propagating through all of its little chunks one at a time, we're just going to analytically derive what that gradient is and we're going to implement that which is much more efficient as we'll see in a bit. Then we're going to do the exact same thing for Batch Normalization. So instead of breaking up BatchNorm into all the little tiny components, we're going to use pen and paper and mathematics and calculus to derive the gradient through the BatchNorm layer. So we're going to calculate the backward pass through BatchNorm layer in a much more efficient expression instead of backward propagating through all of its little pieces independently. So that's going to be exercise three. And then in exercise four we're going to put it all together and this is the full code of training this two-layer MLP. And we're going to basically insert our manual backprop and we're going to take up loss.backward and you will basically see that you can get all the same results using fully your own code. And the only thing we're using from PyTorch is the torch.tensor to make the calculations efficient. But otherwise you will understand fully what it means to forward and backward the neural net and train it and I think that'll be awesome. So let's get to it. Okay so I ran all the cells of this notebook all the way up to here and I'm going to erase this and I'm going to start implementing backward pass starting with dLogProps. So we want to understand what should go here to calculate the gradient of the loss with respect to all the elements of the LogProps tensor. Now I'm going to give away the answer here but I wanted to put a quick note here that I think what would be most pedagogically useful for you is to actually go into the description of this video and find the link to this Jupyter notebook. You can find it both on GitHub but you can also find Google Colab with it so you don't have to install anything you'll just go to a website on Google Colab and you can try to implement these derivatives or gradients yourself and then if you are not able to come to my video and see me do it. And so work in tandem and try it first yourself and then see me give away the answer. And I think that'll be most valuable to you and that's how I recommend you go through this lecture. So we are starting here with dLogProps. Now dLogProps will hold the derivative of the loss with respect to all the elements of LogProps. What is inside LogProps? The shape of this is 32 by 27. So it's not going to surprise you that dLogProps should also be an array of size 32 by 27 because we want the derivative of the loss with respect to all of its elements. So the sizes of those are always going to be equal. Now how does LogProps influence the loss? Loss is negative LogProps indexed with range of n and yb and then the mean of that. Now just as a reminder yb is just basically an array of all the correct indices. So what we're doing here is we're taking the LogProps array of size 32 by 27. And then we are going in every single row and in each row we are plugging out the index 8 and then 14 and 15 and so on. So we're going down the rows that's the iterator range of n and then we are always plucking out the index of the column specified by this tensor yb. So in the zeroth row we are taking the eighth column, in the first row we're taking the 14th column, etc. And so LogProps at this plucks out all those log probabilities of the correct next character in a sequence. So that's what that does and the shape of this or the size of it is of course 32 because our batch size is 32. So these elements get plucked out and then their mean and the negative of that becomes loss. So I always like to work with simpler examples to understand the numerical form of the derivative. What's going on here is once we've plucked out these examples we're taking the mean and then the negative. So the loss basically, I can write it this way, is the negative of say a plus b plus c and the mean of those three numbers would be say negative would divide three. That would be how we achieve the mean of three numbers a, b, c although we actually have 32 numbers here. And so what is basically the loss by say like da, right? Well if we simplify this expression mathematically this is negative 1 over 3 of a and negative plus negative 1 over 3 of b plus negative 1 over 3 of c. And so what is the loss by da? It's just negative 1 over 3. It's just negative 1 over 3. And so you can see that if we don't just have a, b, and c but we have 32 numbers then d loss by d, you know, every one of those numbers is going to be 1 over n more generally because n is the size of the batch, 32 in this case. So d loss by d log props is negative 1 over n in all these places. Now what about the other elements inside log props? Because log props is a large array. You see that log props dot shape is 32 by 27. But only 32 of them participate in the loss calculation. So what's the derivative of all the other most of the elements that do not get plucked out here? Well their loss intuitively is zero. Sorry, their gradient intuitively is zero. And that's because they do not participate in the loss. So most of these numbers inside this tensor does not feed into the loss. And so if we were to change these numbers then the loss doesn't change, which is the equivalent of us saying that the derivative of the loss with respect to them is zero. They don't impact it. So here's a way to implement this derivative then. We start out with tors dot zeros of shape 32 by 27. Or let's just say instead of doing this because we don't want to hard code numbers, let's do tors dot zeros like log props. So basically this is going to create an array of zeros exactly in the shape of log props. And then we need to set the derivative of negative 1 over n inside exactly these locations. So here's what we can do. The log props indexed in the identical way will be just set to negative 1 over 0 divide n. Right, just like we derived here. So now let me erase all of this reasoning. And then this is the candidate derivative for the log props. Let's uncomment the first line and check that this is correct. Okay, so CMP ran. And let's go back to CMP. And you see that what it's doing is it's calculating if the calculated value by us, which is dt, is exactly equal to t dot grad as calculated by PyTorch. And then this is making sure that all of the elements are exactly equal and then converting this to a single Boolean value. Because we don't want a Boolean tensor, we just want a Boolean value. And then here we are making sure that, okay, if they're not exactly equal, maybe they are approximately equal because of some floating point issues. But they're very, very close. So here we are using torch.allclose, which has a little bit of a wiggle available because sometimes you can get very, very close. But if you use a slightly different calculation because of floating point arithmetic, you can get a slightly different result. So this is checking if you get an approximately close result. And then here we are checking the maximum, basically the value that has the highest difference. And what is the difference in the absolute value difference between those two? And so we are printing whether we have an exact equality, an approximate equality, and what is the largest difference? And so here we see that we actually have exact equality. And so therefore, of course, we also have an approximate equality. And the maximum difference is exactly zero. So basically, rdlogprops is exactly equal to what PyTorch calculated to be logprops dot grad in its backpropagation. So, so far, we're doing pretty well. Okay, so let's now continue our backpropagation. We have that logprops depends on probs through a log. So all the elements of probs are being element-wise applied log2. Now, if we want deep props, then, then remember your micrograph training. We have like a log node, it takes in probs and creates logprops. And deep props will be the local derivative of that individual operation log times the derivative of the loss with respect to its output, which in this case is dlogprops. So what is the local derivative of this operation? Well, we are taking log element-wise, and we can come here and we can see, well, from alpha is your friend, that d by dx of log of x is just simply one over x. So therefore, in this case, x is probs. So we have d by dx is one over x, which is one over probs. And then this is the local derivative, and then times we want to chain it. So this is chain rule times dlog probs. Then let me uncomment this and let me run the cell in place. And we see that the derivative of probs as we calculated here is exactly correct. And so notice here how this works. Probs is going to be inverted and then element-wise multiplied here. So if your probs is very, very close to 1, that means your network is currently predicting the character correctly, then this will become 1 over 1 and dlog probs just gets passed through. But if your probabilities are incorrectly assigned, so if the correct character here is getting a very low probability, then 1.0 dividing by it will boost this and then multiply by dlog probs. So basically what this line is doing intuitively is it's taking the examples that have a very low probability currently assigned and it's boosting their gradient. You can look at it that way. Next up is countSumInv. So we want the derivative of this. Now let me just pause here and kind of introduce what's happening here in general because I know it's a little bit confusing. We have the logits that come out of the neural net. Here what I'm doing is I'm finding the maximum in each row and I'm subtracting it for the purpose of numerical stability. And we talked about how if you do not do this, you run into numerical issues if some of the logits take on too large values because we end up exponentiating them. So this is done just for safety, numerically. Then here's the exponentiation of all the logits to create our counts. And then we want to take the sum of these counts and normalize so that all of the probs sum to 1. Now here instead of using 1 over countSum, I use raised to the power of negative 1. Mathematically they are identical. I just found that there's something wrong with the PyTorch implementation of the backward pass of division, and it gives a weird result. But that doesn't happen for star star negative 1, so I'm using this formula instead. But basically all that's happening here is we've got the logits, we want to exponentiate all of them, and we want to normalize the counts to create our probabilities. It's just that it's happening across multiple lines. Now here we want to first backpropagate into countSumInf and then into counts as well. So what should be the countSumInf? Now we actually have to be careful here because we have to scrutinize and be careful with the shapes. So counts.shape and then countSumInf.shape are different. So in particular, counts is 32 by 27, but this countsSumInf is 32 by 1. And so in this multiplication here, we also have an implicit broadcasting that PyTorch will do because it needs to take this column tensor of 32 numbers and replicate it horizontally 27 times to align these two tensors so it can do an element-wise multiply. So really what this looks like is the following, using a toy example again. What we really have here is just props is counts times countsSumInf, so it's C equals A times B, but A is 3 by 3 and B is just 3 by 1, a column tensor. And so PyTorch internally replicated this elements of B, and it did that across all the columns. So for example, B1, which is the first element of B, would be replicated here across all the columns in this multiplication. And now we're trying to backpropagate through this operation to countsSumInf. So when we are calculating this derivative, it's important to realize that this looks like a single operation, but actually it's two operations applied sequentially. The first operation that PyTorch did is it took this column tensor and replicated it across all the columns, basically 27 times. So that's the first operation. It's a replication. And then the second operation is the multiplication. So let's first backprop through the multiplication. If these two arrays were of the same size and we just have A and B, both of them 3 by 3, then how do we backpropagate through a multiplication? So if we just have scalars and not tensors, then if you have C equals A times B, then what is the derivative of C with respect to B? Well, it's just A, and so that's the local derivative. So here in our case, undoing the multiplication and backpropagating through just the multiplication itself, which is element-wise, is going to be the local derivative, which in this case is simply counts, because counts is the A. So this is the local derivative, and then times, because of the chain rule, dprops. So this here is the derivative or the gradient with respect to replicated B. But we don't have a replicated B. We just have a single B column. So how do we now backpropagate through the replication? Intuitively, this B1 is the same variable, and it's just reused multiple times. So you can look at it as being equivalent to a case we've encountered in micrograd. So here I'm just pulling out a random graph we used in micrograd. We had an example where a single node has its output feeding into two branches of basically the graph until the loss function. We're talking about how the correct thing to do in the backward pass is we need to sum all the gradients that arrive at any one node. So across these different branches, the gradients would sum. So if a node is used multiple times, the gradients for all of its uses sum during backpropagation. So here B1 is used multiple times in all these columns, and therefore the right thing to do here is to sum horizontally across all the rows. So we want to sum in dimension 1, but we want to retain this dimension so that the countSumInv and its gradient are going to be exactly the same shape. So we want to make sure that we keep them as true so we don't lose this dimension. And this will make the countSumInv be exactly shaped 32 by 1. So revealing this comparison as well and running this, we see that we get an exact match. So this derivative is exactly correct. And let me erase this. Now let's also backpropagate into counts, which is the other variable here to create props. So from props to countSumInv, we just did that. Let's go into counts as well. So dcounts will be... dcounts is our A, so dc by da is just B, so therefore it's countSumInv, and then times chain rule dprops. Now countSumInv is 32 by 1. dprops is 32 by 27. So those will broadcast fine and will give us dcounts. There's no additional summation required here. There will be a broadcasting that happens in this multiply here because countSumInv needs to be replicated again to correctly multiply dprops, but that's going to give the correct result as far as this single operation is concerned. So we've backpropagated from props to counts, but we can't actually check the derivative of counts. I have it much later on, and the reason for that is because countSumInv depends on counts. And so there's a second branch here that we have to finish because countSumInv backpropagates into countSum, and countSum will backpropagate into counts. And so counts is a node that is being used twice. It's used right here into props, and it goes through this other branch through countSumInv. So even though we've calculated the first contribution of it, we still have to calculate the second contribution of it later. Okay, so we're continuing with this branch. We have the derivative for countsSumInv. Now we want the derivative of countsSum. So dcountsSum equals, what is the local derivative of this operation? So this is basically an element-wise 1 over countsSum. So countsSum raised to the power of negative 1 is the same as 1 over countsSum. If we go to WolframAlpha, we see that x to the negative 1, d by dx of it, is basically negative x to the negative 2. Negative 1 over x squared is the same as negative x to the negative 2. So dcountsSum here will be local derivative is going to be negative countsSum to the negative 2. That's the local derivative, times chain rule, which is dcountsSumInv. So that's dcountsSum. Let's uncomment this and check that I am correct. Okay, so we have perfect equality. And there's no sketchiness going on here with any shapes, because these are of the same shape. Okay, next up, we want to backpropagate through this line. We have that countsSum is counts.sum along the rows. So I wrote out some help here. We have to keep in mind that counts, of course, is 32 by 27, and countsSum is 32 by 1. So in this backpropagation, we need to take this column of derivatives and transform it into an array of derivatives, two-dimensional array. So what is this operation doing? We're taking some kind of an input, like, say, a 3x3 matrix A, and we are summing up the rows into a column tensor B, B1, B2, B3, that is basically this. So now we have the derivatives of the loss with respect to B, all the elements of B. And now we want the derivative of the loss with respect to all these little As. So how do the Bs depend on the As is basically what we're after. What is the local derivative of this operation? Well, we can see here that B1 only depends on these elements here. The derivative of B1 with respect to all of these elements down here is 0. But for these elements here, like A11, A12, et cetera, the local derivative is 1, right? So DB1 by DA11, for example, is 1. So it's 1, 1, and 1. So when we have the derivative of the loss with respect to B1, the local derivative of B1 with respect to these inputs is 0s here, but it's 1 on these guys. So in the chain rule, we have the local derivative times sort of the derivative of B1. And so because the local derivative is 1 on these three elements, the local derivative multiplying the derivative of B1 will just be the derivative of B1. And so you can look at it as a router. Basically, an addition is a router of gradient. Whatever gradient comes from above, it just gets routed equally to all the elements that participate in that addition. So in this case, the derivative of B1 will just flow equally to the derivative of A11, A12, and A13. So if we have a derivative of all the elements of B in this column tensor, which is d counts sum that we've calculated just now, we basically see that what that amounts to is all of these are now flowing to all these elements of A, and they're doing that horizontally. So basically what we want is we want to take the d counts sum of size 32 by 1, and we just want to replicate it 27 times horizontally to create 32 by 27 array. So there's many ways to implement this operation. You could, of course, just replicate the tensor, but I think maybe one clean one is that d counts is simply torched dot once like, so just two-dimensional arrays of ones in the shape of counts, so 32 by 27, times d counts sum. So this way we're letting the broadcasting here basically implement the replication. You can look at it that way. But then we have to also be careful because d counts was already calculated. We calculated it earlier here, and that was just the first branch, and we're now finishing the second branch. So we need to make sure that these gradients add, so plus equals. And then here, let's comment out the comparison, and let's make sure, crossing fingers, that we have the correct result. So PyTorch agrees with us on this gradient as well. Okay, hopefully we're getting a hang of this now. Counts is an element-wise exp of norm logits. So now we want d norm logits, and because it's an element-wise operation, everything is very simple. What is the local derivative of e to the x? It's famously just e to the x, so this is the local derivative. That is the local derivative. Now, we already calculated it, and it's inside counts, so we may as well potentially just reuse counts. That is the local derivative, times d counts. Funny as that looks. Counts times d counts is the derivative on the norm logits. And now let's erase this, and let's verify, and it looks good. So that's norm logits. Okay, so we are here on this line now, d norm logits. We have that, and we're trying to calculate d logits and d logit maxes. So backpropagating through this line. Now, we have to be careful here, because the shapes, again, are not the same, and so there's an implicit broadcasting happening here. So norm logits has the shape of 32 by 27. Logits does as well, but logit maxes is only 32 by 1, so there's a broadcasting here in the minus. Now, here I tried to sort of write out a toy example again. We basically have that this is our c equals a minus b, and we see that because of the shape, these are 3 by 3, but this one is just a column. And so, for example, every element of c, we have to look at how it came to be. And every element of c is just the corresponding element of a minus basically that associated b. So it's very clear now that the derivatives of every one of these c's with respect to their inputs are 1 for the corresponding a, and it's a negative 1 for the corresponding b. And so, therefore, the derivatives on the c will flow equally to the corresponding a's and then also to the corresponding b's, but then in addition to that, the b's are broadcast, so we'll have to do the additional sum just like we did before. And, of course, the derivatives for b's will undergo a minus, because the local derivative here is negative 1. So dc32 by db3 is negative 1. So let's just implement that. Basically, dlogits will be exactly copying the derivative on normlogits. So dlogits equals dnormlogits, and I'll do a.clone for safety, so we're just making a copy. And then we have that dlogitmaxes will be the negative of dnormlogits because of the negative sign. And then we have to be careful because logitmaxes is a column, and so just like we saw before, because we keep replicating the same elements across all the columns, then in the backward pass, because we keep reusing this, these are all just like separate branches of use of that one variable. And so, therefore, we have to do a sum along 1 with keepthem equals true so that we don't destroy this dimension. And then dlogitmaxes will be the same shape. Now, we have to be careful because this dlogits is not the final dlogits, and that's because not only do we get gradient signal into logits through here, but logitmaxes is a function of logits, and that's a second branch into logits. So this is not yet our final derivative for logits. We will come back later for the second branch. For now, dlogitmaxes is the final derivative. So let me uncomment this CMP here, and let's just run this and logitmaxes if PyTorch agrees with us. So that was the derivative into through this line. Now, before we move on, I want to pause here briefly, and I want to look at these logitmaxes and especially their gradients. We've talked previously in the previous lecture that the only reason we're doing this is for the numerical stability of the softmax that we are implementing here. And we talked about how if you take these logits for any one of these examples, so one row of this logits tensor, if you add or subtract any value equally to all the elements, then the value of the probs will be unchanged. You're not changing the softmax. The only thing that this is doing is it's making sure that exp doesn't overflow. And the reason we're using a max is because then we are guaranteed that each row of logits, the highest number, is zero. And so this will be safe. And so basically that has repercussions. If it is the case that changing logitmaxes does not change the probs, and therefore does not change the loss, then the gradient on logitmaxes should be zero, because saying those two things is the same. So indeed, we hope that this is very, very small numbers. Indeed, we hope this is zero. Now, because of floating-point sort of wonkiness, this doesn't come out exactly zero. Only in some of the rows it does. But we get extremely small values, like 1e-9 or 10. And so this is telling us that the values of logitmaxes are not impacting the loss, as they shouldn't. It feels kind of weird to backpropagate through this branch, honestly, because if you have any implementation of, like, f.crossentropy in PyTorch, and you block together all of these elements, and you're not doing the backpropagation piece by piece, then you would probably assume that the derivative through here is exactly zero. So you would be sort of skipping this branch, because it's only done for numerical stability. But it's interesting to see that even if you break up everything into the full atoms and you still do the computation as you'd like with respect to numerical stability, the correct thing happens, and you still get very, very small gradients here, basically reflecting the fact that the values of these do not matter with respect to the final loss. Okay, so let's now continue backpropagation through this line here. We've just calculated the logitmaxes, and now we want to backprop into logits through this second branch. Now, here, of course, we took logits, and we took the max along all the rows, and then we looked at its values here. Now, the way this works is that in PyTorch, this thing here, the max returns both the values, and it returns the indices at which those values to count the maximum value. Now, in the forward pass, we only used values because that's all we needed, but in the backward pass, it's extremely useful to know about where those maximum values occurred, and we have the indices at which they occurred. And this will, of course, help us do the backpropagation because what should the backward pass be here in this case? We have the logit tensor, which is 32 by 27, and in each row, we find the maximum value, and then that value gets plucked out into logit maxes. And so intuitively, basically, the derivative flowing through here then should be 1 times the local derivative is 1 for the appropriate entry that was plucked out, and then times the global derivative of the logit maxes. So really what we're doing here, if you think through it, is we need to take the d logit maxes, and we need to scatter it to the correct positions in these logits from where the maximum values came. And so I came up with one line of code that does that. Let me just erase as much stuff as I can. You could do it very similar to what we've done here, where we create a zeros and then we populate the correct elements. We use the indices here and we would set them to be 1. But you can also use one-hot. So f.one-hot, and then I'm taking the logist.max over the first dimension, dot indices, and I'm telling PyTorch that the dimension of every one of these tensors should be 27. And so what this is going to do is... Okay, I apologize, this is crazy. PLT.imshow of this. It's really just an array of where the maxes came from in each row, and that element is 1 and all the other elements are 0. So it's a one-hot vector in each row, and these indices are now populating a single one in the proper place. And then what I'm doing here is I'm multiplying by the logit.max. And keep in mind that this is a column of 32 by 1. And so when I'm doing this times the logit.max, the logit.max will broadcast, and that column will get replicated, and then an element-wise multiply will ensure that each of these just gets routed to whichever one of these bits is turned on. And so that's another way to implement this kind of an operation, and both of these can be used. I just thought I would show an equivalent way to do it. And I'm using plus equals because we already calculated the logits here, and this is now the second branch. So let's look at logits and make sure that this is correct. And we see that we have exactly the correct answer. Next up, we want to continue with logits here. That is an outcome of a matrix multiplication and a bias offset in this linear layer. So I've printed out the shapes of all these intermediate tensors. We see that logits is, of course, 32 by 27, as we've just seen. Then the h here is 32 by 64, so these are 64-dimensional hidden states. And then this W matrix projects those 64-dimensional vectors into 27 dimensions, and then there's a 27-dimensional offset, which is a one-dimensional vector. Now, we should note that this plus here actually broadcasts because h multiplied by W2 will give us a 32 by 27. And so then this plus B2 is a 27-dimensional vector here. Now, in the rules of broadcasting, what's going to happen with this bias vector is that this one-dimensional vector of 27 will get aligned with a padded dimension of 1 on the left, and it will basically become a row vector, and then it will get replicated vertically 32 times to make it 32 by 27, and then there's an element-wise multiply. Now, the question is, how do we backpropagate from logits to the hidden states, the weight matrix W2, and the bias B2? And you might think that we need to go to some matrix calculus, and then we have to look up the derivative for a matrix multiplication, but actually you don't have to do any of that, and you can go back to first principles and derive this yourself on a piece of paper. And specifically what I like to do and what I find works well for me is you find a specific small example that you then fully write out, and then in the process of analyzing how that individual small example works, you will understand the broader pattern, and you'll be able to generalize and write out the full general formula for how these derivatives flow in an expression like this. So let's try that out. So pardon the low-budget production here, but what I've done here is I'm writing it out on a piece of paper. Really what we are interested in is we have A multiply B plus C, and that creates a D. And we have the derivative of the loss with respect to D, and we'd like to know what the derivative of the loss is with respect to A, B, and C. Now these here are little two-dimensional examples of a matrix multiplication. 2 by 2 times a 2 by 2 plus a 2, a vector of just two elements, C1 and C2, gives me a 2 by 2. Now notice here that I have a bias vector here called C, and the bias vector is C1 and C2. But as I described over here, that bias vector will become a row vector in the broadcasting and will replicate vertically. So that's what's happening here as well. C1, C2 is replicated vertically, and we see how we have two rows of C1, C2 as a result. So now when I say write it out, I just mean like this. Basically break up this matrix multiplication into the actual thing that's going on under the hood. So as a result of matrix multiplication and how it works, D11 is the result of a dot product between the first row of A and the first column of B. So A11, B11 plus A12, B21 plus C1, and so on, so forth, for all the other elements of D. And once you actually write it out, it becomes obvious this is just a bunch of multiplies and adds. And we know from micrograd how to differentiate multiplies and adds. And so this is not scary anymore. It's not just matrix multiplication. It's just tedious, unfortunately, but this is completely tractable. We have DL by D for all of these, and we want DL by all these little other variables. So how do we achieve that, and how do we actually get the gradients? Okay, so the low-budget production continues here. So let's, for example, derive the derivative of the loss with respect to A11. We see here that A11 occurs twice in our simple expression, right here, right here, and influences D11 and D12. So what is DL by D A11? Well, it's DL by D11 times the local derivative of D11, which in this case is just B11, because that's what's multiplying A11 here. So, and likewise here, the local derivative of D12 with respect to A11 is just B12. And so B12 will, in the chain rule, therefore, multiply DL by D12. And then because A11 is used both to produce D11 and D12, we need to add up the contributions of both of those sort of chains that are running in parallel. And that's why we get a plus, just adding up those two contributions. And that gives us DL by D A11. We can do the exact same analysis for the other one, for all the other elements of A. And when you simply write it out, it's just super simple taking the gradients on expressions like this. You find that this matrix DL by DA that we're after, right? If we just arrange all of them in the same shape as A takes, so A is just too much matrix. So DL by DA here will be also just the same shape tensor with the derivatives now. So DL by DA11, et cetera. And we see that actually we can express what we've written out here as a matrix multiply. And so it just so happens that DL by, that all of these formulas that we've derived here by taking gradients can actually be expressed as a matrix multiplication. And in particular, we see that it is the matrix multiplication of these two matrices. So it is the DL by D and then matrix multiplying B, but B transpose actually. So you see that B21 and B12 have changed place. Whereas before we had, of course, B11, B12, B21, B22. So you see that this other matrix B is transposed. And so basically what we have, long story short, just by doing very simple reasoning here, by breaking up the expression in the case of a very simple example, is that DL by DA, which is this, is simply equal to DL by DD matrix multiplied with B transpose. So that is what we have so far. Now we also want the derivative with respect to B and C. Now for B, I'm not actually doing the full derivation because honestly it's not deep. It's just annoying. It's exhausting. You can actually do this analysis yourself. You'll also find that if you take these expressions and you differentiate with respect to B instead of A, you will find that DL by DB is also a matrix multiplication. In this case, you have to take the matrix A and transpose it. And matrix multiply that with DL by DD. And that's what gives you DL by DB. And then here for the offsets, C1 and C2, if you again just differentiate with respect to C1, you will find an expression like this. And C2, an expression like this. And basically you'll find that DL by DC is simply, because they're just offsetting these expressions, you just have to take the DL by DD matrix of the derivatives of D. And you just have to sum across the columns. And that gives you the derivatives for C. So long story short, the backward pass of a matrix multiply is a matrix multiply. And instead of, just like we had D equals A times B plus C, in a scalar case, we sort of like arrived at something very, very similar, but now with a matrix multiplication instead of a scalar multiplication. So the derivative of D with respect to A is DL by DD matrix multiply B transpose. And here it's A transpose multiply DL by DD. But in both cases, it's a matrix multiplication with the derivative and the other term in the multiplication. And for C, it is A sum. Now I'll tell you a secret. I can never remember the formulas that we just derived for propagating through matrix multiplication, and I can back propagate through these expressions just fine. And the reason this works is because the dimensions have to work out. So let me give you an example. Say I want to create DH. Then what should DH be? Number one, I have to know that the shape of DH must be the same as the shape of H. And the shape of H is 32 by 64. And then the other piece of information I know is that DH must be some kind of matrix multiplication of D logits with W2. And D logits is 32 by 27, and W2 is 64 by 27. There is only a single way to make the shape work out in this case, and it is indeed the correct result. In particular here, H needs to be 32 by 64. The only way to achieve that is to take a D logits and matrix multiply it with... You see how I have to take W2, but I have to transpose it to make the dimensions work out. So W2 transpose. And it's the only way to matrix multiply those two pieces to make the shapes work out. And that turns out to be the correct formula. So if we come here, we want DH, which is DA. And we see that DA is DL by DD matrix multiply B transpose. So that's D logits multiply, and B is W2. So W2 transpose, which is exactly what we have here. So there's no need to remember these formulas. Similarly, now if I want DEW2, well, I know that it must be a matrix multiplication of D logits and H. And maybe there's a few transpose... Like, there's one transpose in there as well. And I don't know which way it is, so I have to come to W2. And I see that its shape is 64 by 27. And that has to come from some matrix multiplication of these two. And so to get a 64 by 27, I need to take H. I need to transpose it. And then I need to matrix multiply it. So that will become 64 by 32. And then I need to matrix multiply it with a 32 by 27. And that's going to give me a 64 by 27. So I need to matrix multiply this with D logits.shape, just like that. That's the only way to make the dimensions work out. And just use matrix multiplication. And if we come here, we see that that's exactly what's here. So A transpose, A for us is H, multiplied with D logits. So that's W2. And then DB2 is just the vertical sum. And actually, in the same way, there's only one way to make the shapes work out. I don't have to remember that it's a vertical sum along the 0th axis, because that's the only way that this makes sense. Because B2's shape is 27. So in order to get a D logits here, it's 32 by 27. So knowing that it's just sum over D logits in some direction, that direction must be 0, because I need to eliminate this dimension. So it's this. So this is kind of like the hacky way. Let me copy, paste, and delete that. And let me swing over here. And this is our backward pass for the linear layer, hopefully. So now let's uncomment these three. And we're checking that we got all the three derivatives correct. And run. And we see that H, W2, and B2 are all exactly correct. So we backpropagated through a linear layer. Now next up, we have derivative for the H already. And we need to backpropagate through 10H into H preact. So we want to derive DH preact. And here we have to backpropagate through a 10H. And we've already done this in micrograd. And we remember that 10H is a very simple backward formula. Now unfortunately, if I just put in D by DX of 10H of X into both from alpha, it lets us down. It tells us that it's a hyperbolic secant function squared of X. It's not exactly helpful. But luckily, Google Image Search does not let us down. And it gives us the simpler formula. And in particular, if you have that A is equal to 10H of Z, then DA by DZ, backpropagating through 10H, is just 1 minus A squared. And take note that 1 minus A squared, A here is the output of the 10H, not the input to the 10H, Z. So the DA by DZ is here formulated in terms of the output of that 10H. And here also in Google Image Search, we have the full derivation if you want to actually take the actual definition of 10H and work through the math to figure out 1 minus 10H squared of Z. So 1 minus A squared is the local derivative. In our case, that is 1 minus the output of 10H squared, which here is H. So it's H squared. And that is the local derivative. And then times the chain rule, DH. So that is going to be our candidate implementation. So if we come here and then uncomment this, let's hope for the best. And we have the right answer. Okay, next up, we have DH Preact, and we want to backpropagate into the gain, the BNRAW, and the BNBIAS. So here, this is the BashNorm parameters, BNGAIN and BIAS, inside the BashNorm that take the BNRAW, that is exact unit Gaussian, and they scale it and shift it. And these are the parameters of the BashNorm. Now, here we have a multiplication, but it's worth noting that this multiply is very, very different from this matrix multiply here. Matrix multiply are dot products between rows and columns of these matrices involved. This is an element-wise multiply, so things are quite a bit simpler. Now, we do have to be careful with some of the broadcasting happening in this line of code, though. So you see how BNGAIN and BNBIAS are 1 by 64, but HPreact and BNRAW are 32 by 64. So we have to be careful with that and make sure that all the shapes work out fine and that the broadcasting is correctly backpropagated. So in particular, let's start with DB and gain. So DB and gain should be, and here this is, again, element-wise multiply, and whenever we have A times B equals C, we saw that the local derivative here is just, if this is A, the local derivative is just the B, the other one. So the local derivative is just BNRAW and then times chain rule, so DHPreact. So this is the candidate gradient. Now, again, we have to be careful because BNGAIN is of size 1 by 64, but this here would be 32 by 64. And so the correct thing to do in this case, of course, is that BNGAIN, here is a rule vector of 64 numbers, it gets replicated vertically in this operation, and so therefore the correct thing to do is to sum because it's being replicated, and therefore all the gradients in each of the rows that are now flowing backwards need to sum up to that same tensor DB and gain. So we have to sum across all the 0, all the examples, basically, which is the direction in which this gets replicated. And now we have to be also careful because BNGAIN is of shape 1, BNGAIN is of shape 1 by 64. So in fact, I need to keep them as true. Otherwise, I would just get 64. Now, I don't actually really remember why the BNGAIN and the BNBIAS, I made them be 1 by 64, but the biases, B1 and B2, I just made them be one-dimensional vectors. They're not two-dimensional tensors. So I can't recall exactly why I left the gain and the bias as two-dimensional, but it doesn't really matter as long as you are consistent and you're keeping it the same. So in this case, we want to keep the dimension so that the tensor shapes work. Next up, we have BNRAW. So DBNRAW will be BNGAIN multiplying DHPREACT. That's our chain rule. Now, what about the dimensions of this? We have to be careful, right? DHPREACT is 32 by 64. BNGAIN is 1 by 64. So it will just get replicated to create this multiplication, which is the correct thing because in a forward pass, it also gets replicated in just the same way. So in fact, we don't need the brackets here. We're done, and the shapes are already correct. And finally, for the bias, very similar. This bias here is very, very similar to the bias we saw in the linear layer. And we see that the gradients from HPREACT will simply flow into the biases and add up because these are just offsets. And so basically, we want this to be DHPREACT, but it needs to sum along the right dimension. And in this case, similar to the gain, we need to sum across the 0th dimension, the examples, because of the way that the bias gets replicated vertically. And we also want to have keepDim as true. And so this will basically take this and sum it up and give us a 1 by 64. So this is the candidate implementation. It makes all the shapes work. Let me bring it up down here. And then let me uncomment these three lines to check that we are getting the correct result for all the three tensors. And indeed, we see that all of that got backpropagated correctly. So now we get to the batch norm layer. We see how here bnGain and bnBias are the parameters, so the backpropagation ends. And bnRaw now is the output of the standardization. So here what I'm doing, of course, is I'm breaking up the batch norm into manageable pieces so we can backpropagate through each line individually. But basically what's happening is... bn-mean i is the sum. So this is the bn-mean i. I apologize for the variable naming. bn-diff is x minus mu. bn-diff two is x minus mu squared, here inside the variance. bn-var is the variance, so sigma squared. This is bn-var. And it's basically the sum of squares. So this is the x minus mu squared, and then the sum. Now you'll notice one departure here. Here it is normalized as one over m, which is the number of examples. Here I am normalizing as one over n minus one instead of m, and this is deliberate, and I'll come back to that in a bit when we are at this line. It is something called the Bessel's correction, but this is how I want it in our case. bn-var inv then becomes basically bn-var plus epsilon. Epsilon is one negative five. And then it's one over square root is the same as raising to the power of negative 0.5, because 0.5 is square root, and then negative makes it one over square root. So bn-var inv is one over this denominator here. And then we can see that bn-raw, which is the x hat here, is equal to the bn-diff, the numerator, multiplied by the bn-var inv. And this line here that creates pre-H preact was the last piece we've already back-propagated through it. So now what we want to do is we are here, and we have bn-raw, and we have to first back-propagate into bn-diff and bn-var inv. So now we're here, and we have dbn-raw, and we need to back-propagate through this line. Now I've written out the shapes here, and indeed bn-var inv is a shape one by 64, so there is a broadcasting happening here that we have to be careful with. But it is just an element-wise simple multiplication. By now we should be pretty comfortable with that. To get dbn-diff, we know that this is just bn-var inv multiplied with dbn-raw. And conversely, to get dbn-var inv, we need to take bn-diff and multiply that by dbn-raw. So this is the candidate, but of course, we need to make sure that broadcasting is obeyed. So in particular, bn-var inv multiplying with dbn-raw will be okay and give us 32 by 64 as we expect. But dbn-var inv would be taking a 32 by 64, multiplying it by 32 by 64. So this is a 32 by 64. But of course, this bn-var inv is only one by 64. So the second line here needs a sum across the examples. And because there's this dimension here, we need to make sure that keep them is true. So this is the candidate. Let's erase this and let's swing down here and implement it. And then let's comment out dbn-var inv and dbn-diff. Now, we'll actually notice that dbn-diff, by the way, is going to be incorrect. So when I run this, bn-var inv is correct. bn-diff is not correct. And this is actually expected because we're not done with bn-diff. So in particular, when we slide here, we see here that bn-raw is a function of bn-diff, but actually bn-var inv is a function of bn-var, which is a function of bn-diff2, which is a function of bn-diff. So it comes here. So bn-diff, these variable names are crazy, I'm sorry. It branches out into two branches and we've only done one branch of it. We have to continue our back propagation and eventually come back to bn-diff. And then we'll be able to do a plus equals and get the actual correct gradient. For now, it is good to verify that cmp also works. It doesn't just lie to us and tell us that everything is always correct. It can in fact detect when your gradient is not correct. So that's good to see as well. Okay, so now we have the derivative here and we're trying to back propagate through this line. And because we're raising to a power of negative 0.5, I brought up the power rule. And we see that basically we have that the bn-var will now be, we bring down the exponent. So negative 0.5 times x, which is this. And now raised to the power of negative 0.5 minus one, which is negative 1.5. Now we would have to also apply a small chain rule here in our head because we need to take further derivative of bn-var with respect to this expression here inside the bracket. But because this is an element-wise operation and everything is fairly simple, that's just one. And so there's nothing to do there. So this is the local derivative and then times the global derivative to create the chain rule. This is just times the bn-var. So this is our candidate. Let me bring this down and uncomment the check. And we see that we have the correct result. Now, before we back propagate through the next line, I want to briefly talk about the note here where I'm using the Bessel's correction, dividing by n minus one, instead of dividing by n. When I normalize here, the sum of squares. Now you'll notice that this is a departure from the paper, which uses one over n instead, not one over n minus one. Their m is our n. And so it turns out that there are two ways of estimating variance of an array. One is the biased estimate, which is one over n. And the other one is the unbiased estimate, which is one over n minus one. Now, confusingly, in the paper, this is not very clearly described. And also it's a detail that kind of matters, I think. They are using the biased version of train time. But later when they are talking about the inference, they are mentioning that when they do the inference, they are using the unbiased estimate, which is the n minus one version, basically for inference, and to calibrate the running mean and the running variance, basically. And so they actually introduce a train test mismatch, where in training, they use the biased version, and in test time, they use the unbiased version. I find this extremely confusing. You can read more about the Bessel's correction and why dividing by n minus one gives you a better estimate of the variance in the case where you have population sizes or samples from a population that are very small. And that is indeed the case for us because we are dealing with mini-batches. And these mini-batches are a small sample of a larger population, which is the entire training set. And so it just turns out that if you just estimate it using one over n, that actually almost always underestimates the variance. And it is a biased estimator, and it is advised that you use the unbiased version and divide by n minus one. And you can go through this article here that I liked that actually describes the full reasoning, and I'll link it in the video description. Now, when you calculate the Torchdot variance, you'll notice that they take the unbiased flag, whether or not you wanna divide by n or n minus one. Confusingly, they do not mention what the default is for unbiased, but I believe unbiased by default is true. I'm not sure why the docs here don't cite that. Now, in the batch norm 1D, the documentation again is kind of wrong and confusing. It says that the standard deviation is calculated via the biased estimator, but this is actually not exactly right. And people have pointed out that it is not right in a number of issues since then, because actually the rabbit hole is deeper and they follow the paper exactly. And they use the biased version for training, but when they're estimating the running standard deviation, we are using the unbiased version. So again, there's the train test mismatch. So long story short, I'm not a fan of train test discrepancies. I basically kind of consider the fact that we use the biased version, the training time and the unbiased test time. I basically consider this to be a bug, and I don't think that there's a good reason for that. It's not really, they don't really go into the detail of the reasoning behind it in this paper. So that's why I basically prefer to use the Bessel's correction in my own work. Unfortunately, BatchNorm does not take a keyword argument that tells you whether or not you wanna use the unbiased version or the biased version in both train and test. And so therefore anyone using BatchNormalization, basically in my view has a bit of a bug in the code. And this turns out to be much less of a problem if your mini batch sizes are a bit larger, but still I just find it kind of unpalatable. So maybe someone can explain why this is okay. But for now, I prefer to use the unbiased version consistently both during training and at test time. And that's why I'm using one over N minus one here. Okay, so let's now actually back propagate through this line. So the first thing that I always like to do is I like to scrutinize the shapes first. So in particular here, looking at the shapes of what's involved, I see that bnvar shape is one by 64. So it's a row vector and bndiff2.shape is 32 by 64. So clearly here, we're doing a sum over the zeroth axis to squash the first dimension of the shapes here, using a sum. So that right away actually hints to me that there will be some kind of a replication or broadcasting in the backward pass. And maybe you're noticing the pattern here, but basically anytime you have a sum in the forward pass, that turns into a replication or broadcasting in the backward pass along the same dimension. And conversely, when we have a replication or a broadcasting in the forward pass, that indicates a variable reuse. And so in the backward pass, that turns into a sum over the exact same dimension. And so hopefully you're noticing that duality, that those two are kind of like the opposites of each other in the forward and the backward pass. Now, once we understand the shapes, the next thing I like to do always is I like to look at a toy example in my head to sort of just like understand roughly how the variable dependencies go in the mathematical formula. So here we have a two-dimensional array, a BNDIF2, which we are scaling by a constant, and then we are summing vertically over the columns. So if we have a two by two matrix A, and then we sum over the columns and scale, we would get a row vector B1, B2, and B1 depends on A in this way, where it's just sum that is scaled of A, and B2 in this way, where it's the second column summed and scaled. And so looking at this basically, what we want to do now is we have the derivatives on B1 and B2, and we want to back propagate them into A's. And so it's clear that just differentiating in your head, the local derivative here is one over N minus one times one for each one of these A's. And basically the derivative of B1 has to flow through the columns of A, scaled by one over N minus one. And that's roughly what's happening here. So intuitively the derivative flow tells us that DB and F2 will be the local derivative of this operation. And there are many ways to do this by the way, but I like to do something like this, torch dot once like of B and F2. So I'll create a large array two dimensional of ones, and then I will scale it. So 1.0 divide by N minus one. So this is a array of one over N minus one. And that's sort of like the local derivative. And now for the chain rule, I will simply just multiply it by DB and VAR. And notice here what's gonna happen. This is 32 by 64, and this is just one by 64. So I'm letting the broadcasting do the replication, because internally in PyTorch, basically DB and VAR, which is one by 64 row vector, will in this multiplication get copied vertically until the two are of the same shape, and then there will be an element wise multiply. And so that the broadcasting is basically doing the replication. And I will end up with the derivatives of DB and F2 here. So this is the candidate solution. Let's bring it down here. Let's uncomment this line where we check it, and let's hope for the best. And indeed we see that this is the correct formula. Next up, let's differentiate here into B and diff. So here we have that B and diff is element wise squared to create B and diff two. So this is a relatively simple derivative because it's a simple element wise operation. So it's kind of like the scalar case. And we have that DB and diff should be, if this is X squared, then derivative of this is two X. So it's simply two times B and diff. That's the local derivative. And then times chain rule. And the shape of these is the same. They are of the same shape. So times this. So that's the backward pass for this variable. Let me bring that down here. And now we have to be careful because we already calculated DB and diff, right? So this is just the end of the other branch coming back to B and diff. Because B and diff will already back-propagated to way over here from B and raw. So we now completed the second branch. And so that's why I have to do plus equals. And if you recall, we had an incorrect derivative for B and diff before. And I'm hoping that once we append this last missing piece, we have the exact correctness. So let's run. And B and diff now actually shows the exact correct derivative. So that's comforting. Okay, so let's now back-propagate through this line here. The first thing we do, of course, is we check the shapes. And I wrote them out here. And basically the shape of this is 32 by 64. H pre Bn is the same shape. But B and mean i is a row vector, one by 64. So this minus here will actually do broadcasting. And so we have to be careful with that. And as a hint to us, again, because of the duality, a broadcasting in the forward pass means a variable reuse. And therefore there will be a sum in the backward pass. So let's write out the backward pass here now. So let's back-propagate into the H pre Bn. Because these are the same shape, then the local derivative for each one of the elements here is just one for the corresponding element in here. So basically what this means is that the gradient just simply copies. It's just a variable assignment, it's equality. So I'm just going to clone this tensor just for safety to create an exact copy of D, B and diff. And then here to back-propagate into this one, what I'm inclined to do here is D, Bn, mean i will basically be, what is the local derivative? Well, it's negative torch dot ones like of the shape of Bn diff, right? And then times the derivative here, D, Bn diff. And this here is the back-propagation for the replicated Bn, mean i. So I still have to back-propagate through the replication in the broadcasting. And I do that by doing a sum. So I'm going to take this whole thing and I'm going to do a sum over the zero dimension, which was the replication. So if you scrutinize this, by the way, you'll notice that this is the same shape as that. And so what I'm doing here doesn't actually make that much sense because it's just a array of ones multiplying Bn diff. So in fact, I can just do this and that is equivalent. So this is the candidate backward pass. Let me copy it here. And then let me comment out this one and this one. Enter. And it's wrong. Damn. Actually, sorry, this is supposed to be wrong. And it's supposed to be wrong because we are back-propagating from a Bn diff into H pre-Bn, but we're not done because Bn mean i depends on H pre-Bn and there will be a second portion of that derivative coming from this second branch. So we're not done yet and we expect it to be incorrect. So there you go. So let's not back-propagate from Bn mean i into H pre-Bn. And so here again, we have to be careful because there's a broadcasting along or there's a sum along the zero dimension. So this will turn into broadcasting in the backward pass now. And I'm going to go a little bit faster on this line because it is very similar to the line that we had before and multiple lines in the past, in fact. So the H pre-Bn will be, the gradient will be scaled by one over N. And then basically this gradient here on Bn mean i is going to be scaled by one over N. And then it's going to flow across all the columns and deposit itself into the H pre-Bn. So what we want is this thing scaled by one over N. Let me put the constant up front here. So scale down the gradient and now we need to replicate it across all the rows here. So I like to do that by torch.onceLike of basically H pre-Bn. And I will let the broadcasting do the work of replication. So, like that. So this is the H pre-Bn and hopefully we can plus equals that. So this here is broadcasting and then this is the scaling. So this should be correct. Okay. So that completes the back propagation of the bastion layer. And we're now here. Let's back propagate through the linear layer one here. Now, because everything is getting a little vertically crazy I copy pasted the line here and let's just back propagate through this one line. So first of course, we inspect the shapes and we see that this is 32 by 64. MCAT is 32 by 30. W1 is 30 by 64 and B1 is just 64. So as I mentioned, back propagating through linear layers is fairly easy just by matching the shapes. So let's do that. We have that DMPCAT should be some matrix multiplication of DH pre-Bn with W1 and one transpose thrown in there. So to make MCAT be 32 by 30, I need to take DH pre-Bn, 32 by 64 and multiply it by W1 dot transpose. To get DW1, I need to end up with 30 by 64. So to get that, I need to take MCAT transpose and multiply that by DH pre-Bn. And finally to get DB1, I need to take DH pre-Bn This is an addition, and we saw that I need to sum the elements in dhPBn along some dimension. To make the dimensions work out, I need to sum along the 0th axis here to eliminate this dimension. We do not keep dims, so we want to get a single one-dimensional vector of 64. These are the claimed derivatives. Let me uncomment three lines and cross our fingers. Everything is great. We now continue, almost there. We have the derivative of mcat, and we want to backpropagate it into mb. I again copied this line over here. This is the forward pass, and this is the shapes. Remember that the shape here was 32x30, and the original shape of mb was 32x3x10. This layer in the forward pass, as you recall, did the concatenation of these three 10-dimensional character vectors. Now we just want to undo that. This is a relatively straightforward operation. What is a view? A view is just a representation of the array. It's just a logical form of how you interpret the array. Let's just reinterpret it to be what it was before. In other words, dmb is not 32x30, it is basically dmbcat, but if you view it as the original shape, so just m.shape, you can pass in tuples into view. This should just be okay. We just re-represent that view, and then we uncomment this line here, and hopefully the derivative of m is correct. In this case, we just have to re-represent the shape of those derivatives into the original view. Now we are at the final line, and the only thing that's left to backpropagate through is this indexing operation here, msc at xb. As I did before, I copy-pasted this line here, and let's look at the shapes of everything that's involved and remind ourselves how this worked. So m.shape was 32x3x10. So it's 32 examples, and then we have three characters, each one of them has a 10-dimensional embedding. This was achieved by taking the lookup table C, which has 27 possible characters, each of them 10-dimensional, and we looked up at the rows that were specified inside this tensor xb. So xb is 32x3, and it's basically giving us for each example the identity, or the index, of which character is part of that example. And so here I'm showing the first five rows of this tensor xb. And so we can see that, for example, here, it was the first example in this batch is that the first character, and the first character, and the fourth character comes into the neural net, and then we want to predict the next character in a sequence after the character is 114. So basically what's happening here is there are integers inside xb, and each one of these integers is specifying which row of C we want to pluck out, and then we arrange those rows that we've plucked out into a 32x3x10 tensor, and we just package them into this tensor. And now what's happening is that we have d.imp. So for every one of these basically plucked out rows, we have their gradients now, but they're arranged inside this 32x3x10 tensor. So all we have to do now is we just need to route this gradient backwards through this assignment. So we need to find which row of C did every one of these 10-dimensional embeddings come from, and then we need to deposit them into d.c. So we just need to undo the indexing, and of course if any of these rows of C was used multiple times, which almost certainly is the case, like the row 1 and 1 was used multiple times, then we have to remember that the gradients that arrive there have to add. So for each occurrence, we have to have an addition. So let's now write this out, and I don't actually know of a much better way to do this than a for loop, unfortunately, in Python. So maybe someone can come up with a vectorized efficient operation, but for now let's just use for loops. So let me create a torch.zeros like C to initialize just a 27x10 tensor of all zeros. And then honestly, for k in range xb.shape at 0, maybe someone has a better way to do this, but for j in range xb.shape at 1, this is going to iterate over all the elements of xb, all these integers. And then let's get the index at this position. So the index is basically xb at kj. So an example of that is 11 or 14 and so on. And now in a forward pass, we basically took the row of C at index, and we deposited it into emb at kj. That's what happened. That's where they are packaged. So now we need to go backwards, and we just need to route demb at the position kj. We now have these derivatives for each position, and it's 10-dimensional. And you just need to go into the correct row of C. So dc, rather, at ix is this, but plus equals because there could be multiple occurrences, like the same row could have been used many, many times. And so all of those derivatives will just go backwards through the indexing, and they will add. So this is my candidate solution. Let's copy it here. Let's uncomment this and cross our fingers. So that's it. We backpropagated through this entire beast. So there we go. It totally made sense. So now we come to exercise two. It basically turns out that in this first exercise, we were doing way too much work. We were backpropagating way too much, and it was all good practice and so on, but it's not what you would do in practice. And the reason for that is, for example, here I separated out this loss calculation over multiple lines, and I broke it up all to its smallest atomic pieces, and we backpropagated through all of those individually. But it turns out that if you just look at the mathematical expression for the loss, then actually you can do the differentiation on pen and paper, and a lot of terms cancel and simplify, and the mathematical expression you end up with can be significantly shorter and easier to implement than backpropagating through all the little pieces of everything you've done. So before we had this complicated forward pass going from logits to the loss, but in PyTorch, everything can just be glued together into a single call, f.crossentropy. You just pass in logits and the labels, and you get the exact same loss, as I verify here. So our previous loss and the fast loss, coming from the chunk of operations as a single mathematical expression, is the same, but it's much, much faster in a forward pass. It's also much, much faster in a backward pass. And the reason for that is, if you just look at the mathematical form of this and differentiate again, you will end up with a very small and short expression. So that's what we want to do here. We want to, in a single operation, or in a single go, or very quickly, go directly into dlogits. And we need to implement dlogits as a function of logits and yb's. But it will be significantly shorter than whatever we did here, where to get to dlogits we had to go all the way here. So all of this work can be skipped in a much, much simpler mathematical expression that you can implement here. So you can give it a shot yourself, basically look at what exactly is the mathematical expression of loss, and differentiate with respect to the logits. So let me show you a hint. You can of course try it fully yourself, but if not, I can give you some hint of how to get started mathematically. So basically what's happening here is, we have logits, then there's the softmax that takes the logits and gives you probabilities. Then we are using the identity of the correct next character to pluck out a row of probabilities, take the negative log of it to get our negative log probability, and then we average up all the log probabilities, or negative log probabilities to get our loss. So basically what we have is for a single individual example, rather, we have that loss is equal to negative log probability, where p here is kind of like thought of as a vector of all the probabilities. So at the yth position, where y is the label. And we have that p here, of course, is the softmax. So the ith component of p, of this probability vector, is just the softmax function. So raising all the logits basically to the power of e, and normalizing so everything sums to one. Now if you write out p of y here, you can just write out the softmax. And then basically what we're interested in is we're interested in the derivative of the loss with respect to the ith logit. And so basically it's a d by d li of this expression here, where we have l indexed with the specific label y, and on the bottom we have a sum over j of e to the lj, and the negative log of all that. So potentially give it a shot, pen and paper, and see if you can actually derive the expression for the loss by d li, and then we're going to implement it here. Okay, so I'm going to give away the result here. So this is some of the math I did to derive the gradients analytically. And so we see here that I'm just applying the rules of calculus from your first or second year of bachelor's degree, if you took it. And we see that the expressions actually simplify quite a bit. You have to separate out the analysis in the case where the ith index that you're interested in inside logits is either equal to the label, or it's not equal to the label. And then the expressions simplify and cancel in a slightly different way. And what we end up with is something very, very simple. We either end up with basically p at i, where p is again this vector of probabilities after a softmax, or p at i minus one, where we just simply subtract a one. But in any case, we just need to calculate the softmax p, and then in the correct dimension, we need to subtract a one. And that's the gradient, the form that it takes analytically. So let's implement this basically. And we have to keep in mind that this is only done for a single example. But here we are working with batches of examples. So we have to be careful of that. And then the loss for a batch is the average loss over all the examples. So in other words, is the example for all the individual examples, is the loss for each individual example summed up, and then divided by n. And we have to backpropagate through that as well and be careful with it. So d logits is going to be f dot softmax. PyTorch has a softmax function that you can call. And we want to apply the softmax on the logits. And we want to go in the dimension that is one. So basically, we want to do the softmax along the rows of these logits. Then at the correct positions, we need to subtract a one. So d logits at iterating over all the rows and indexing into the columns provided by the correct labels inside yb, we need to subtract one. And then finally, it's the average loss that is the loss. And in the average, there's a one over n of all the losses added up. And so we need to also backpropagate through that division. So the gradient has to be scaled down by n as well because of the mean. But this otherwise should be the result. So now if we verify this, we see that we don't get an exact match. But at the same time, the maximum difference from logits from PyTorch and rd logits here is on the order of 5e negative 9. So it's a tiny, tiny number. So because of floating point wonkiness, we don't get the exact bitwise result. But we basically get the correct answer approximately. Now I'd like to pause here briefly before we move on to the next exercise because I'd like us to get an intuitive sense of what d logits is because it has a beautiful and very simple explanation, honestly. So here I'm taking d logits and I'm visualizing it. And we can see that we have a batch of 32 examples of 27 characters. And what is d logits intuitively, right? d logits is the probabilities that the probabilities matrix in the forward pass. But then here, these black squares are the positions of the correct indices where we subtracted a 1. And so what is this doing, right? These are the derivatives on d logits. And so let's look at just the first row here. So that's what I'm doing here. I'm calculating the probabilities of these logits and then I'm taking just the first row. And this is the probability row. And then d logits of the first row and multiplying by n just for us so that we don't have the scaling by n in here and everything is more interpretable. We see that it's exactly equal to the probability, of course, but then the position of the correct index has a minus equals 1. So minus 1 on that position. And so notice that if you take d logits at 0 and you sum it, it actually sums to 0. And so you should think of these gradients here at each cell as like a force. We are going to be basically pulling down on the probabilities of the incorrect characters and we're going to be pulling up on the probability at the correct index. And that's what's basically happening in each row. And the amount of push and pull is exactly equalized because the sum is 0. So the amount to which we pull down on the probabilities and the amount that we push up on the probability of the correct character is equal. So the repulsion and the attraction are equal. And think of the neural net now as a massive pulley system or something like that. We're up here on top of the logits and we're pulling down the probabilities of incorrect and pulling up the probability of the correct. And in this complicated pulley system, because everything is mathematically just determined, just think of it as this tension translating to this complicating pulley mechanism. And then eventually we get a tug on the weights and the biases. And basically in each update, we just kind of like tug in the direction that we'd like for each of these elements. And the parameters are slowly given in to the tug. And that's what training a neural net kind of like looks like on a high level. And so I think the forces of push and pull in these gradients are actually very intuitive here. We're pushing and pulling on the correct answer and the incorrect answers. And the amount of force that we're applying is actually proportional to the probabilities that came out in the forward pass. And so for example, if our probabilities came out exactly correct, so they would have had zero everywhere except for one at the correct position, then the logits would be all a row of zeros for that example. There would be no push and pull. So the amount to which your prediction is incorrect is exactly the amount by which you're going to get a pull or a push in that dimension. So if you have, for example, a very confidently mispredicted element here, then what's going to happen is that element is going to be pulled down very heavily. And the correct answer is going to be pulled up to the same amount. And the other characters are not going to be influenced too much. So the amount to which you mispredict is then proportional to the strength of the pull. And that's happening independently in all the dimensions of this tensor. And it's sort of very intuitive and very easy to think through. And that's basically the magic of the cross entropy loss and what it's doing dynamically in the backward pass of the neural net. So now we get to exercise number three, which is a very fun exercise, depending on your definition of fun. And we are going to do for batch normalization exactly what we did for cross entropy loss in exercise number two. That is, we are going to consider it as a glued single mathematical expression and back propagate through it in a very efficient manner, because we are going to derive a much simpler formula for the backward pass of batch normalization. And we're going to do that using pen and paper. So previously, we've broken up batch normalization into all of the little intermediate pieces and all the atomic operations inside it. And then we back propagated through it one by one. Now we just have a single sort of forward pass of a batch norm. And it's all glued together. And we see that we get the exact same result as before. Now for the backward pass, we'd like to also implement a single formula basically for back propagating through this entire operation, that is the batch normalization. So in the forward pass previously, we took H pre-BN, the hidden states of the pre-batch normalization, and created H pre-act, which is the hidden states just before deactivation. In the batch normalization paper, H pre-BN is X and H pre-act is Y. So in the backward pass, what we'd like to do now is we have DH pre-act, and we'd like to produce DH pre-BN. And we'd like to do that in a very efficient manner. So that's the name of the game. Calculate DH pre-BN given DH pre-act. And for the purposes of this exercise, we're going to ignore gamma and beta and their derivatives, because they take on a very simple form in a very similar way to what we did up above. So let's calculate this, given that, right here. So to help you a little bit, like I did before, I started off the implementation here on pen and paper. And I took two sheets of paper to derive the mathematical formulas for the backward pass. And basically, to set up the problem, just write out the mu, sigma square, variance, Xi hat, and Yi, exactly as in the paper, except for the Bessel correction. And then in the backward pass, we have the derivative of the loss with respect to all the elements of Y. And remember that Y is a vector. There's multiple numbers here. So we have all of the derivatives with respect to all the Ys. And then there's a gamma and a beta. And this is kind of like the compute graph. The gamma and the beta, there's the X hat, and then the mu and the sigma squared, and the X. So we have dL by dYi, and we want dL by dXi for all the Is in these vectors. So this is the compute graph. And you have to be careful, because I'm trying to note here that these are vectors. There's many nodes here inside X, X hat and Y. But mu and sigma, sorry, sigma square, are just individual scalars, single numbers. So you have to be careful with that. You have to imagine there's multiple nodes here, or you're going to get your math wrong. So as an example, I would suggest that you go in the following order, 1, 2, 3, 4, in terms of the backpropagation. So backpropagate into X hat, then into sigma square, then into mu, and then into X. Just like in a topological sort in micrograd, we would go from right to left. You're doing the exact same thing, except you're doing it with symbols and on pieces paper. So for number one, I'm not giving away too much. If you want dl of dx i hat, then we just take dl by dy i and multiply it by gamma because of this expression here, where any individual y i is just gamma times x i hat plus beta. So it didn't help you too much there, but this gives you basically the derivatives for all the x hats. And so now try to go through this computational graph and derive what is dl by d sigma square, and then what is dl by d mu, and then what is dl by dx eventually. So give it a go and I'm going to be revealing the answer one piece at a time. Okay, so to get dl by d sigma square, we have to remember again, like I mentioned, that there are many x hats here. And remember that sigma square is just a single individual number here. So when we look at the expression for dl by d sigma square, we have to actually consider all the possible paths that we basically have that there's many x hats and they all feed off from, they all depend on sigma square. So sigma square has a large fan out. There's lots of arrows coming out from sigma square into all the x hats. And then there's a back propagating signal from each x hat into sigma square. And that's why we actually need to sum over all those i's from i equal to one to m of the dl by d x i hat, which is the global gradient times the x i hat by d sigma square, which is the local gradient of this operation here. And then mathematically, I'm just working it out here and I'm simplifying and you get a certain expression for dl by d sigma square. And we're going to be using this expression when we back propagate into mu and then eventually into x. So now let's continue our back propagation into mu. So what is dl by d mu? Now again, be careful that mu influences x hat and x hat is actually lots of values. So for example, if our mini batch size is 32, as it is in our example that we were working on, then this is 32 numbers and 32 arrows going back to mu. And then mu going to sigma square is just a single arrow because sigma square is a scalar. So in total, there are 33 arrows emanating from mu. And then all of them have gradients coming into mu and they all need to be summed up. And so that's why when we look at the expression for dl by d mu, I am summing up over all the gradients of dl by d x hat times d x hat by d mu. So that's this arrow and that's 32 arrows here. And then plus the one arrow from here, which is dl by d sigma square times d sigma square by d mu. So now we have to work out that expression and let me just reveal the rest of it. Simplifying here is not complicated, the first term, and you just get an expression here. For the second term though, there's something really interesting that happens. When we look at d sigma square by d mu and we simplify, at one point, if we assume that in a special case where mu is actually the average of x i's, as it is in this case, then if we plug that in, then actually the gradient vanishes and becomes exactly zero. And that makes the entire second term cancel. And so if you just have a mathematical expression like this and you look at d sigma square by d mu, you would get some mathematical formula for how mu impacts sigma square. But if it is the special case that mu is actually equal to the average, as it is in the case of batch normalization, that gradient will actually vanish and become zero. So the whole term cancels and we just get a fairly straightforward expression here for dl by d mu. Okay, and now we get to the craziest part, which is deriving dl by d x i, which is ultimately what we're after. Now, let's count. First of all, how many numbers are there inside x? As I mentioned, there are 32 numbers. There are 32 little x i's. And let's count the number of arrows emanating from each x i. There's an arrow going to mu, an arrow going to sigma square, and then there's an arrow going to x hat. But this arrow here, let's scrutinize that a little bit. Each x i hat is just a function of x i and all the other scalars. So x i hat only depends on x i and none of the other x's. And so therefore, there are actually in this single arrow, there are 32 arrows, but those 32 arrows are going exactly parallel. They don't interfere, they're just going parallel between x and x hat. You can look at it that way. And so how many arrows are emanating from each x i? There are three arrows, mu, sigma square, and the associated x hat. And so in backpropagation, we now need to apply the chain rule, and we need to add up those three contributions. So here's what that looks like if I just write that out. We're chaining through mu, sigma square, and through x hat, and those three terms are just here. Now, we already have three of these. We have dL by d x i hat, we have dL by d mu, which we derived here, and we have dL by d sigma square, which we derived here. But we need three other terms here. This one, this one, and this one. So I invite you to try to derive them. It's not that complicated. You're just looking at these expressions here and differentiating with respect to x i. So give it a shot, but here's the result, or at least what I got. I'm just differentiating with respect to x i for all of these expressions. And honestly, I don't think there's anything too tricky here. It's basic calculus. Now what gets a little bit more tricky is we are now going to plug everything together. So all of these terms multiplied with all of these terms and added up according to this formula. And that gets a little bit hairy. So what ends up happening is you get a large expression. And the thing to be very careful with here, of course, is we are working with a dL by d x i for a specific i here. But when we are plugging in some of these terms, like say this term here, dL by d sigma squared, you see how dL by d sigma squared, I end up with an expression, and I'm iterating over little i's here. But I can't use i as the variable when I plug in here, because this is a different i from this i. This i here is just a placeholder, like a local variable for a for loop in here. So here, when I plug that in, you notice that I renamed the i to a j, because I need to make sure that this j is not this i. This j is like a little local iterator over 32 terms. And so you have to be careful with that when you're plugging in the expressions from here to here. You may have to rename i's into j's, and you have to be very careful what is actually an i with respect to dL by d x i. So some of these are j's, some of these are i's. And then we simplified this expression. And I guess the big thing to notice here is a bunch of terms just going to come out to the front, and you can refactor them. There's a sigma squared plus epsilon raised to the power of negative 3 over 2. This sigma squared plus epsilon can be actually separated out into three terms. Each of them are sigma squared plus epsilon to the negative 1 over 2. So the three of them multiplied is equal to this. And then those three terms can go different places because of the multiplication. So one of them actually comes out to the front, and we'll end up here outside. One of them joins up with this term, and one of them joins up with this other term. And then when you simplify the expression, you'll notice that some of these terms that are coming out are just the x i hats. So you can simplify just by rewriting that. And what we end up with at the end is a fairly simple mathematical expression over here that I cannot simplify further. But basically, you'll notice that it only uses the stuff we have, and it derives the thing we need. So we have dL by dy for all the i's, and those are used plenty of times here. And also, in addition, what we're using is these x i hats and x j hats, and they just come from the forward pass. And otherwise, this is a simple expression, and it gives us dL by d x i for all the i's, and that's ultimately what we're interested in. So that's the end of BatchNorm backward pass analytically. Let's now implement this final result. Okay, so I implemented the expression into a single line of code here, and you can see that the max diff is tiny, so this is the correct implementation of this formula. Now, I'll just basically tell you that getting this formula here from this mathematical expression was not trivial, and there's a lot going on packed into this one formula. And this is a whole exercise by itself, because you have to consider the fact that this formula here is just for a single neuron and a batch of 32 examples. But what I'm doing here is we actually have 64 neurons, and so this expression has to in parallel evaluate the Batch Norm backward pass for all of those 64 neurons in parallel and independently. So this has to happen basically in every single column of the inputs here. And in addition to that, you see how there are a bunch of sums here, and we need to make sure that when I do those sums, that the sums that they broadcast correctly onto everything else that's here. And so getting this expression is just like highly non-trivial, and I invite you to basically look through it and step through it, and it's a whole exercise to make sure that this checks out. But once all the shapes agree, and once you convince yourself that it's correct, you can also verify that PyTorch gets the exact same answer as well. And so that gives you a lot of peace of mind that this mathematical formula is correctly implemented here and broadcasted correctly and replicated in parallel for all of the 64 neurons inside this Batch Norm layer. Okay, and finally, exercise number four asks you to put it all together. And here we have a redefinition of the entire problem. So you see that we re-initialize the neural net from scratch and everything. And then here, instead of calling loss.backward, we want to have the manual backpropagation here as we derived it up above. So go up, copy-paste all the chunks of code that we've already derived, put them here, and derive your own gradients, and then optimize this neural net, basically using your own gradients all the way to the calibration of the Batch Norm and the evaluation of the loss. And I was able to achieve quite a good loss, basically the same loss you would achieve before. And that shouldn't be surprising because all we've done is we've really gotten into loss.backward and we've pulled out all the code and inserted it here. But those gradients are identical, and everything is identical, and the results are identical. It's just that we have full visibility on exactly what goes on under the hood of loss.backward in this specific case. Okay, and this is all of our code. This is the full backward pass using basically the simplified backward pass for the cross-entropy loss and the Batch Normalization. So backpropagating through cross-entropy, the second layer, the 10-H nonlinearity, the Batch Normalization through the first layer, and through the embedding. And so you see that this is only maybe, what is this, 20 lines of code or something like that, and that's what gives us gradients. And now we can potentially erase loss.backward. So the way I have the code set up is you should be able to run this entire cell once you fill this in, and this will run for only 100 iterations and then break. And it breaks because it gives you an opportunity to check your gradients against PyTorch. So here, our gradients we see are not exactly equal. They are approximately equal, and the differences are tiny, 1 and negative 9 or so, and I don't exactly know where they're coming from, to be honest. So once we have some confidence that the gradients are basically correct, we can take out the gradient checking. We can disable this breaking statement, and then we can basically disable loss.backward. We don't need it anymore. Feels amazing to say that. And then here, when we are doing the update, we're not going to use p.grad. This is the old way of PyTorch. We don't have that anymore because we're not doing backward. We are going to use this update where we, you see that I'm iterating over, I've arranged the grads to be in the same order as the parameters, and I'm zipping them up, the gradients and the parameters, into p and grad. And then here, I'm going to step with just the grad that we derived manually. So the last piece is that none of this now requires gradients from PyTorch. And so one thing you can do here is you can do withTorch.noGrad and offset this whole code block. And really what you're saying is you're telling PyTorch that, hey, I'm not going to call backward on any of this. And this allows PyTorch to be a bit more efficient with all of it. And then we should be able to just run this. And it's running. And you see that lostItBackward is commented out and we're optimizing. So we're going to leave this run and hopefully we get a good result. Okay, so I allowed the neural net to finish optimization. Then here, I calibrate the bastion parameters because I did not keep track of the real-time keep track of the running mean and variance in their training loop. Then here, I ran the loss. And you see that we actually obtained a pretty good loss, very similar to what we've achieved before. And then here, I'm sampling from the model. And we see some of the name-like gibberish that we're sort of used to. So basically, the model worked and samples pretty decent results compared to what we're used to. So everything is the same. But of course, the big deal is that we did not use lots of backward. We did not use PyTorch.autograd. And we estimated our gradients ourselves by hand. And so hopefully, you're looking at this, the backward pass of this neural net, and you're thinking to yourself, actually, that's not too complicated. Each one of these layers is like three lines of code or something like that. And most of it is fairly straightforward, potentially with the notable exception of the batch normalization backward pass. Otherwise, it's pretty good. Okay, and that's everything I wanted to cover for this lecture. So hopefully, you found this interesting. And what I liked about it, honestly, is that it gave us a very nice diversity of layers to back propagate through. And I think it gives a pretty nice and comprehensive sense of how these backward passes are implemented and how they work. And you'd be able to derive them yourself. But of course, in practice, you probably don't want to, and you want to use the PyTorch autograd. But hopefully, you have some intuition about how gradients flow backwards through the neural net, starting at the loss, and how they flow through all the variables and all the intermediate results. And if you understood a good chunk of it, and if you have a sense of that, then you can count yourself as one of these buff dojis on the left, instead of the dojis on the right here. Now, in the next lecture, we're actually going to go to recurrent neural nets, LSTMs, and all the other variants of RNNs. And we're going to start to complexify the architecture and start to achieve better log likelihoods. And so I'm really looking forward to that, and I'll see you then. Hi everyone. Today we are continuing our implementation of MakeMore, our favorite character-level language model. Now, you'll notice that the background behind me is different. That's because I am in Kyoto and it is awesome. So I'm in a hotel room here. Now, over the last few lectures, we've built up to this architecture that is a multi-layer perceptron character-level language model. So we see that it receives three previous characters and tries to predict the fourth character in a sequence using a very simple multi-layer perceptron using one hidden layer of neurons with tenational neurons. So what I'd like to do now in this lecture is I'd like to complexify this architecture. In particular, we would like to take more characters in a sequence as an input, not just three. And in addition to that, we don't just want to feed them all into a single hidden layer because that squashes too much information too quickly. Instead, we would like to make a deeper model that progressively fuses this information to make its guess about the next character in a sequence. And so we'll see that as we make this architecture more complex, we're actually going to arrive at something that looks very much like a WaveNet. So WaveNet is this paper published by Dequined in 2016. And it is also a language model, basically, but it tries to predict audio sequences instead of character-level sequences or word-level sequences. But fundamentally, the modeling setup is identical. It is an autoregressive model, and it tries to predict the next character in a sequence. And the architecture actually takes this interesting hierarchical sort of approach to predicting the next character in a sequence with this tree-like structure. And this is the architecture, and we're going to implement it in the course of this video. So let's get started. So the story code for part five is very similar to where we ended up in part three. Recall that part four was the manual backpropagation exercise that is kind of an aside. So we are coming back to part three, copy-pasting chunks out of it, and that is our starter code for part five. I've changed very few things otherwise. So a lot of this should look familiar to you if you've gone through part three. So in particular, very briefly, we are doing imports. We are reading our data set of words, and we are processing the data set of words into individual examples, and none of this data generation code has changed. And basically, we have lots and lots of examples. In particular, we have 182,000 examples of three characters trying to predict the fourth one. And we've broken up every one of these words into little problems of given three characters, predict the fourth one. So this is our data set, and this is what we're trying to get the neural net to do. Now, in part three, we started to develop our code around these layer modules that are, for example, a class linear. And we're doing this because we want to think of these modules as building blocks and like a Lego building block bricks that we can sort of like stack up into neural networks. And we can feed data between these layers and stack them up into a sort of graphs. Now, we also developed these layers to have APIs and signatures very similar to those that are found in PyTorch. So we have torch.nn, and it's got all these layer building blocks that you would use in practice. And we were developing all of these to mimic the APIs of these. So for example, we have linear. So there will also be a torch.nn.linear, and its signature will be very similar to our signature, and the functionality will be also quite identical as far as I'm aware. So we have the linear layer with the batch norm 1D layer and the 10H layer that we developed previously. And linear just does a matrix multiply in the forward pass of this module. Batch norm, of course, is this crazy layer that we developed in the previous lecture. And what's crazy about it is, well, there's many things. Number one, it has these running mean and variances that are trained outside of backpropagation. They are trained using exponential moving average inside this layer, what we call the forward pass. In addition to that, there's this training flag because the behavior of batch norm is different during train time and evaluation time. And so suddenly, we have to be very careful that batch norm is in its correct state, that it's in the evaluation state or training state. So that's something to now keep track of, something that sometimes introduces bugs because you forget to put it into the right mode. And finally, we saw that batch norm couples the statistics or the activations across the examples in the batch. So normally, we thought of the batch as just an efficiency thing. But now, we are coupling the computation across batch elements, and it's done for the purposes of controlling the activation statistics as we saw in the previous video. So it's a very weird layer, at least to a lot of bugs, partly, for example, because you have to modulate the training and eval phase and so on. In addition, for example, you have to wait for the mean and the variance to settle and to actually reach a steady state. And so you have to make sure that you basically, there's state in this layer, and state is harmful, usually. Now, I brought out the generator object. Previously, we had a generator equals G and so on inside these layers. I've discarded that in favor of just initializing the torch RNG outside here just once globally, just for simplicity. And then here, we are starting to build out some of the neural network elements. This should look very familiar. We have our embedding table C, and then we have a list of layers. And it's a linear, feeds to batch norm, feeds to batch norm, feeds to 10H, and then a linear output layer. And its weights are scaled down, so we are not confidently wrong at initialization. We see that this is about 12,000 parameters. We're telling PyTorch that the parameters require gradients. The optimization is, as far as I'm aware, identical and should look very, very familiar. Nothing changed here. Loss function looks very crazy. We should probably fix this. And that's because 32 batch elements are too few. And so you can get very lucky or unlucky in any one of these batches, and it creates a very thick loss function. So we're going to fix that soon. Now, once we want to evaluate the trained neural network, we need to remember, because of the batch norm layers, to set all the layers to be training equals false. This only matters for the batch norm layer so far. And then we evaluate. We see that currently, we have a validation loss of 2.10, which is fairly good, but there's still a ways to go. But even at 2.10, we see that when we sample from the model, we actually get relatively name-like results that do not exist in a training set. So for example, Yvonne, Kilo, Pras, Alaya, et cetera. So certainly not unreasonable, I would say, but not amazing. And we can still push this validation loss even lower and get much better samples that are even more name-like. So let's improve this model now. OK, first, let's fix this graph, because it is daggers in my eyes, and I just can't take it anymore. So loss i, if you recall, is a Python list of floats. So for example, the first 10 elements look like this. Now, what we'd like to do, basically, is we need to average up some of these values to get a more representative value along the way. So one way to do this is the following. In PyTorch, if I create, for example, a tensor of the first 10 numbers, then this is currently a one-dimensional array. But recall that I can view this array as two-dimensional. So for example, I can view it as a 2x5 array, and this is a 2D tensor now, 2x5. And you see what PyTorch has done is that the first row of this tensor is the first five elements, and the second row is the second five elements. I can also view it as a 5x2, as an example. And then recall that I can also use negative 1 in place of one of these numbers. And PyTorch will calculate what that number must be in order to make the number of elements work out. So this can be this, or like that. Both will work. Of course, this would not work. OK, so this allows it to spread out some of the consecutive values into rows. So that's very helpful, because what we can do now is, first of all, we're going to create a Torch.Tensor out of the list of floats. And then we're going to view it as whatever it is, but we're going to stretch it out into rows of 1,000 consecutive elements. So the shape of this now becomes 200 by 1,000. And each row is 1,000 consecutive elements in this list. So that's very helpful, because now we can do a mean along the rows. And the shape of this will just be 200. And so we've taken basically the mean on every row. So plt.plot of that should be something nicer. Much better. So we see that we've basically made a lot of progress. And then here, this is the learning rate decay. So here we see that the learning rate decay subtracted a ton of energy out of the system and allowed us to settle into the local minimum in this optimization. So this is a much nicer plot. Let me come up and delete the monster. And we're going to be using this going forward. Now, next up, what I'm bothered by is that you see our forward pass is a little bit gnarly and takes way too many lines of code. So in particular, we see that we've organized some of the layers inside the layers list, but not all of them for no reason. So in particular, we see that we still have the embedding table special case outside of the layers. And in addition to that, the viewing operation here is also outside of our layers. So let's create layers for these, and then we can add those layers to just our list. So in particular, the two things that we need is here, we have this embedding table, and we are indexing at the integers inside the batch xb, inside the tensor xb. So that's an embedding table lookup just done with indexing. And then here we see that we have this view operation, which if you recall from the previous video, simply rearranges the character embeddings and stretches them out into a row. And effectively what that does is the concatenation operation, basically, except it's free because viewing is very cheap in PyTorch. And no memory is being copied. We're just re-representing how we view that tensor. So let's create modules for both of these operations, the embedding operation and the flattening operation. So I actually wrote the code just to save some time. So we have a module embedding and a module flatten, and both of them simply do the indexing operation in the forward pass and the flattening operation here. And this c now will just become a self.weight inside an embedding module. And I'm calling these layers specifically embedding and flatten because it turns out that both of them actually exist in PyTorch. So in PyTorch we have n and dot embedding, and it also takes the number of embeddings and the dimensionality of the embedding, just like we have here. But in addition, PyTorch takes in a lot of other keyword arguments that we are not using for our purposes yet. And for flatten, that also exists in PyTorch, and it also takes additional keyword arguments that we are not using. So we have a very simple flatten. But both of them exist in PyTorch, they're just a bit more simpler. And now that we have these, we can simply take out some of these special cased things. So instead of c, we're just going to have an embedding and a vocab size and n embed. And then after the embedding, we are going to flatten. So let's construct those modules. And now I can take out this c. And here I don't have to special case it anymore, because now c is the embedding's weight, and it's inside layers. So this should just work. And then here, our forward pass simplifies substantially, because we don't need to do these now outside of these layer, outside and explicitly. They're now inside layers, so we can delete those. But now to kick things off, we want this little x, which in the beginning is just xb, the tensor of integers specifying the identities of these characters at the input. And so these characters can now directly feed into the first layer, and this should just work. So let me come here and insert a break, because I just want to make sure that the first iteration of this runs and that there's no mistake. So that ran properly. And basically, we've substantially simplified the forward pass here. Okay, I'm sorry, I changed my microphone. So hopefully, the audio is a little bit better. Now, one more thing that I would like to do in order to PyTorchify our code in further, is that right now we are maintaining all of our modules in a naked list of layers. And we can also simplify this, because we can introduce the concept of PyTorch containers. So in torch.nn, which we are basically rebuilding from scratch here, there's a concept of containers. And these containers are basically a way of organizing layers into lists or dicts, and so on. So in particular, there's a sequential, which maintains a list of layers, and is a module class in PyTorch. And it basically just passes a given input through all the layers sequentially, exactly as we are doing here. So let's write our own sequential. I've written a code here. And basically, the code for sequential is quite straightforward. We pass in a list of layers, which we keep here. And then given any input in a forward pass, we just call all the layers sequentially and return the result. And in terms of the parameters, it's just all the parameters of the child modules. So we can run this. And we can again simplify this substantially, because we don't maintain this naked list of layers. We now have a notion of a model, which is a module. And in particular, is a sequential of all these layers. And now, parameters are simply just a model dot parameters. And so that list comprehension now lives here. And then here we are doing all the things we used to do. Now here, the code again simplifies substantially, because we don't have to do this forwarding here. Instead, we just call the model on the input data. And the input data here are the integers inside xb. So we can simply do logits, which are the outputs of our model, are simply the model called on xb. And then the cross entropy here takes the logits and the targets. So this simplifies substantially. And then this looks good. So let's just make sure this runs. That looks good. Now here, we actually have some work to do still here, but I'm going to come back later. For now, there's no more layers. There's a model dot layers, but it's not easy to access attributes of these classes directly. So we'll come back and fix this later. And then here, of course, this simplifies substantially as well, because logits are the model called on x. And then these logits come here. So we can evaluate the training validation loss, which currently is terrible, because we just initialized the neural net. And then we can also sample from the model. And this simplifies dramatically as well, because we just want to call the model onto the context and outcome logits. And then these logits go into Softmax and get the probabilities, et cetera. So we can sample from this model. What did I screw up? Okay, so I fixed the issue and we now get the result that we expect, which is gibberish, because the model is not trained, because we reinitialize it from scratch. The problem was that when I fixed this cell to be model dot layers instead of just layers, I did not actually run the cell. And so our neural net was in a training mode. And what caused the issue here is the batch norm layer, as batch norm layer often likes to do, because batch norm was in the training mode. And here we are passing in an input, which is a batch of just a single example made up of the context. And so if you are trying to pass in a single example into a batch norm that is in the training mode, you're going to end up estimating the variance using the input. And the variance of a single number is not a number, because it is a measure of a spread. So for example, the variance of just a single number five, you can see is not a number. And so that's what happened. And the batch norm basically caused an issue. And then that polluted all of the further processing. So all that we had to do was make sure that this runs. And we basically made the issue of, again, we didn't actually see the issue with the loss. We could have evaluated the loss, but we got the wrong result because batch norm was in the training mode. And so we still get a result, it's just the wrong result, because it's using the sample statistics of the batch. Whereas we want to use the running mean and running variance inside the batch norm. And so again, an example of introducing a bug inline, because we did not properly maintain the state of what is training or not. Okay, so I re-run everything. And here's where we are. As a reminder, we have the training loss of 2.05 and validation 2.10. Now, because these losses are very similar to each other, we have a sense that we are not overfitting too much on this task. And we can make additional progress in our performance by scaling up the size of the neural network and making everything bigger and deeper. Now, currently, we are using this architecture here, where we are taking in some number of characters, going into a single hidden layer, and then going to the prediction of the next character. The problem here is, we don't have a naive way of making this bigger in a productive way. We could, of course, use our layers, sort of building blocks and materials, to introduce additional layers here and make the network deeper. But it is still the case that we are crushing all of the characters into a single layer all the way at the beginning. And even if we make this a bigger layer and add neurons, it's still kind of like silly to squash all that information so fast in a single step. So what we'd like to do instead is we'd like our network to look a lot more like this in the WaveNet case. So you see in the WaveNet, when we are trying to make the prediction for the next character in the sequence, it is a function of the previous characters that feed in. But not, all of these different characters are not just crushed to a single layer, and then you have a sandwich. They are crushed slowly. So in particular, we take two characters and we fuse them into sort of like a bigram representation. And we do that for all these characters consecutively. And then we take the bigrams and we fuse those into four character level chunks. And then we fuse that again. And so we do that in this tree-like hierarchical manner. So we fuse the information from the previous context slowly into the network as it gets deeper. And so this is the kind of architecture that we want to implement. Now, in the WaveNet's case, this is a very simple visualization of a stack of dilated causal convolution layers. And this makes it sound very scary, but actually the idea is very simple. And the fact that it's a dilated causal convolution layer is really just an implementation detail to make everything fast. We're going to see that later. But for now, let's just keep the basic idea of it, which is this progressive fusion. So we want to make the network deeper. And at each level, we want to fuse only two consecutive elements, two characters, then two bigrams, then two fours, and then two bigrams. And two fourgrams and so on. So let's implement this. Okay, so first up, let me scroll to where we built a dataset. And let's change the block size from three to eight. So we're going to be taking eight characters of context to predict the ninth character. So the dataset now looks like this. We have a lot more context feeding in to predict any next character in a sequence. And these eight characters are going to be processed in this tree-like structure. Now, if we scroll here, everything here should just be able to work. So we should be able to redefine the network. You can do this along the way in any of the management structures that I've put together and You see that the number of parameters has increased by 10,000, and that's because the block size has grown. So this first linear layer is much, much bigger. Our linear layer now takes 8 characters into this middle layer. So there's a lot more parameters there. But this should just run. Let me just break right after the very first iteration. So you see that this runs just fine. It's just that this network doesn't make too much sense. We're crushing way too much information way too fast. So let's now come in and see how we could try to implement the hierarchical scheme. Now before we dive into the detail of the re-implementation here, I was just curious to actually run it and see where we are in terms of the baseline performance of just lazily scaling up the context length. So I let it run. We get a nice loss curve. And then evaluating the loss, we actually see quite a bit of improvement just from increasing the context length. So I started a little bit of a performance log here. And previously where we were is we were getting a performance of 2.10 on the validation loss. And now simply scaling up the context length from 3 to 8 gives us a performance of 2.02. So quite a bit of an improvement here. And also when you sample from the model, you see that the names are definitely improving qualitatively as well. So we could of course spend a lot of time here tuning things and making it even bigger and scaling up the network further, even with the simple set up here. But let's continue and let's implement the hierarchical model and treat this as just a rough baseline performance. But there's a lot of optimization left on the table in terms of some of the hyperparameters that you're hopefully getting a sense of now. Okay, so let's scroll up now and come back up. And what I've done here is I've created a bit of a scratch space for us to just look at the forward pass of the neural net and inspect the shape of the tensors along the way as the neural net forwards. So here I'm just temporarily for debugging, creating a batch of just say four examples, so four random integers. Then I'm plucking out those rows from our training set. And then I'm passing into the model the input xb. Now the shape of xb here, because we have only four examples, is 4 by 8. And this 8 is now the current block size. So inspecting xb, we just see that we have four examples. Each one of them is a row of xb. And we have eight characters here. And this integer tensor just contains the identities of those characters. So the first layer of our neural net is the embedding layer. So passing xb, this integer tensor, through the embedding layer creates an output that is 4 by 8 by 10. So our embedding table has for each character a 10-dimensional vector that we are trying to learn. And so what the embedding layer does here is it plucks out the embedding vector for each one of these integers and organizes it all in a 4 by 8 by 10 tensor now. So all of these integers are translated into 10-dimensional vectors inside this 3-dimensional tensor now. Now passing that through the flattened layer, as you recall, what this does is it views this tensor as just a 4 by 8 tensor. And what that effectively does is that all these 10-dimensional embeddings for all these eight characters just end up being stretched out into a long row. And that looks kind of like a concatenation operation, basically. So by viewing the tensor differently, we now have a 4 by 8. And inside this 80, it's all the 10-dimensional vectors just concatenated next to each other. And the linear layer, of course, takes 80 and creates 200 channels just via matrix multiplication. So so far, so good. Now I'd like to show you something surprising. Let's look at the insides of the linear layer and remind ourselves how it works. The linear layer here in the forward pass takes the input x, multiplies it with a weight, and then optionally adds a bias. And the weight here is 2-dimensional, as defined here, and the bias is 1-dimensional here. So effectively, in terms of the shapes involved, what's happening inside this linear layer looks like this right now. And I'm using random numbers here, but I'm just illustrating the shapes and what happens. Basically a 4 by 80 input comes into the linear layer, gets multiplied by this 80 by 200 weight matrix inside, and there's a plus 200 bias. And the shape of the whole thing that comes out of the linear layer is 4 by 200, as we see here. Now notice here, by the way, that this here will create a 4 by 200 tensor, and then plus 200, there's a broadcasting happening here. But 4 by 200 broadcasts with 200, so everything works here. So now the surprising thing that I'd like to show you that you may not expect is that this input here that is being multiplied doesn't actually have to be 2-dimensional. This matrix multiply operator in PyTorch is quite powerful, and in fact you can actually pass in higher dimensional arrays or tensors and everything works fine. So for example, this could be 4 by 5 by 80, and the result in that case will become 4 by 5 by 200. You can add as many dimensions as you like on the left here. And so effectively what's happening is that the matrix multiplication only works on the last dimension, and the dimensions before it in the input tensor are left unchanged. So basically these dimensions on the left are all treated as just a batch dimension. So we can have multiple batch dimensions, and then in parallel over all those dimensions we are doing the matrix multiplication on the last dimension. So this is quite convenient because we can use that in our network now. Just remember that we have these 8 characters coming in, and we don't want to now flatten all of it out into a large 8-dimensional vector, because we don't want to matrix multiply 80 into a weight matrix multiply immediately. Instead we want to group these like this. So every consecutive 2 elements, 1, 2, 3, 4, 5, 6, 7, 8, all of these should be now basically flattened out and multiplied by a weight matrix. But all of these 4 groups here we'd like to process in parallel. So it's kind of like a batch dimension that we can introduce. And then we can in parallel basically process all of these bigram groups in the 4 batch dimensions of an individual example, and also over the actual batch dimension of the 4 examples in our example here. So let's see how that works. Effectively what we want is right now we take a 4 by 80 and multiply it by 80 by 200 in the linear layer. This is what happens. But instead what we want is we don't want 80 characters or 80 numbers to come in. We only want 2 characters to come in on the very first layer, and those 2 characters should be fused. So in other words we just want 20 to come in. 20 numbers would come in. And here we don't want a 4 by 80 to feed into the linear layer. We actually want these groups of 2 to feed in. So instead of 4 by 80 we want this to be a 4 by 4 by 20. So these are the 4 groups of 2, and each one of them is a 10-dimensional vector. So what we want is now is we need to change the flattened layer so it doesn't output a 4 by 80, but it outputs a 4 by 4 by 20 where basically every 2 consecutive characters are packed in on the very last dimension. And then these 4 is the first batch dimension, and this 4 is the second batch dimension, referring to the 4 groups inside every one of these examples. And then this will just multiply like this. So this is what we want to get to. So we're going to have to change the linear layer in terms of how many inputs it expects. It shouldn't expect 80, it should just expect 20 numbers. And we have to change our flattened layer so it doesn't just fully flatten out this entire example. It needs to create a 4 by 4 by 20 instead of a 4 by 80. So let's see how this could be implemented. Basically right now we have an input that is a 4 by 8 by 10 that feeds into the flattened layer, and currently the flattened layer just stretches it out. So if you remember the implementation of flatten, it takes our x and it just views it as whatever the batch dimension is, and then negative 1. So effectively what it does right now is it does E.view of 4, negative 1, and the shape of this of course is 4 by 80. So that's what currently happens, and we instead want this to be a 4 by 4 by 20, where these consecutive 10-dimensional vectors get concatenated. So you know how in Python you can take a list of range of 10, so we have numbers from 0 to 9, and we can index like this to get all the even parts, and we can also index like starting at 1 and going in steps of 2 to get all the odd parts. So one way to implement this would be as follows. We can take E, and we can index into it for all the batch elements, and then just even elements in this dimension, so at indexes 0, 2, 4, and 8, and then all the parts here from this last dimension, and this gives us the even characters, and then here this gives us all the odd characters. And basically what we want to do is we want to make sure that these get concatenated in PyTorch, and then we want to concatenate these two tensors along the second dimension. So this and the shape of it would be 4x4x20. This is definitely the result we want. We are explicitly grabbing the even parts and the odd parts, and we're arranging those 4x4x10 right next to each other and concatenate. So this works, but it turns out that what also works is you can simply use view again and just request the right shape, and it just so happens that in this case, those vectors will again end up being arranged exactly the way we want. So in particular, if we take E and we just view it as a 4x4x20, which is what we want, we can check that this is exactly equal to, let me call this, this is the explicit concatenation I suppose, so explicit.shape is 4x4x20. If you just view it as 4x4x20, you can check that when you compare to explicit, you get a big, this is element-wise operation, so making sure that all of them are true, the values are true. So basically, long story short, we don't need to make an explicit call to concatenate, etc. We can simply take this input tensor to flatten, and we can just view it in whatever way we want. And in particular, we don't want to stretch things out with negative 1. We want to actually create a three-dimensional array, and depending on how many vectors that are consecutive we want to fuse, like for example 2, then we can just simply ask for this dimension to be 20, and use a negative 1 here, and PyTorch will figure out how many groups it needs to pack into this additional batch dimension. So let's now go into flatten and implement this. Okay, so I scrolled up here to flatten, and what we'd like to do is we'd like to change it now. So let me create a constructor and take the number of elements that are consecutive that we would like to concatenate now in the last dimension of the output. So here we're just going to remember, cell.n equals n. And then I want to be careful here because PyTorch actually has a torch.flatten, and its keyword arguments are different, and they kind of function differently. So our flatten is going to start to depart from PyTorch.flatten. So let me call it flatten consecutive or something like that, just to make sure that our APIs are about equal. So this basically flattens only some n consecutive elements and puts them into the last dimension. Now here, the shape of x is b by t by c, so let me pop those out into variables, and recall that in our example down below, b was 4, t was 8, and c was 10. Now instead of doing x.view of b by negative 1, this is what we had before, we want this to be b by negative 1 by, and basically here, we want c times n. That's how many consecutive elements we want. And here instead of negative 1, I don't super love the use of negative 1 because I like to be very explicit so that you get error messages when things don't go according to your expectation. So what do we expect here? We expect this to become t, divide, n, using integer division here. So that's what I expect to happen. And then one more thing I want to do here is, remember previously, all the way in the beginning, n was 3, and basically we're concatenating all the 3 characters that existed there. So we basically concatenated everything. And so sometimes that can create a spurious dimension of 1 here. So if it is the case that x.shape at 1 is 1, then it's kind of like a spurious dimension. So we don't want to return a 3-dimensional tensor with a 1 here, we just want to return a 2-dimensional tensor exactly as we did before. So in this case, basically we will just say x equals x.squeeze, that is a PyTorch function. And squeeze takes a dimension that either squeezes out all the dimensions of a tensor that are 1, or you can specify the exact dimension that you want to be squeezed. And again, I like to be as explicit as possible always, so I expect to squeeze out the first dimension only of this tensor, this 3-dimensional tensor. And if this dimension here is 1, then I just want to return b by c times n. And so self.out will be x, and then we return self.out. So that's the candidate implementation. And of course this should be self.in instead of just n. So let's run, and let's come here now and take it for a spin. So flatten consecutive. And in the beginning, let's just use 8. So this should recover the previous behavior. So flatten consecutive of 8, which is the current block size, we can do this. That should recover the previous behavior. So we should be able to run the model. And here we can inspect, I have a little code snippet here, where I iterate over all the layers, I print the name of this class, and the shape. And so we see the shapes as we expect them after every single layer in its output. So now let's try to restructure it using our flatten consecutive, and do it hierarchically. So in particular, we want to flatten consecutive not block size, but just 2, and then we want to process this with linear. Now the number of inputs to this linear will not be n embed times block size. It will now only be n embed times 2, 20. This goes through the first layer. And now we can, in principle, just copy paste this. Now the next linear layer should expect n hidden times 2. And the last piece of it should expect n hidden times 2 again. So this is sort of like the naive version of it. So running this, we now have a much, much bigger model. And we should be able to basically just forward the model. And now we can inspect the numbers in between. So 4x8x20 was flattened consecutively into 4x4x20. This was projected into 4x4x200. And then Bashorm just worked out of the box. And we have to verify that Bashorm does the correct thing, even though it takes a three-dimensional embed instead of two-dimensional embed. Then we have 10h, which is element-wise. Then we crushed it again. So we flattened consecutively and ended up with a 4x2x400 now. Then linear brought it back down to 200, Bashorm 10h. And lastly, we get a 4x400. And we see that the flattened consecutive for the last flattened here, it squeezed out that dimension of 1. So we only ended up with 4x400. And then linear, Bashorm 10h, and the last linear layer to get our logits. And so the logits end up in the same shape as they were before. But now we actually have a nice three-layer neural net. And it basically corresponds to this network now, except only this piece here, because we only have three layers. Whereas here in this example, there's four layers with a total receptive field size of 16 characters instead of just eight characters. So the block size here is 16. So this piece of it is basically implemented here. Now we just have to figure out some good channel numbers to use here. Now in particular, I changed the number of hidden units to be 68 in this architecture, because when I use 68, the number of parameters comes out to be 22,000. So that's exactly the same that we had before. And we have the same amount of capacity in this neural net in terms of the number of parameters. But the question is whether we are utilizing those parameters in a more efficient architecture. So what I did then is I got rid of a lot of the debugging cells here, and I rerun the optimization. And scrolling down to the result, we see that we get the identical performance roughly. So our validation loss now is 2.029, and previously it was 2.027. So controlling for the number of parameters, changing from the flat to hierarchical, is not giving us anything yet. That said, there are two things to point out. Number one, we didn't really torture the architecture here very much. This is just my first guess. And there's a bunch of hyperparameter search that we could do in terms of how we allocate our budget of parameters to what layers. Number two, we still may have a bug inside the BatchNorm1D layer. So let's take a look at that, because it runs, but does it do the right thing? So I pulled up the layer inspector that we have here and printed out the shapes along the way. And currently it looks like the BatchNorm is receiving an input that is 32 by 4 by 68. And here on the right, I have the current implementation of BatchNorm that we have right now. Now, this BatchNorm assumed, in the way we wrote it, and at the time, that x is two-dimensional. So it was n by d, where n was the batch size. So that's why we only reduced the mean and the variance over the zeroth dimension. But now x will basically become three-dimensional. So what's happening inside the BatchNorm layer right now, and how come it's working at all and not giving any errors? The reason for that is basically because everything broadcasts properly, but the BatchNorm is not doing what we want it to do. So in particular, let's basically think through what's happening inside the BatchNorm, looking at what's happening here. I have the code here. So we're receiving an input of 32 by 4 by 68. And then we are doing here, x dot mean. Here I have e instead of x. But we're doing the mean over zero. And that's actually giving us 1 by 4 by 68. So we're doing the mean over zero. doing the mean only over the very first dimension and it's given us a mean and a variance that still maintain this dimension here. So these means are only taken over 32 numbers in the first dimension and then when we perform this everything broadcasts correctly still. But basically what ends up happening is when we also look at the running mean, the shape of it, so I'm looking at the model.layerZ3 which is the first batch norm layer and then looking at whatever the running mean became and its shape. The shape of this running mean now is 1 by 4 by 68, right? Instead of it being you know just size of dimension because we have 68 channels we expect to have 68 means and variances that we're maintaining but actually we have an array of 4 by 68 and so basically what this is telling us is this batch norm is only this batch norm is currently working in parallel over 4 times 68 instead of just 68 channels. So basically we are maintaining statistics for every one of these four positions individually and independently and instead what we want to do is we want to treat this 4 as a batch dimension just like the 0th dimension. So as far as the batch norm is concerned it doesn't want to average, we don't want to average over 32 numbers, we want to now average over 32 times 4 numbers for every single one of these 68 channels. And so let me now remove this. It turns out that when you look at the documentation of torch.mean, so let's go to torch.mean, in one of its signatures when we specify the dimension we see that the dimension here is not just it can be int or it can also be a tuple of ints so we can reduce over multiple integers at the same time over multiple dimensions at the same time so instead of just reducing over 0 we can pass in a tuple 0 1 and here 0 1 as well and then what's going to happen is the output of course is going to be the same but now what's going to happen is because we reduce over 0 and 1 if we look at in mean.shape we see that now we've reduced we took the mean over both the 0th and the 1st dimension so we're just getting 68 numbers and a bunch of spurious dimensions here so now this becomes 1 by 1 by 68 and the running mean and the running variance analogously will become 1 by 1 by 68 so even though there are the spurious dimensions the correct thing will happen in that we are only maintaining means and variances for 68 channels and we're not calculating the mean and variance across 32 times 4 dimensions so that's exactly what we want and let's change the implementation of BatchNorm1D that we have so that it can take in two-dimensional or three-dimensional inputs and perform accordingly so at the end of the day the fix is relatively straightforward basically the dimension we want to reduce over is either 0 or the tuple 0 and 1 depending on the dimensionality of X so if X dot endim is 2 so it's a two-dimensional tensor then the dimension we want to reduce over is just the integer 0. If X dot endim is 3 so it's a three-dimensional tensor then the dims we're going to assume are 0 and 1 that we want to reduce over and then here we just pass in dim and if the dimensionality of X is anything else we'll now get an error which is good so that should be the fix now I want to point out one more thing we're actually departing from the API of PyTorch here a little bit because when you come to BatchNorm1D in PyTorch you can scroll down and you can see that the input to this layer can either be n by c where n is the batch size and c is the number of features or channels or it actually does accept three-dimensional inputs but it expects it to be n by c by L where L is say like the sequence length or something like that so this is a problem because you see how c is nested here in the middle and so when it gets three-dimensional inputs this BatchNorm layer will reduce over 0 and 2 instead of 0 and 1 so basically PyTorch BatchNorm1D layer assumes that c will always be the first dimension whereas we assume here that c is the last dimension and there are some number of batch dimensions beforehand and so it expects n by c or n by c by L we expect n by c or n by L by c and so it's a deviation I think it's okay I prefer it this way honestly so this is the way that we will keep it for our purposes so I redefined the layers reinitialized the neural net and did a single forward pass with a break just for one step looking at the shapes along the way they're of course identical all the shapes are the same but the way we see that things are actually working as we want them to now is that when we look at the BatchNorm layer the running mean shape is now 1 by 1 by 68 so we're only maintaining 68 means for every one of our channels and we're treating both the 0th and the first dimension as a batch dimension which is exactly what we want so let me retrain the neural net now okay so I retrained the neural net with the bug fix we get a nice curve and when we look at the validation performance we do actually see a slight improvement so it went from 2.0 to 9 to 2.0 to 2 so basically the bug inside the BatchNorm was holding us back like a little bit it looks like and we are getting a tiny improvement now but it's not clear if this is statistically significant and the reason we slightly expect an improvement is because we're not maintaining so many different means and variances that are only estimated using 32 numbers effectively now we are estimating them using 32 times 4 numbers so you just have a lot more numbers that go into any one estimate of the mean and variance and it allows things to be a bit more stable and less wiggly inside those estimates of those statistics so pretty nice with this more general architecture in place we are now set up to push the performance further by increasing the size of the network so for example I've bumped up the number of embeddings to 24 instead of 10 and also increased number of hidden units but using the exact same architecture we now have 76,000 parameters and the training takes a lot longer but we do get a nice curve and then when you actually evaluate the performance we are now getting validation performance of 1.993 so we've crossed over the 2.0 sort of territory and we're at about 1.99 but we are starting to have to wait quite a bit longer and we're a little bit in the dark with respect to the correct setting of the hyperparameters here and the learning rates and so on because the experiments are starting to take longer to train and so we are missing sort of like an experimental harness on which we could run a number of experiments and really tune this architecture very well so I'd like to conclude now with a few notes we basically improved our performance from a starting of 2.1 down to 1.9 but I don't want that to be the focus because honestly we're kind of in the dark we have no experimental harness we're just guessing and checking and this whole thing is terrible we're just looking at the training loss normally you want to look at both the training and the validation loss together the whole thing looks different if you're actually trying to squeeze out numbers that said we did implement this architecture from the WaveNet paper but we did not implement this specific forward pass of it where you have a more complicated a linear layer sort of that is this gated linear layer kind of and there's residual connections and skip connections and so on so we did not implement that we just implemented this structure I would like to briefly hint or preview how what we've done here relates to convolutional neural networks as used in the WaveNet paper and basically the use of convolutions is strictly for efficiency it doesn't actually change the model we've implemented so here for example let me look at a specific name to work with an example so there's a name in our training set and it's DeAndre and it has seven letters so that is eight independent examples in our model so all these rows here are independent examples of DeAndre now you can forward of course any one of these rows independently so I can take my model and call it on any individual index notice by the way here I'm being a little bit tricky the reason for this is that extra at 7 dot shape is just one dimensional array of 8 so you can't actually call the model on it you're going to get an error because there's no batch dimension so when you do extra at list of 7 then the shape of this becomes 1 by 8 so I get an extra batch dimension of 1 and then we can forward the model so that forwards a single example and you might imagine that you actually may want to forward all of these 8 at the same time so pre allocating some memory and then doing a for loop 8 times and forwarding all of those 8 here will give us all the logits in all these different cases now for us with the model as we've implemented it right now this is 8 independent calls to our model but what convolutions allow you to do is it allow you to basically slide this model efficiently over the input sequence and so this for loop can be done not outside in Python but inside of kernels in CUDA and so this for loop gets hidden into the convolution so the convolution basically you can think of it as it's a for loop applying a little linear filter over space of some input sequence and in our case the space we're interested in is one-dimensional and we're interested in sliding these filters over the input data so this diagram actually is fairly good as well basically what we've done is here they are highlighting in black one single sort of like tree of this calculation so just calculating the single output example here and so this is basically what we've implemented here we've implemented a single this black structure we've implemented that and calculate a single output like a single example but with convolutions allow you to do is it allows you to take this black structure and kind of like slide it over the input sequence here and calculate all of these orange outputs at the same time or here that corresponds to calculating all of these outputs of at all the positions of DeAndre at the same time and the reason that this is much more efficient is because number one as I mentioned the for loop is inside the CUDA kernels in the sliding so that makes it efficient but number two notice the variable reuse here for example if we look at this circle this node here this node here is the right child of this node but it's also the left child of the node here and so basically this node and its value is used twice and so right now in this naive way we'd have to recalculate it but here we are allowed to reuse it so in the convolutional neural network you think of these linear layers that we have it up above as filters and we take these filters and they're linear filters and you slide them over input sequence and we calculate the first layer and then the second layer and then third layer and then the output layer of the sandwich and it's all done very efficiently using these convolutions so we're going to cover that in a future video the second thing I hope you took away from this video is you've seen me basically implement all of these layer Lego building blocks or module building blocks and I'm implementing them over here and we've implemented a number of layers together and we've also implementing these these containers and we've overall by torchified our code quite a bit more now basically what we're doing here is we're re-implementing torch.nn which is the neural networks library on top of torch.tensor and it looks very much like this except it is much better because it's in PyTorch instead of a janky-lingy my Jupyter notebook so I think going forward I will probably have considered us having unlocked torch.nn we understand roughly what's in there how these modules work how they're nested and what they're doing on top of torch.tensor so hopefully we'll just switch over and continue and start using torch.nn directly the next thing I hope you got a bit of a sense of is what the development process of building deep neural networks looks like which I think was relatively representative to some extent so number one we are spending a lot of time in the documentation page of PyTorch and we're reading through all the layers looking at documentations what are the shapes of the inputs what can they be what does the layer do and so on unfortunately I have to say the PyTorch documentation is not very good they spend a ton of time on hardcore engineering of all kinds of distributed primitives etc but as far as I can tell no one is maintaining documentation it will lie to you it will be wrong it will be incomplete it will be unclear so unfortunately it is what it is and you just kind of do your best with what they've given us number two the other thing that I hope you got a sense of is there's a ton of trying to make the shapes work and there's a lot of gymnastics around these multi-dimensional arrays and are they two-dimensional three-dimensional four-dimensional what layers take what shapes is it NCL or NLC and you're permuting and viewing and it's just gonna get pretty messy and so that brings me to number three I very often prototype these layers and implementations in Jupyter notebooks and make sure that all the shapes work out and I'm spending a lot of time basically babysitting the shapes and making sure everything is correct and then once I'm satisfied with the functionality in a Jupyter notebook I will take that code and copy paste it into my repository of actual code that I'm training with and so then I'm working with VS code on the side so I usually have Jupyter notebook and VS code I develop a Jupyter notebook I paste into VS code and then I kick off experiments from from the repo of course from the code repository so that's roughly some notes on the development process of working with neural nets lastly I think this lecture unlocks a lot of potential further lectures because number one we have to convert our neural network to actually use these dilated causal convolutional layers so implementing the ConvNet number two potentially starting to get into what this means where our residual connections and skip connections and why are they useful number three a week as I mentioned we don't have any experimental harness so right now I'm just guessing checking everything this is not representative of typical deep learning workflows you have to set up your evaluation harness you can kick off experiments you have lots of arguments that your script can take you're kicking off a lot of experimentation you're looking at a lot of plots of training and validation losses and you're looking at what is working and what is not working and you're working on this like population level and you're doing all these hyperparameter searches and so we've done none of that so far so how to set that up and how to make it good I think it's a whole another topic number three we should probably cover recurrent neural networks, RNNs, LSTMs, GRUs and of course transformers so many places to go and we'll cover that in the future for now sorry I forgot to say that if you are interested I think it is kind of interesting to try to beat this number 1.993 because I really haven't tried a lot of experimentation here and there's quite a bit of longing fruit potentially to still push this further so I haven't tried any other ways of allocating these channels in this neural net maybe the number of dimensions for the embedding is all wrong maybe it's possible to actually take the original network which is one hidden layer and make it big enough and actually beat my fancy hierarchical network it's not obvious that would be kind of embarrassing if this did not do better even once you torture it a little bit maybe you can read the WaveNet paper and try to figure out how some of these layers work and implement them yourselves using what we have and of course you can always tune some of the initialization or some of the optimization and see if you can improve it that way so I'd be curious if people can come up with some ways to beat this and yeah that's it for now Hi everyone. So by now you have probably heard of ChatGPT. It has taken the world and the AI community by storm and it is a system that allows you to interact with an AI and give it text-based tasks. So for example, we can ask ChatGPT to write us a small haiku about how important it is that people understand AI and then they can use it to improve the world and make it more prosperous. So when we run this, AI knowledge brings prosperity for all to see, embrace its power. Okay, not bad. And so you could see that ChatGPT went from left to right and generated all these words sequentially. Now I asked it already the exact same prompt a little bit earlier and it generated a slightly different outcome. AI's power to grow, ignorance holds us back, learn, prosperity waits. So pretty good in both cases and slightly different. So you can see that ChatGPT is a probabilistic system and for any one prompt it can give us multiple answers, sort of replying to it. Now this is just one example of a prompt. People have come up with many, many examples and there are entire websites that index interactions with ChatGPT and so many of them are quite humorous. Explain HTML to me like I'm a dog, write release notes for chess too, write a note about Elon Musk buying a Twitter and so on. So as an example, please write a breaking news article about a leaf falling from a tree and a shocking turn of events. A leaf has fallen from a tree in the local park. Witnesses report that the leaf, which was previously attached to a branch of a tree, detached itself and fell to the ground. Very dramatic. So you can see that this is a pretty remarkable system and it is what we call a language model because it models the sequence of words or characters or tokens more generally and it knows how certain words follow each other in English language. And so from its perspective, what it is doing is it is completing the sequence. So I give it the start of a sequence and it completes the sequence with the outcome. And so it's a language model in that sense. Now I would like to focus on the under the hood components of what makes ChatGPT work. So what is the neural network under the hood that models the sequence of these words? And that comes from this paper called Attention is All You Need. In 2017, a landmark paper, a landmark paper in AI that produced and proposed the transformer architecture. So GPT is short for generatively pre-trained transformer. So transformer is the neural net that actually does all the heavy lifting under the hood. It comes from this paper in 2017. Now, if you read this paper, this reads like a pretty random machine translation paper. And that's because I think the authors didn't fully anticipate the impact that the transformer would have on the field. And this architecture that they produced in the context of machine translation, in their case, actually ended up taking over the rest of AI in the next five years after. And so this architecture with minor changes was copy pasted into a huge amount of applications in AI in more recent years. And that includes at the core of ChatGPT. Now, we are not going to, what I'd like to do now is I'd like to build out something like ChatGPT, but we're not going to be able to, of course, reproduce ChatGPT. This is a very serious production grade system. It is trained on a good chunk of internet, and then there's a lot of pre-training and fine tuning stages to it. And so it's very complicated. What I'd like to focus on is just to train a transformer-based language model. And in our case, it's going to be a character level language model. I still think that is very educational with respect to how these systems work. So I don't want to train on the chunk of internet. We need a smaller dataset. In this case, I propose that we work with my favorite toy dataset. It's called Tiny Shakespeare. And what it is is basically, it's a concatenation of all of the works of Shakespeare, in my understanding. And so this is all of Shakespeare in a single file. This file is about one megabyte, and it's just all of Shakespeare. And what we are going to do now is we're going to basically model how these characters follow each other. So for example, given a chunk of these characters like this, given some context of characters in the past, the transformer neural network will look at the characters that I've highlighted, and it's going to predict that G is likely to come next in the sequence. And it's going to do that because we're going to train that transformer on Shakespeare. And it's just going to try to produce character sequences that look like this. And in that process, it's going to model all the patterns inside this data. So once we've trained the system, I'd just like to give you a preview. We can generate infinite Shakespeare. And of course, it's a fake thing that looks kind of like Shakespeare. Apologies for there's some jank that I'm not able to resolve in here, but you can see how this is going character by character. And it's kind of like predicting Shakespeare-like language. So, verily, my lord, the sights have left thee again, the king, coming with my curses with precious pale. And then Traniosa, something else, et cetera. And this is just coming out of the transformer in a very similar manner as it would come out in chatGPT. In our case, character by character, in chatGPT, it's coming out on the token by token level. And tokens are these sort of like little subword pieces. So they're not word level. They're kind of like word chunk level. And now I've already written this entire code to train these transformers. And it is in a GitHub repository that you can find, and it's called nanoGPT. So, nanoGPT is a repository that you can find on my GitHub. And it's a repository for training transformers on any given text. And what I think is interesting about it, because there's many ways to train transformers, but this is a very simple implementation. So it's just two files of 300 lines of code each. One file defines the GPT model, the transformer, and one file trains it on some given text dataset. And here I'm showing that if you train it on a open web text dataset, which is a fairly large dataset of web pages, then I reproduce the performance of GPT2. So, GPT2 is an early version of OpenAI's GPT from 2017, if I recall correctly. And I've only so far reproduced the smallest 124 million parameter model. But basically, this is just proving that the code base is correctly arranged. And I'm able to load the neural network weights that OpenAI has released later. So, you can take a look at the finished code here in nanoGPT. But what I would like to do in this lecture is I would like to basically write this repository from scratch. So, we're going to begin with an empty file, and we're going to define a transformer piece by piece. We're going to train it on the tiny Shakespeare dataset. And we'll see how we can train it on the big Shakespeare dataset. And we'll see how we can then generate infinite Shakespeare. And, of course, this can copy-paste to any arbitrary text dataset that you like. But my goal really here is to just make you understand and appreciate how under the hood, chat GPT works. And really, all that's required is a proficiency in Python and some basic understanding of calculus and if you also see my previous videos on the same YouTube channel, in particular, my Make More series where I define smaller and simpler neural network language models. So, multi-layered perceptrons and so on. It really introduces the language modeling framework. And then here in this video, we're going to focus on the transformer neural network itself. Okay, so I created a new Google Colab Jupyter Notebook here. And this will allow me to later easily share this code that we're going to develop together with you so you can follow along. So, this will be in a video description later. Now, here I've just done some preliminaries. I downloaded the dataset, the tiny Shakespeare dataset at this URL. And you can see that it's about a one megabyte file. Then here, I open the input.txt file and just read in all the text as a string. And we see that we are working with one million characters roughly. And the first 1000 characters, if we just print them out, are basically what you would expect. This is the first 1000 characters of the tiny Shakespeare dataset, roughly up to here. So, so far, so good. Next, we're going to take this text. And the text is a sequence of characters in Python. So, when I call the set constructor on it, I'm just going to get the set of all the characters that occur in this text. And then I call list on that to create a list of those characters instead of just a set so that I have an ordering, an arbitrary ordering. And then I sort that. So, basically, we get just all the characters that occur in the entire dataset and they're sorted. Now, the number of them is going to be our vocabulary size. These are the possible elements of our sequences. And we see that when I print here the characters, there's 65 of them in total. There's a space character, and then all kinds of special characters, and then capitals and lowercase letters. So, that's our vocabulary. And that's the sort of like possible characters that the model can see or emit. Okay, so next, we would like to develop some strategy to tokenize the input text. Now, when people say tokenize, they mean convert the raw text as a string to some sequence of integers according to some notebook, according to some vocabulary of possible elements. So, as an example, here we are going to be building a character level language model. So, we're simply going to be translating individual characters into integers. So, let me show you a chunk of code that sort of does that for us. So, we're building both the encoder and the decoder. And let me just talk through what's happening here. When we encode an arbitrary text like hi there, we're going to receive a list of integers that represents that string. So, for example, 46, 47, et cetera. And then we also have the reverse mapping. So, we can take this list and decode it to get back the exact same string. So, it's really just like a translation to integers and back for arbitrary string. And for us, it is done on a character level. Now, the way this was achieved is we just iterate over all the characters here and create a lookup table from the character to the integer and vice versa. And then to encode some string we simply translate all the characters individually and to decode it back we use the reverse mapping and concatenate all of it. Now, this is only one of many possible encodings or many possible sort of tokenizers. And it's a very simple one. But there's many other schemas that people have come up with in practice. So, for example, Google uses a sentence piece. So, sentence piece will also encode text into integers, but in a different schema and using a different vocabulary. So, for example, OpenAI uses a byte pair encoder. And sentence piece is a subword tokenizer. And what that means is that you're not encoding entire words, but you're not also encoding individual characters. It's a subword unit level. And that's usually what's adopted in practice. For example, also OpenAI has this library called TicToken that uses a byte pair encoding tokenizer. And you can also just encode words into like Hello World into a list of integers. So, as an example, I'm using the TicToken library here. I'm getting the encoding for GPT-2 or that was used for GPT-2. Instead of just having 65 possible characters or tokens, they have 50,000 tokens. And so, when they encode the exact same string high there, we only get a list of three integers. But those integers are not between zero and 64. They are between zero and 50,256. So, basically, you can trade off the codebook size and the sequence lengths. So, you can have very long sequences of integers with very small vocabularies, or you can have short sequences of integers with very large vocabularies. And so, typically, people use in practice these subword encodings, but I'd like to keep our tokenizer very simple. So, we're using character level tokenizer. And that means that we have very small codebooks. We have very simple encode and decode functions, but we do get very long sequences as a result. But that's the level at which we're going to stick with this lecture because it's the simplest thing. Okay, so now that we have an encoder and a decoder, effectively a tokenizer, we can tokenize the entire training set of Shakespeare. So, here's a chunk of code that does that. And I'm going to start to use the PyTorch library, and specifically the Torch.Tensor from the PyTorch library. So, we're going to take all of the text in Tiny Shakespeare, encode it, and then wrap it into a Torch.Tensor to get the data tensor. So, here's what the data tensor looks like when I look at just the first 1,000 characters, or the 1,000 elements of it. So, we see that we have a massive sequence of integers. And this sequence of integers here is basically an identical translation of the first 1,000 characters here. So, I believe, for example, that 0 is a newline character, and maybe 1 is a space. I'm not 100% sure. But from now on, the entire data set of text is re-represented as just, it's just stretched out as a single, very large sequence of integers. Let me do one more thing before we move on here. I'd like to separate out our data set into a train and a validation split. So, in particular, we're going to take the first 90% of the data set and consider that to be the training data for the transformer. And we're going to withhold the last 10% at the end of it to be the validation data. And this will help us understand to what extent our model is overfitting. So, we're going to basically hide and keep the validation data on the side, because we don't want just a perfect memorization of this exact Shakespeare. We want a neural network that sort of creates Shakespeare-like text. And so, it should be fairly likely for it to produce the actual, like, stowed-away, true Shakespeare text. And so, we're going to use this to get a sense of the overfitting. Okay, so now we would like to start plugging these text sequences or integer sequences into the transformer so that it can train and learn those patterns. Now, the important thing to realize is we're never going to actually feed entire text into a transformer all at once. That would be computationally very expensive and prohibitive. So, when we actually train a transformer on a lot of these data sets, we only work with chunks of the data set. And when we train the transformer, we basically sample random little chunks out of the training set and train on just chunks at a time. And these chunks have basically some kind of a length and some maximum length. Now, the maximum length typically, at least in the code I usually write, is called block size. You can find it under different names, like context length or something like that. Let's start with the block size of just eight. And let me look at the first trained data characters, the first block size plus one characters. I'll explain why plus one in a second. So, this is the first nine characters in the sequence in the training set. Now, what I'd like to point out is that when you sample a chunk of data like this, so say these nine characters out of the training set, this actually has multiple examples packed into it. And that's because all of these characters follow each other. And so, what this thing is going to say when we plug it into a transformer is we're going to actually simultaneously train it to make prediction at every one of these positions. Now, in a chunk of nine characters, there's actually eight individual examples packed in there. So, there's the example that when 18, in the context of 18, 47 likely comes next. In the context of 18 and 47, 56 comes next. In the context of 18, 47, 56, 57 can come next, and so on. So, that's the eight individual examples. Let me actually spell it out with code. So, here's a chunk of code to illustrate. X are the inputs to the transformer. It will just be the first block size characters. Y will be the next block size characters. So, it's offset by one. And that's because Y are the targets for each position in the input. And then here, I'm iterating over all the block size of eight. And the context is always all the characters in X up to T and including T. And the target is always the T-th character, but in the targets array Y. So, let me just run this. And basically, it spells out what I said in words. These are the eight examples hidden in a chunk of nine characters that we sampled from the training set. I want to mention one more thing. We train on all the eight examples here with context between one all the way up to context of block size. And we train on that not just for computational reasons because we happen to have the sequence already or something like that. It's not just done for efficiency. It's also done to make the transformer network be used to seeing contexts all the way from as little as one all the way to block size. And we'd like the transformer to be used to seeing everything in between. And that's going to be useful later during inference because while we're sampling, we can start sampling generation with as little as one character of context. And the transformer knows how to predict the next character with all the way up to just context of one. And so, then it can predict everything up to block size. And after block size, we have to start truncating because the transformer will never receive more than block size inputs when it's predicting the next character. Okay. So, we've looked at the time dimension of the tensors that are going to be feeding into the transformer. There's one more dimension to care about, and that is the batch dimension. And so, as we're sampling these chunks of text, we're going to be actually every time we're going to feed them into a transformer, we're going to have many batches of multiple chunks of text that are all stacked up in a single tensor. And that's just done for efficiency just so that we can keep the GPUs busy because they are very good at parallel processing of data. And so, we just want to process multiple chunks all at the same time. But those chunks are processed completely, they don't talk to each other, and so on. So, let me basically just generalize this and introduce a batch dimension. Here's a chunk of code. Let me just run it, and then I'm going to explain what it does. So, here, because we're going to start sampling random locations in the data sets to pull chunks from, I am setting the seed in the random number generator so that the numbers I see here are going to be the same numbers you see later if you try to reproduce this. Now, the batch size here is how many independent sequences we are processing every forward-backward pass of the transformer. The block size, as I explained, is the maximum context length to make those predictions. So, let's say batch size 4, block size 8, and then here's how we get batch for any arbitrary split. If the split is a training split, then we're going to look at train data, otherwise at val data. That gives us the data array. And then, when I generate random positions to grab a chunk out of, I actually generate batch size number of random offsets. So, because this is 4, IX is going to be 4 numbers that are randomly generated between 0 and len of data minus block size. So, it's just random offsets into the training set. And then, Xs, as I explained, are the first block size character. starting at i. The y's are the offset by one of that, so just add plus one. And then we're going to get those chunks for every one of integers i in ix and use a torch.stack to take all those one-dimensional tensors as we saw here, and we're going to stack them up as rows. And so they all become a row in a 4x8 tensor. So here's where I'm printing them. When I sample a batch xB and yB, the inputs to the transformer now are... The input x is the 4x8 tensor, four rows of eight columns, and each one of these is a chunk of the training set. And then the targets here are in the associated array y, and they will come in to the transformer all the way at the end to create the loss function. So they will give us the correct answer for every single position inside x. And then these are the four independent rows. So spelled out as we did before, this 4x8 array contains a total of 32 examples, and they're completely independent as far as the transformer is concerned. So when the input is 24, the target is 43, or rather 43 here in the y array. When the input is 24, 43, the target is 58. When the input is 24, 43, 58, the target is 5, etc. Or like when it is a 52, 58, 1, the target is 58. So you can sort of see this spelled out. These are the 32 independent examples packed in to a single batch of the input x, and then the desired targets are in y. And so now this integer tensor of x is going to feed into the transformer, and that transformer is going to simultaneously process all these examples and then look up the correct integers to predict in every one of these positions in the tensor y. Okay, so now that we have our batch of input that we'd like to feed into a transformer, let's start basically feeding this into neural networks. Now we're going to start off with the simplest possible neural network, which in the case of language modeling, in my opinion, is the bigram language model. And we've covered the bigram language model in my Make More series in a lot of depth. And so here I'm going to sort of go faster, and let's just implement the PyTorch module directly that implements the bigram language model. So I'm importing the PyTorch NN module for reproducibility. And then here I'm constructing a bigram language model, which is a subclass of NN module. And then I'm calling it, and I'm passing in the inputs and the targets. And I'm just printing. Now when the inputs and targets come here, you see that I'm just taking the index, the inputs x here, which I renamed to idx, and I'm just passing them into this token embedding table. So what's going on here is that here in the constructor, we are creating a token embedding table, and it is of size vocab size by vocab size. And we're using an n.embedding, which is a very thin wrapper around basically a tensor of shape, vocab size by vocab size. And what's happening here is that when we pass idx here, every single integer in our input is going to refer to this embedding table, and is going to pluck out a row of that embedding table corresponding to its index. So 24 here will go to the embedding table, and will pluck out the 24th row. And then 43 will go here and pluck out the 43rd row, et cetera. And then PyTorch is going to arrange all of this into a batch by time by channel tensor. In this case, batch is 4, time is 8, and c, which is the channels, is vocab size or 65. And so we're just going to pluck out all those rows, arrange them in a b by t by c, and now we're going to interpret this as the logits, which are basically the scores for the next character in the sequence. And so what's happening here is we are predicting what comes next based on just the individual identity of a single token. And you can do that because, I mean, currently the tokens are not talking to each other, and they're not seeing any context except for they're just seeing themselves. So I'm a token number 5, and then I can actually make pretty decent predictions about what comes next just by knowing that I'm token 5, because some characters follow other characters in typical scenarios. So we saw a lot of this in a lot more depth in the Makemore series. And here, if I just run this, then we currently get the predictions, the scores, the logits for every one of the 4 by 8 positions. Now that we've made predictions about what comes next, we'd like to evaluate the loss function. And so in Makemore series, we saw that a good way to measure a loss or a quality of the predictions is to use the negative log likelihood loss, which is also implemented in PyTorch under the name cross entropy. So what we'd like to do here is loss is the cross entropy on the predictions and the targets. And so this measures the quality of the logits with respect to the targets. In other words, we have the identity of the next character, so how well are we predicting the next character based on the logits? And intuitively, the correct dimension of logits, depending on whatever the target is, should have a very high number and all the other dimensions should be a very low number. Now, the issue is that this won't actually... this is what we want. We want to basically output the logits and the loss. This is what we want, but unfortunately, this won't actually run. We get an error message, but intuitively, we want to measure this. Now, when we go to the PyTorch cross entropy documentation here, we're trying to call the cross entropy in its functional form, so that means we don't have to create a module for it. But here, when we go to the documentation, you have to look into the details of how PyTorch expects these inputs. And basically, the issue here is PyTorch expects, if you have multidimensional input, which we do because we have a B by T by C tensor, then it actually really wants the channels to be the second dimension here. So basically, it wants a B by C by T instead of a B by T by C. And so it's just the details of how PyTorch treats these kinds of inputs, and so we don't actually want to deal with that. So what we're going to do instead is we need to basically reshape our logits. So here's what I like to do. I like to basically give names to the dimensions. So logits.shape is B by T by C and unpack those numbers. And then let's say that logits equals logits.view, and we want it to be a B times T by C, so just a two-dimensional array. So we're going to take all of these positions here, and we're going to stretch them out in a one-dimensional sequence. And preserve the channel dimension as the second dimension. So we're just kind of like stretching out the array so it's two-dimensional, and in that case, it's going to better conform to what PyTorch sort of expects in its dimensions. Now, we have to do the same to targets, because currently targets are of shape B by T, and we want it to be just B times T, so one-dimensional. Now, alternatively, you could also still just do minus one, because PyTorch will guess what this should be if you want to lay it out, but let me just be explicit and say B times T. Once we reshape this, it will match the cross-entropy case, and then we should be able to evaluate our loss. Okay, so hit that right now, and we can do loss. And so currently we see that the loss is 4.87. Now, because we have 65 possible vocabulary elements, we can actually guess at what the loss should be, and in particular, we covered negative log likelihood in a lot of detail. We are expecting log or ln of 1 over 65 and negative of that. So we're expecting the loss to be about 4.17, but we're getting 4.87, and so that's telling us that the initial predictions are not super diffuse. They've got a little bit of entropy, and so we're guessing wrong. So yes, but actually we are able to evaluate the loss. Okay, so now that we can evaluate the quality of the model on some data, we'd like to also be able to generate from the model. So let's do the generation. Now, I'm going to go again a little bit faster here, because I covered all this already in previous videos. So here's a generate function for the model. So we take the same kind of input, idx, here, and basically this is the current context of some characters in some batch. So it's also b by t, and the job of generate is to basically take this b by t and extend it to be b by t plus 1, plus 2, plus 3, and so it's just basically it continues the generation in all the batch dimensions in the time dimension. So that's its job, and it will do that for max new tokens. So you can see here on the bottom, there's going to be some stuff here, but on the bottom, whatever is predicted is concatenated on top of the previous idx along the first dimension, which is the time dimension, to create a b by t plus 1. So that becomes a new idx. So the job of generate is to take a b by t and make it a b by t plus 1, plus 2, plus 3, as many as we want max new tokens. So this is the generation from the model. Now inside the generation, what are we doing? We're taking the current indices. We're getting the predictions. So we get those are in the logits, and then the loss here is going to be ignored because we're not using that, and we have no targets that are sort of ground truth targets that we're going to be comparing with. Then once we get the logits, we are only focusing on the last step. So instead of a b by t by c, we're going to pluck out the negative 1, the last element in the time dimension because those are the predictions for what comes next. So that gives us the logits, which we then convert to probabilities via softmax, and then we use torsion multinomial to sample from those probabilities, and we ask PyTorch to give us one sample. And so idx next will become a b by 1 because in each one of the batch dimensions, we're going to have a single prediction for what comes next. So if some samples equals 1, we'll make this b a 1. And then we're going to take those integers that come from the sampling process according to the probability distribution given here, and those integers get just concatenated on top of the current sort of like running stream of integers, and this gives us a b by t plus 1. And then we can return that. Now, one thing here is you see how I'm calling self of idx, which will end up going to the forward function. I'm not providing any targets. So currently this would give an error because targets is sort of like not given. So targets has to be optional. So targets is none by default. And then if targets is none, then there's no loss to create. So it's just loss is none. But else all of this happens and we can create a loss. So this will make it so if we have the targets, we provide them and get a loss. And then the targets will just get the logits. So this here will generate from the model. And let's take that for a ride now. Oops. So I have another code chunk here, which will generate for the model from the model. And OK, this is kind of crazy. So maybe let me break this down. So these are the idx, right? So I'm creating a batch will be just one time will be just one. So I'm creating a little one by one tensor, and it's holding a zero. And the D type, the data type is integer. So zero is going to be how we kick off the generation. And remember that zero is the element standing for a new line character. So it's kind of like a reasonable thing to feed in as the very first character in a sequence to be the new line. So it's going to be idx, which we're going to feed in here. Then we're going to ask for 100 tokens. And then end.generate will continue that. Now, because generate works on the level of batches, we then have to index into the zeroth row to basically unplug the single batch dimension that exists. And then that gives us a time steps, just a one dimensional array of all the indices, which we will convert to simple Python list from PyTorch tensor so that that can feed into our decode function and convert those integers into text. So let me bring this back. And we're generating 100 tokens. Let's run. And here's the generation that we achieved. So obviously it's garbage. And the reason it's garbage is because this is a totally random model. So next up, we're going to want to train this model. Now, one more thing I wanted to point out here is this function is written to be general, but it's kind of like ridiculous right now because we're feeding in all this, we're building out this context, and we're concatenating it all, and we're always feeding it all into the model. But that's kind of ridiculous because this is just a simple bigram model. So to make, for example, this prediction about k, we only needed this w. But actually what we fed into the model is we fed the entire sequence. And then we only looked at the very last piece and predicted k. So the only reason I'm writing it in this way is because right now this is a bigram model, but I'd like to keep this function fixed, and I'd like it to work later when our characters actually basically look further in the history. And so right now the history is not used, so this looks silly. But eventually the history will be used, and so that's why we want to do it this way. So just a quick comment on that. So now we see that this is random, so let's train the model so it becomes a bit less random. Okay, let's now train the model. So first what I'm going to do is I'm going to create a PyTorch optimization object. So here we are using the optimizer AdamW. Now in the Makemore series, we've only ever used stochastic gradient descent, the simplest possible optimizer, which you can get using the SGD instead. But I want to use Adam, which is a much more advanced and popular optimizer, and it works extremely well. For a typical good setting for the learning rate is roughly 3E-4, but for very, very small networks, like is the case here, you can get away with much, much higher learning rates, 1E-3 or even higher probably. But let me create the optimizer object, which will basically take the gradients and update the parameters using the gradients. And then here our batch size up above was only 4, so let me actually use something bigger, let's say 32. And then for some number of steps, we are sampling a new batch of data, we're evaluating the loss, we're zeroing out all the gradients from the previous step, getting the gradients for all the parameters, and then using those gradients to update our parameters. So typical training loop, as we saw in the Makemore series. So let me now run this for say 100 iterations, and let's see what kind of losses we're going to get. So we started around 4.7, and now we're getting down to like 4.6, 4.5, etc. So the optimization is definitely happening, but let's sort of try to increase the number of iterations and only print at the end, because we probably will not train for longer. Okay, so we're down to 3.6 roughly, down to 3. This is the most janky optimization. Okay, it's working. Let's just do 10,000. And then from here, we want to copy this, and hopefully we're going to get something reasonable. And of course, it's not going to be Shakespeare from a bigram model, but at least we see that the loss is improving, and hopefully we're expecting something a bit more reasonable. Okay, so we're down at about 2.5-ish. Let's see what we get. Okay, a dramatic improvement certainly on what we had here. So let me just increase the number of tokens. Okay, so we see that we're starting to get something at least like reasonable-ish. Certainly not Shakespeare, but the model is making progress. So that is the simplest possible model. So now what I'd like to do is... Obviously, this is a very simple model, because the tokens are not talking to each other. So given the previous context of whatever was generated, we're only looking at the very last character to make the predictions about what comes next. So now these tokens have to start talking to each other and figuring out what is in the context so that they can make better predictions for what comes next. And this is how we're going to kick off the transformer. Okay, so next I took the code that we developed in this Jupyter notebook and I converted it to be a script. And I'm doing this because I just want to simplify our intermediate work into just the final product that we have at this point. So in the top here, I put all the hyperparameters that we've defined. I introduced a few, and I'm going to speak to that in a little bit. Otherwise, a lot of this should be recognizable. Reproducibility, read data, get the encoder and the decoder, create the train and test splits, use the kind of like data loader that gets a batch of the inputs and targets. This is new, and I'll talk about it in a second. Now, this is the bigram language model that we developed, and it can forward and give us a logits and loss, and it can generate. And then here we are creating the optimizer, and this is the training loop. So everything here should look pretty familiar. Now, some of the small things that I added. Number one, I added the ability to run on a GPU if you have it. So if you have a GPU, then this will use CUDA instead of just CPU, and everything will be a lot more faster. Now, when device becomes CUDA, then we need to make sure that when we load the data, we move it to device. When we create the model, we want to move the model parameters to device. So as an example, here we have the NN embedding table, and it's got a dot weight inside it, which stores the lookup table. So that would be moved to the GPU so that all the calculations here happen on the GPU, and they can be a lot faster. And then finally here, when I'm creating the context that feeds it to generate, I have to make sure that I create on the device. Number two, what I introduced is the fact that here in the training loop, here I was just printing the lost.item inside the training loop. But this is a very noisy measurement of the current loss because every batch will be more or less lucky. And so what I want to do usually is... is I have an estimate loss function and the estimate loss basically then goes up here and it averages up the loss over multiple batches. So in particular we're going to iterate evaliter times and we're going to basically get our loss and then we're going to get the average loss for both splits and so this will be a lot less noisy. So here when we call the estimate loss we're going to report the pretty accurate train and validation loss. Now when we come back up you'll notice a few things here. I'm setting the model to evaluation phase and down here I'm resetting it back to training phase. Now right now for our model as is this doesn't actually do anything because the only thing inside this model is this nn.embedding and this network would behave both would behave the same in both evaluation mode and training mode. We have no dropout layers we have no batch norm layers etc but it is a good practice to think through what mode your neural network is in because some layers will have different behavior at inference time or training time. And there's also this context manager torch.nograd and this is just telling PyTorch that everything that happens inside this function we will not call.backward on and so PyTorch can be a lot more efficient with its memory use because it doesn't have to store all the intermediate variables because we're never going to call backward and so it can it can be a lot more efficient in that way. So also a good practice to tell PyTorch when we don't intend to do backpropagation. So right now the script is about 120 lines of code and that's kind of our starter code. I'm calling it by gram.py and I'm going to release it later. Now running this script gives us output in the terminal and it looks something like this. It basically as I ran this code it was giving me the train loss and the val loss and we see that we convert to somewhere around 2.5 with the by gram model and then here's the sample that we produced at the end. And so we have everything packaged up in the script and we're in a good position now to iterate on this. Okay so we are almost ready to start writing our very first self-attention block for processing these tokens. Now before we actually get there I want to get you used to a mathematical trick that is used in the self-attention inside a transformer and it's really just like at the heart of an efficient implementation of self-attention. And so I want to work with this toy example to just get you used to this operation and then it's going to make it much more clear once we actually get to it in the script again. So let's create a b by t by c where b, t and c are just 4, 8 and 2 in the story example. And these are basically channels and we have batches and we have the time component and we have some information at each point in the sequence so c. Now what we would like to do is we would like these tokens so we have up to eight tokens here in a batch and these eight tokens are currently not talking to each other and we would like them to talk to each other. We'd like to couple them and in particular we want to couple them in this very specific way so the token for example at the fifth location it should not communicate with tokens in the sixth seventh and eighth location because those are future tokens in the sequence. The token on the fifth location should only talk to the one in the fourth, third, second and first. So it's only so information only flows from previous context to the current time step and we cannot get any information from the future because we are about to try to predict the future. So what is the easiest way for tokens to communicate? Okay the easiest way I would say is okay if we are up to if we're a fifth token and I'd like to communicate with my past the simplest way we can do that is to just do a weight is to just do an average of all the of all the preceding elements. So for example if I'm the fifth token I would like to take the channels that make up that are information at my step but then also the channels from the fourth step third step second step in the first step I'd like to average those up and then that would become sort of like a feature vector that summarizes me in the context of my history. Now of course just doing a sum or like an average is an extremely weak form of interaction like this communication is extremely lossy we've lost a ton of information about spatial arrangements of all those tokens but that's okay for now we'll see how we can bring that information back later. For now what we would like to do is for every single batch element independently for every teeth token in that sequence we'd like to now calculate the average of all the vectors in all the previous tokens and also at this token so let's write that out. I have a small snippet here and instead of just fumbling around let me just copy paste it and talk to it. So in other words we're going to create X and BOW is short for bag of words because bag of words is is kind of like a term that people use when you are just averaging up things so this is just a bag of words basically there's a word stored on every one of these eight locations and we're doing a bag of words just averaging. So in the beginning we're going to say that it's just initialized at zero and then I'm doing a for loop here so we're not being efficient yet that's coming but for now we're just iterating over all the batch dimensions independently iterating over time and then the previous tokens are at this batch a dimension and then everything up to and including the teeth token. So when we slice out X in this way Xprev becomes of shape how many teeth elements there were in the past and then of course C so all the two-dimensional information from these little tokens. So that's the previous sort of chunk of tokens from my current sequence and then I'm just doing the average or the mean over the zero dimension so I'm averaging out the time here and I'm just going to get a little C one-dimensional vector which I'm going to store in X bag of words so I can run this and this is not going to be very informative because let's see so this is X of zero so this is the zeroth batch element and then Xpo at zero now you see how the at the first location here you see that the two are equal and that's because it's we're just doing an average of this one token but here this one is now an average of these two and now this one is an average of these three and so on so and this last one is the average of all of these elements so vertical average just averaging up all the tokens now gives this outcome here so this is all well and good but this is very inefficient now the trick is that we can be very very efficient about doing this using matrix multiplication so that's the mathematical trick and let me show you what I mean let's work with the toy example here let me run it and I'll explain I have a simple matrix here that is a 3x3 of all ones a matrix B of just random numbers and it's a 3x2 and a matrix C which will be 3x3 multiply 3x2 which will give out a 3x2 so here we're just using matrix multiplication so a multiply B gives us C okay so how are these numbers in C achieved right so this number in the top left is the first row of a dot product with the first column of B and since all the the row of a right now is all just ones then the dot product here with with this column of B is just going to do a sum of these of this column so 2 plus 6 plus 6 is 14 the element here in the output of C is also the first column here the first row of a multiplied now with the second column of B so 7 plus 4 plus plus 5 is 16 now you see that there's repeating elements here so this 14 again is because this row is again all ones and it's multiplying the first column of B so we get 14 and this one is and so on so this last number here is the last row dot product last column now the trick here is the following this is just a boring number of it's just a boring array of all ones but Torch has this function called the trill which is short for a triangular something like that and you can wrap it in Torch.once and we'll just return the lower triangular portion of this okay so now it will basically zero out these guys here so we just get the lower triangular part well what happens if we do that so now we'll have A like this and B like this and now what are we getting here in C? well what is this number? well this is the first row times the first column and because this is zeros these elements here are now ignored so we just get a 2 and then this number here is the first row times the second column and because these are zeros they get ignored and it's just 7. the 7 multiplies this 1. but look what happened here because this is 1 and then zeros we what ended up happening is we're just plucking out the row of this row of B and that's what we got now here we have 1 1 0 so here 1 1 0 dot product with these two columns will now give us 2 plus 6 which is 8 and 7 plus 4 which is 11 and because this is 1 1 1 we ended up with the addition of all of them and so basically depending on how many ones and zeros we have here we are basically doing a sum currently of the variable number of these rows and that gets deposited into C so currently we're doing sums because these are ones but we can also do average right and you can start to see how we could do average of the rows of B sort of in an incremental fashion because we don't have to we can basically normalize these rows so that they sum to 1 and then we're gonna get an average so if we took A and then we did A equals A divide torch dot sum in the of A in the 1th dimension and then let's keep them as true so therefore the broadcasting will work out so if I rerun this you see now that these rows now sum to 1 so this row is 1 this row is 0.5.5 is 0 and here we get 1 3rd and now when we do A multiply B what are we getting here we are just getting the first row first row here now we are getting the average of the first two rows okay so 2 and 6 average is 4 and 4 and 7 average is 5.5 and on the bottom here we are now getting the average of these three rows so the average of all of elements of B are now deposited here and so you can see that by manipulating these elements of this multiplying matrix and then multiplying it with any given matrix we can do these averages in this incremental fashion because we just get and we can manipulate that based on the elements of A. Okay so that's very convenient so let's swing back up here and see how we can vectorize this and make it much more efficient using what we've learned so in particular we are going to produce an array A but here I'm going to call it way short for weights but this is our A and this is how much of every row we want to average up and it's going to be an average because you can see that these rows sum to 1 so this is our A and then our B in this example of course is X so what's going to happen here now is that we are going to have an X bow 2 and this X bow 2 is going to be way multiplying our X so let's think this through way is T by T and this is matrix multiplying in PyTorch a B by T by C and it's giving us what shape so PyTorch will come here and it will see that these shapes are not the same so it will create a batch dimension here and this is a batch matrix multiply and so it will apply this matrix multiplication in all the batch elements in parallel and individually and then for each batch element there will be a T by T multiplying T by C exactly as we had below so this will now create B by T by C and X bow 2 will now become identical to X bow so we can see that Torch.allclose of X bow and X bow 2 should be true now so this kind of like convinces us that these are in fact the same so X bow and X bow 2 if I just print them okay we're not gonna be able to okay we're not gonna be able to just stare it down but well let me try X bow basically just at the zeroth element and X bow 2 at the zeroth element so just the first batch and we should see that this and that should be identical which they are right so what happened here the trick is we were able to use batched matrix multiply to do this aggregation really and it's a weighted aggregation and the weights are specified in this T by T array and we're basically doing weighted sums and these weighted sums are according to the weights inside here they take on sort of this triangular form and so that means that a token at the Tth dimension will only get sort of information from the tokens preceding it so that's exactly what we want and finally I would like to rewrite it in one more way and we're going to see why that's useful so this is the third version and it's also identical to the first and second but let me talk through it it uses softmax so trill here is this matrix lower triangular ones way begins as all zero okay so if I just print way in the beginning it's all zero then I used masked fill so what this is doing is way dot masked fill it's all zeros and I'm saying for all the elements where trill is equal to equal zero make them be negative infinity so all the elements where trill is zero will become negative infinity now so this is what we get and then the final line here is softmax so if I take a softmax along every single so dim is negative one so along every single row if I do a softmax what is that going to do well softmax is it's also like a normalization operation right and so spoiler alert you get the exact same matrix let me bring back to softmax and recall that in softmax we're going to exponentiate every single one of these and then we're going to divide by the sum and so if we exponentiate every single element here we're going to get a one and here we're going to get basically zero zero zero zero everywhere else and then when we normalize we just get one here we're gonna get one one and then zeros and then softmax will again divide and this will give us 0.5 0.5 and so on and so this is also the the same way to produce this mask now the reason that this is a bit more interesting and the reason we're going to end up using it in self-attention is that these weights here begin with zero and you can think of this as like an interaction strength or like an affinity so basically it's telling us how much of each token from the past do we want to aggregate and average up and then this line is saying tokens from the past cannot communicate by setting them to negative infinity we're saying that we will not aggregate anything from those tokens and so basically this then goes through softmax and through the weighted and this is the aggregation through matrix multiplication and so what this is now is you can think of these as these zeros are currently just set by us to be zero but a quick preview is that these affinities between the tokens are not going to be just constant at zero they're going to be data dependent these tokens are going to start looking at each other and some tokens will find other tokens more or less interesting and depending on what their values are they're going to find each other interesting to different amounts and I'm going to call those affinities I think and then here we are saying the future cannot communicate with the past we're gonna clamp them and then when we normalize and sum we're gonna aggregate sort of their values depending on how interesting they find each other and so that's the preview for self-attention and basically long story short from this entire section is that you can do weighted aggregations of your past elements by having by using matrix multiplication of a lower triangular fashion and then the elements here in the lower triangular part are telling you how much of each element fuses into this position so we're going to use this trick now to develop the self-attention block so first let's get some quick preliminaries out of the way first the thing I'm kind of bothered by is that you see how we're passing in vocab size into the constructor there's no need to do that because vocab size is already defined up top as a global variable so there's no need to pass this stuff around next what I want to do is I don't want to actually create I want to create like a level of indirection here where we don't directly go to the embedding for the logits but instead we go through this intermediate phase because we're going to start making that bigger so let me introduce a new variable nembed it's short for number of embedding dimensions so nembed here will be say 32 that was a suggestion from github copilot by the way it also should have 32 which is a good number so this is an embedding table and only 32 dimensional embeddings so then here this is not going to give us logits directly instead this is going to give us token embeddings that's what I'm going to call it and then to go from the token embeddings to the logits we're going to need a linear layer so self.lmhead let's call it short for language modeling head is nnlinear from nembed up to vocab size and then when we swing over here we're actually going to get the logits by exactly what the copilot says now we have to be careful here because this C and this C are not equal this is nembed C and this is vocab size so let's just say that nembed is equal to C and then this just goes This creates one spurious layer of interaction through a linear layer, but this should basically run. So we see that this runs, and this currently looks kind of spurious, but we're going to build on top of this. Now next up, so far we've taken these indices and we've encoded them based on the identity of the tokens inside IDX. The next thing that people very often do is that we're not just encoding the identity of these tokens, but also their position. So we're going to have a second position embedding table here. So self.positionEmbeddingTable is an embedding of block size by an embed. And so each position from 0 to block size minus 1 will also get its own embedding vector. And then here, first let me decode b by t from idx.shade. And then here we're also going to have a pos.embedding, which is the positional embedding, and this is tor.arrange. So this will be basically just integers from 0 to t minus 1. And all of those integers from 0 to t minus 1 get embedded through the table to create a t by c. And then here, this gets renamed to just say x, and x will be the addition of the token embeddings with the positional embeddings. And here, the broadcasting node will work out. So b by t by c plus t by c. This gets right aligned, a new dimension of 1 gets added, and it gets broadcasted across batch. So at this point, x holds not just the token identities, but the positions at which these tokens occur. And this is currently not that useful because, of course, we just have a simple bigram model. So it doesn't matter if you're in the fifth position, the second position, or wherever. It's all translation invariant at this stage. So this information currently wouldn't help. But as we work on the self-attention block, we'll see that this starts to matter. OK, so now we get the crux of self-attention. So this is probably the most important part of this video to understand. We're going to implement a small self-attention for a single individual head, as they're called. So we start off with where we were. So all of this code is familiar. So right now, I'm working with an example where I change the number of channels from 2 to 32. So we have a 4 by 8 arrangement of tokens. And the information at each token is currently 32-dimensional. But we just are working with random numbers. Now, we saw here that the code as we had it before does a simple weight, a simple average, of all the past tokens and the current token. So it's just the previous information and the current information is just being mixed together in an average. And that's what this code currently achieves. And it does so by creating this lower triangular structure, which allows us to mask out this weight matrix that we create. So we mask it out, and then we normalize it. And currently, when we initialize the affinities between all the different tokens or nodes, I'm going to use those terms interchangeably. So when we initialize the affinities between all the different tokens to be 0, then we see that weight gives us this structure where every single row has these uniform numbers. And so that's what then in this matrix multiply makes it so that we're doing a simple average. Now, we don't actually want this to be all uniform, because different tokens will find different other tokens more or less interesting, and we want that to be data dependent. So, for example, if I'm a vowel, then maybe I'm looking for consonants in my past, and maybe I want to know what those consonants are, and I want that information to flow to me. And so I want to now gather information from the past, but I want to do it in a data dependent way. And this is the problem that self-attention solves. Now, the way self-attention solves this is the following. Every single node or every single token at each position will emit two vectors. It will emit a query, and it will emit a key. Now, the query vector, roughly speaking, is what am I looking for? And the key vector, roughly speaking, is what do I contain? And then the way we get affinities between these tokens now in a sequence is we basically just do a dot product between the keys and the queries. So my query dot products with all the keys of all the other tokens, and that dot product now becomes way. And so if the key and the query are sort of aligned, they will interact to a very high amount, and then I will get to learn more about that specific token as opposed to any other token in the sequence. So let's implement this now. We're going to implement a single what's called head of self-attention. So this is just one head. There's a hyperparameter involved with these heads, which is the head size. And then here I'm initializing linear modules, and I'm using bias equals false, so these are just going to apply a matrix multiply with some fixed weights. And now let me produce a key and queue, k and q, by forwarding these modules on x. So the size of this will now become b by t by 16, because that is the head size, and the same here, b by t by 16. So this being the head size. So you see here that when I forward this linear on top of my x, all the tokens in all the positions in the b by t arrangement, all of them in parallel and independently produce a key and a query. So no communication has happened yet. But the communication comes now. All the queries will dot product with all the keys. So basically what we want is we want way now, or the affinities between these, to be query multiplying key. But we have to be careful with, we can't matrix multiply this, we actually need to transpose k. But we have to be also careful because these are, we need to have the bash dimension. So in particular we want to transpose the last two dimensions, dimension negative one and dimension negative two. So negative two, negative one. And so this matrix multiply now will basically do the following. B by t by 16, matrix multiplies b by 16 by t to give us b by t by t. Right? So for every row of b, we're now going to have a t square matrix giving us the affinities. And these are now the way. So they're not zeros. They are now coming from this dot product between the keys and the queries. So this can now run. I can run this. And the weighted aggregation now is a function in a data-dependent manner between the keys and queries of these nodes. So just inspecting what happened here, the way takes on this form. And you see that before way was just a constant. So it was applied in the same way to all the batch elements. But now every single batch element will have different sort of way because every single batch element contains different tokens at different positions. And so this is now data-dependent. So when we look at just the zeroth row, for example, in the input, these are the weights that came out. And so you can see now that they're not just exactly uniform. And in particular, as an example here for the last row, this was the eighth token. And the eighth token knows what content it has, and it knows at what position it's in. And now the eighth token, based on that, creates a query. Hey, I'm looking for this kind of stuff. I'm a vowel. I'm on the eighth position. I'm looking for any consonants at positions up to four. And then all the nodes get to emit keys. And maybe one of the channels could be I am a consonant, and I am in a position up to four. And that key would have a high number in that specific channel. And that's how the query and the key, when they dark product, can find each other and create a high affinity. And when they have a high affinity, like say this token was pretty interesting to this eighth token, when they have a high affinity, then through the softmax, I will end up aggregating a lot of its information into my position. And so I'll get to learn a lot about it. Now, we're looking at way after this has already happened. Let me erase this operation as well. So let me erase the masking and the softmax just to show you the under the hood internals and how that works. So without the masking and the softmax, way comes out like this, right? This is the outputs of the dark products. And these are the raw outputs, and they take on values from negative two to positive two, et cetera. So that's the raw interactions and raw affinities between all the nodes. But now, if I'm a fifth node, I will not want to aggregate anything from the sixth node, seventh node and the eighth node. So actually, we use the upper triangular masking. So those are not allowed to communicate. And now we actually want to have a nice distribution. So we don't want to aggregate negative 0.11 of this node. That's crazy. So instead, we exponentiate and normalize. And now we get a nice distribution that sums to one. And this is telling us now in a data dependent manner, how much of information to aggregate from any of these tokens in the past. So that's way, and it's not zeros anymore, but it's calculated in this way. Now, there's one more part to a single self-attention head. And that is that when we do the aggregation, we don't actually aggregate the tokens exactly. We aggregate, we produce one more value here, and we call that the value. So in the same way that we produced key and query, we're also going to create a value. And then here, we don't aggregate X. We calculate a V, which is just achieved by propagating this linear on top of X again. And then we output Y multiplied by V. So V is the elements that we aggregate or the vector that we aggregate instead of the raw X. And now, of course, this will make it so that the output here of the single head will be 16 dimensional because that is the head size. So you can think of X as kind of like private information to this token, if you think about it that way. So X is kind of private to this token. So I'm a fifth token, and I have some identity, and my information is kept in vector X. And now for the purposes of the single head, here's what I'm interested in. Here's what I have, and if you find me interesting, here's what I will communicate to you. And that's stored in V. And so V is the thing that gets aggregated for the purposes of this single head between the different nodes. And that's basically the self-attention mechanism. This is what it does. There are a few notes that I would like to make about attention. Number one, attention is a communication mechanism. You can really think about it as a communication mechanism where you have a number of nodes in a directed graph where basically you have edges pointing between nodes like this. And what happens is every node has some vector of information, and it gets to aggregate information via a weighted sum from all of the nodes that point to it. And this is done in a data-dependent manner, so depending on whatever data is actually stored at each node at any point in time. Now, our graph doesn't look like this. Our graph has a different structure. We have eight nodes because the block size is eight, and there's always eight tokens. And the first node is only pointed to by itself. The second node is pointed to by the first node and itself, all the way up to the eighth node, which is pointed to by all the previous nodes and itself. And so that's the structure that our directed graph has, or happens to have, in an autoregressive sort of scenario like language modeling. But in principle, attention can be applied to any arbitrary directed graph, and it's just a communication mechanism between the nodes. The second node is that notice that there is no notion of space. So attention simply acts over like a set of vectors in this graph. And so by default, these nodes have no idea where they are positioned in the space, and that's why we need to encode them positionally and sort of give them some information that is anchored to a specific position so that they sort of know where they are. And this is different than, for example, from convolution, because if you run, for example, a convolution operation over some input, there is a very specific sort of layout of the information in space, and the convolutional filters sort of act in space. And so it's not like an attention. An attention is just a set of vectors out there in space. They communicate, and if you want them to have a notion of space, you need to specifically add it, which is what we've done when we calculated the positional encodings and added that information to the vectors. The next thing that I hope is very clear is that the elements across the batch dimension, which are independent examples, never talk to each other. They're always processed independently, and this is a batched matrix multiply that applies basically a matrix multiplication kind of in parallel across the batch dimension. So maybe it would be more accurate to say that in this analogy of a directed graph, we really have, because the batch size is four, we really have four separate pools of eight nodes, and those eight nodes only talk to each other. But in total, there's like 32 nodes that are being processed, but there's sort of four separate pools of eight. You can look at it that way. The next note is that here in the case of language modeling, we have this specific structure of directed graph where the future tokens will not communicate to the past tokens. But this doesn't necessarily have to be the constraint in the general case. And in fact, in many cases, you may want to have all of the nodes talk to each other fully. So as an example, if you're doing sentiment analysis or something like that with a transformer, you might have a number of tokens, and you may want to have them all talk to each other fully, because later you are predicting, for example, the sentiment of the sentence. And so it's okay for these nodes to talk to each other. And so in those cases, you will use an encoder block of self-attention. And all it means that it's an encoder block is that you will delete this line of code, allowing all the nodes to completely talk to each other. What we're implementing here is sometimes called a decoder block, and it's called a decoder because it is sort of like decoding language, and it's got this autoregressive format where you have to mask with the triangular matrix so that nodes from the future never talk to the past, because they would give away the answer. And so basically, in encoder blocks, you would delete this, allow all the nodes to talk. In decoder blocks, this will always be present so that you have this triangular structure. But both are allowed, and attention doesn't care. Attention supports arbitrary connectivity between nodes. The next thing I wanted to comment on is you keep hearing me say attention, self-attention, etc. There's actually also something called cross-attention. What is the difference? Basically, the reason this attention is self-attention is because the keys, queries, and the values are all coming from the same source, from X. So the same source, X, produces keys, queries, and values. So these nodes are self-attending. But in principle, attention is much more general than that. So for example, in encoder-decoder transformers, you can have a case where the queries are produced from X, but the keys and the values come from a whole separate external source, and sometimes from encoder blocks that encode some context that we'd like to condition on. And so the keys and the values will actually come from a whole separate source. Those are nodes on the side, and here we're just producing queries, and we're reading off information from the side. So cross-attention is used when there's a separate source of nodes we'd like to pool information from into our nodes. And it's self-attention if we just have nodes that would like to look at each other and talk to each other. So this attention here happens to be self-attention. But in principle, attention is a lot more general. Okay, and the last note at this stage is, if we come to the attention is all you need paper here, we've already implemented attention. So given query key and value, we've multiplied the query on the key, we've soft-maxed it, and then we are aggregating the values. There's one more thing that we're missing here, which is the dividing by 1 over square root of the head size. The decay here is the head size. Why are they doing this? Why is this important? So they call it a scaled attention. And it's kind of like an important normalization to basically have. The problem is if you have unit Gaussian inputs, so 0 mean unit variance, k and q are unit Gaussian, and if you just do weigh naively, then you see that your weigh actually will be, the variance will be on the order of head size, which in our case is 16. But if you multiply by 1 over head size square root, so this is square root and this is 1 over, then the variance of weigh will be 1, so it will be preserved. Now why is this important? You'll notice that weigh here will feed into softmax. And so it's really important, especially at initialization, that weigh be fairly diffuse. So in our case here, we sort of locked out here, and weigh had fairly diffuse numbers here. Like this. Now the problem is that because of softmax, if weigh takes on very positive and very negative numbers inside it, softmax will actually converge towards one-hot vectors. And so I can illustrate that here. Say we are applying softmax to a tensor of values that are very close to 0, then we're going to get a diffuse thing out of softmax. But the moment I take the exact same thing and I start sharpening it, making it bigger by multiplying these numbers by 8, for example, you'll see that the softmax will start to sharpen. And in fact, it will sharpen towards the max, so it will sharpen towards whatever number here is the highest. And so basically we don't want these values to be too extreme, especially at initialization, otherwise softmax will be way too peaky. And you're basically aggregating information from a single node. Every node just aggregates information from a single other node. That's not what we want, especially at initialization. And so the scaling is used just to control the variance at initialization. Okay, so having said all that, let's now take our self-attention knowledge and let's take it for a spin. So here in the code, I've created this head module and it implements a single head of self-attention. So you give it a head size and then here it creates the key query and the value linear layers. Typically people don't use biases in these. So those are the linear projections that we're going to apply to all of our nodes. Now here I'm creating this trill variable. Trill is not a parameter of the module. So in sort of PyTorch naming conventions, this is called a buffer. It's not a parameter. And you have to assign it to the module using a register buffer. So that creates the trill, the lower triangular matrix. And when we're given the input x, this should look very familiar now. We calculate the keys, the queries, we call it calculate the attention scores in Sideway. We normalize it, so we're using scaled attention here. Then we make sure that Future doesn't communicate with the past, so this makes it a decoder block. And then softmax, and then aggregate the value and output. Then here in the language model, I'm creating a head in the constructor, and I'm calling it selfAttentionHead. And the head size, I'm going to keep as the same, an embed, just for now. And then here, once we've encoded the information with the token embeddings and the position embeddings, we're simply going to feed it into the selfAttentionHead, and then the output of that is going to go into the decoder language modeling head, and create the logits. So this is sort of the simplest way to plug in a selfAttention component into our network right now. I had to make one more change, which is that here, in the generate, we have to make sure that our IDX that we feed into the model, because now we're using positional embeddings, we can never have more than block size coming in, because if IDX is more than block size, then our position embedding table is going to run out of scope, because it only has embeddings for up to block size. And so therefore, I added some code here to crop the context that we're gonna feed into self, so that we never pass in more than block size elements. So those are the changes, and let's now train the network. Okay, so I also came up to the script here, and I decreased the learning rate, because the selfAttention can't tolerate very, very high learning rates. And then I also increased the number of iterations, because the learning rate is lower. And then I trained it, and previously, we were only able to get to up to 2.5, and now we are down to 2.4. So we definitely see a little bit of an improvement from 2.5 to 2.4, roughly, but the text is still not amazing. So clearly, the selfAttention head is doing some useful communication, but we still have a long way to go. Okay, so now we've implemented the scale.productAttention. Now, next up in the attention is all you need paper, there's something called multi-headAttention. And what is multi-headAttention? It's just applying multiple attentions in parallel and concatenating their results. So they have a little bit of diagram here. I don't know if this is super clear. It's really just multiple attentions in parallel. So let's implement that, fairly straightforward. If we want a multi-headAttention, then we want multiple heads of selfAttention running in parallel. So in PyTorch, we can do this by simply creating multiple heads. So however many heads you want, and then what is the head size of each? And then we run all of them in parallel into a list and simply concatenate all of the outputs. And we're concatenating over the channel dimension. So the way this looks now is we don't have just a single attention that has a head size of 32, because remember, an embed is 32. Instead of having one communication channel, we now have four communication channels in parallel. And each one of these communication channels typically will be smaller correspondingly. So because we have four communication channels, we want eight-dimensional selfAttention. And so from each communication channel, we're getting together eight-dimensional vectors, and then we have four of them, and that concatenates to give us 32, which is the original, an embed. And so this is kind of similar to, if you're familiar with convolutions, this is kind of like a group convolution, because basically, instead of having one large convolution, we do convolution in groups, and that's multi-headed selfAttention. And so then here, we just use SA heads, selfAttention heads instead. Now, I actually ran it, and scrolling down, I ran the same thing, and then we now get this down to 2.28, roughly. And the output is still, the generation is still not amazing, but clearly, the validation loss is improving, because we were at 2.4 just now. And so it helps to have multiple communication channels, because obviously, these tokens have a lot to talk about. They want to find the consonants, the vowels, they want to find the vowels just from certain positions, they want to find any kinds of different things. And so it helps to create multiple independent channels of communication, gather lots of different types of data, and then decode the output. Now, going back to the paper for a second, of course, I didn't explain this figure in full detail, but we are starting to see some components of what we've already implemented. We have the positional encodings, the token encodings that add, we have the masked multi-headed attention implemented. Now, here's another multi-headed attention, which is a cross attention to an encoder, which we haven't, we're not going to implement in this case. I'm going to come back to that later. But I want you to notice that there's a feedforward part here, and then this is grouped into a block that gets repeated again and again. Now, the feedforward part here is just a simple multi-headed perceptron. So the multi-headed, so here position-wise feedforward networks, is just a simple little MLP. So I want to start basically in a similar fashion, also adding computation into the network. And this computation is on a per node level. So I've already implemented it, and you can see the diff highlighted on the left here when I've added or changed things. Now, before we had the multi-headed self-attention that did the communication, but we went way too fast to calculate the logits. So the tokens looked at each other, but didn't really have a lot of time to think on what they found from the other tokens. And so what I've implemented here is a little feedforward single layer. And this little layer is just a linear followed by a ReLU non-linearity, and that's it. So it's just a little layer, and then I call it feedforward and embed. And then this feedforward is just called sequentially right after the self-attention. So we self-attend, then we feedforward. And you'll notice that the feedforward here, when it's applying linear, this is on a per token level. All the tokens do this independently. So the self-attention is the communication, and then once they've gathered all the data, now they need to think on that data individually. And so that's what feedforward is doing, and that's why I've added it here. Now, when I train this, the validation loss actually continues to go down, now to 2.24, which is down from 2.28. The output still look kind of terrible, but at least we've improved the situation. And so as a preview, we're going to now start to intersperse the communication with the computation. And that's also what the transformer does when it has blocks that communicate and then compute, and it groups them and replicates them. Okay, so let me show you what we'd like to do. We'd like to do something like this. We have a block, and this block is basically this part here, except for the cross-attention. Now, the block basically intersperses communication and then computation. The communication is done using multi-headed self-attention, and then the computation is done using a feedforward network on all the tokens independently. Now, what I've added here also is, you'll notice, this takes the number of embeddings in the embedding dimension and the number of heads that we would like, which is kind of like group size in group convolution. And I'm saying that number of heads we'd like is four. And so because this is 32, we calculate that because this is 32, the number of heads should be four. The head size should be eight so that everything sort of works out channel-wise. So this is how the transformer structures sort of the sizes, typically. So the head size will become eight, and then this is how we want to intersperse them. And then here, I'm trying to create blocks, which is just a sequential application of block, block, block. So then we're interspersing communication and feedforward many, many times, and then finally we decode. Now, I actually tried to run this, and the problem is this doesn't actually give a very good answer, a very good result. And the reason for that is we're starting to actually get like a pretty deep neural net. And deep neural nets suffer from optimization issues, and I think that's what we're kind of like slightly starting to run into. So we need one more idea that we can borrow from the transformer paper to resolve those difficulties. Now, there are two optimizations that dramatically help with the depth of these networks and make sure that the networks remain optimizable. Let's talk about the first one. The first one in this diagram is you see this arrow here, and then this arrow and this arrow, those are skip connections, or sometimes called residual connections. They come from this paper, the Presidual Learning for Image Recognition from about 2015 that introduced the concept. Now, these are basically, what it means is you transform the data, but then you have a skip connection with addition from the previous features. Now, the way I like to visualize it that I prefer is the following. Here, the computation happens from the top to bottom. And basically you have this residual pathway, and you are free to fork off from the residual pathway, perform some computation, and then project back to the residual pathway via addition. And so you go from the inputs to the targets only via plus and plus and plus. And the reason this is useful is because during back propagation, remember from our micrograd video earlier, addition distributes gradients equally to both of its branches that fed us the input. And so the supervision or the gradients from the loss basically hop through every addition node all the way to the input, and then also fork off into the residual blocks. But basically you have this gradient superhighway that goes directly from the supervision all the way to the input, unimpeded. And then these residual blocks are usually initialized in the beginning. So they contribute very, very little, if anything, to the residual pathway. They are initialized that way. So in the beginning, they are sort of almost kind of like not there. But then during the optimization, they come online over time and they start to contribute. But at least at the initialization, you can go from directly supervision to the input, gradient is unimpeded and just flows. And then the blocks over time kick in. And so that dramatically helps with the optimization. So let's implement this. So coming back to our block here, basically what we want to do is we want to do X equals X plus self-attention and X equals X plus self.feedforward. So this is X, and then we fork off and do some communication and come back. And we fork off and we do some computation and come back. So those are residual connections. And then swinging back up here, we also have to introduce this projection. So nn.linear. And this is going to be from, after we concatenate this, this is the size n-embed. So this is the output of the self-attention itself. But then we actually want to apply the projection, and that's the result. So the projection is just a linear transformation of the outcome of this layer. So that's the projection back into the residual pathway. And then here in the feedforward, it's going to be the same thing. I could have a self.projection here as well, but let me just simplify it and let me couple it inside the same sequential container. And so this is the projection layer going back into the residual pathway. And so that's, well, that's it. So now we can train this. So I implemented one more small change. When you look into the paper again, you see that the dimensionality of input and output is 512 for them. And they're saying that the inner layer here in the feedforward has dimensionality of 2048. So there's a multiplier of four. And so the inner layer of the feedforward network should be multiplied by four in terms of channel sizes. So I came here and I multiplied four times embed here for the feedforward, and then from four times n-embed coming back down to n-embed when we go back to the projection. So adding a bit of computation here and growing that layer that is in the residual block on the side of the residual pathway. And then I train this, and we actually get down all the way to 2.08 validation loss. And we also see that the network is starting to get big enough that our train loss is getting ahead of validation loss. So we started to see like a little bit of overfitting. And our generations here are still not amazing, but at least you see that we can see like is here, this now, grief, sink, like this starts to almost look like English. So yeah, we're starting to really get there. Okay, and the second innovation that is very helpful for optimizing very deep neural networks is right here. So we have this addition now, that's the residual part, but this norm is referring to something called layer norm. So layer norm is implemented in PyTorch. It's a paper that came out a while back here. And layer norm is very, very similar to bash norm. So remember back to our make more series part three, we implemented bash normalization. And bash normalization basically just made sure that across the bash dimension, any individual neuron had unit Gaussian distribution. So it was zero mean and unit standard deviation, one standard deviation output. So what I did here is I'm copy pasting the bash norm 1D that we developed in our make more series. And see here, we can initialize, for example, this module, and we can have a batch of 32, 100 dimensional vectors feeding through the bash norm layer. So what this does is it guarantees that when we look at just the zeroth column, it's a zero mean, one standard deviation. So it's normalizing every single column of this input. Now the rows are not going to be normalized by default because we're just normalizing columns. So let's not implement layer norm. It's very complicated. Look, we come here, we change this from zero to one. So we don't normalize the columns, we normalize the rows. And now we've implemented layer norm. So now the columns are not going to be normalized, but the rows are going to be normalized for every individual example. It's 100 dimensional vector is normalized in this way. And because our computation now does not span across examples, we can delete all of this buffers stuff because we can always apply this operation and don't need to maintain any running buffers. So we don't need the buffers. There's no distinction between training and test time. And we don't need these running buffers. We do keep gamma and beta. We don't need the momentum. We don't care if it's training or not. And this is now a layer norm and it normalizes the rows instead of the columns. And this here is identical to basically this here. So let's now implement layer norm in our transformer. Before I incorporate the layer norm, I just wanted to note that, as I said, very few details about the transformer have changed in the last five years, but this is actually something that slightly departs from the original paper. You see that the add and norm is applied after the transformation, but now it is a bit more basically common to apply the layer norm before the transformation. So there's a reshuffling of the layer norms. So this is called the pre-norm formulation and that's the one that we're going to implement as well. So slight deviation from the original paper. Basically we need to layer norms. Layer norm one is an n dot layer norm. And we tell it what is the embedding dimension and we need the second layer norm. And then here, the layer norms are applied immediately on X. So self dot layer norm one applied on X and self dot layer norm two applied on X before it goes into self-attention and feed forward. And the size of the layer norm here is an embed, so 32. So when the layer norm is normalizing our features, it is the normalization here happens. The mean and the variance are taken over 32 numbers. So the batch and the time act as batch dimensions, both of them. So this is kind of like a per token transformation that just normalizes the features and makes them a unit mean, unit Gaussian at initialization. But of course, because these layer norms inside it have these gamma and beta trainable parameters, the layer norm will eventually create outputs that might not be unit Gaussian, but the optimization will determine that. So for now, this is incorporating the layer norms and let's train them up. Okay, so I let it run and we see that we get down to 2.06, which is better than the previous 2.08. So a slight improvement by adding the layer norms. And I'd expect that they help even more if we had bigger and deeper network. One more thing I forgot to add is that there should be a layer norm here also typically as at the end of the transformer and right before the final linear layer that decodes into vocabulary. So I added that as well. So at this stage, we actually have a pretty complete transformer according to the original paper, and it's a decoder only transformer. I'll talk about that in a second. But at this stage, the major pieces are in place, so we can try to scale this up and see how well we can push this number. Now, in order to scale up the model, I had to perform some cosmetic changes here to make it nicer. So I introduced this variable called nlayer, which just specifies how many layers of the blocks we're going to have. I create a bunch of blocks and we have a new variable number of heads as well. I pulled out the layer norm here, and so this is identical. Now, one thing that I did briefly change is I added a dropout. So dropout is something that you can add right before the residual connection back, right before the connection back into the residual pathway. So we can drop out that as the last layer here. We can drop out here at the end of the multi-headed restriction as well. And we can also drop out here when we calculate the basically affinities and after the softmax, we can drop out some of those. So we can randomly prevent some of the nodes from communicating. And so dropout comes from this paper from 2014 or so. And basically it takes your neural net and it randomly, every forward-backward pass shuts off some subset of neurons. So randomly drops them to zero and trains without them. And what this does effectively is because the mask of what's being dropped out has changed every single forward-backward pass, it ends up kind of training an ensemble of sub-networks. And then at test time, everything is fully enabled and kind of all of those sub-networks are merged into a single ensemble, if you wanna think about it that way. So I would read the paper to get the full detail. For now, we're just going to stay on the level of this is a regularization technique. And I added it because I'm about to scale up the model quite a bit, and I was concerned about overfitting. So now when we scroll up to the top, we'll see that I changed a number of hyperparameters here about our neural net. So I made the batch size be much larger, now it's 64. I changed the block size to be 256. So previously it was just eight, eight characters of context. Now it is 256 characters of context to predict the 257th. I brought down the learning rate a little bit because the neural net is now much bigger. So I brought down the learning rate. The embedding dimension is now 384, and there are 6 heads. So 384 divide 6 means that every head is 64-dimensional, as a standard. And then there are going to be 6 layers of that. And the dropout will be at 0.2. So every forward-backward pass, 20% of all of these intermediate calculations are disabled and dropped to 0. And then I already trained this and I ran it, so drumroll, how well does it perform? So let me just scroll up here. We get a validation loss of 1.48, which is actually quite a bit of an improvement on what we had before, which I think was 2.07. So we went from 2.07 all the way down to 1.48 just by scaling up this neural net with the code that we have. And this, of course, ran for a lot longer. This may be trained for, I want to say, about 15 minutes on my A100 GPU. So that's a pretty good GPU. And if you don't have a GPU, you're not going to be able to reproduce this. On a CPU, this would be  I would not run this on a CPU or a MacBook or something like that. You'll have to break down the number of layers and the embedding dimension and so on. But in about 15 minutes, we can get this kind of a result. And I'm printing some of the Shakespeare here, but what I did also is I printed 10,000 characters, so a lot more, and I wrote them to a file. And so here we see some of the outputs. So it's a lot more recognizable as the input text file. So the input text file, just for reference, looked like this. So there's always someone speaking in this manner. And our predictions now take on that form. Except, of course, they're nonsensical when you actually read them. So it is, every crimpty be house. Oh, those prepation. We give heed. You know. Oh, ho, sent me you mighty lord. Anyway, so you can read through this. It's nonsensical, of course, but this is just a transformer trained on the character level for 1 million characters that come from Shakespeare. So there's sort of like blabbers on in Shakespeare-like manner, but it doesn't, of course, make sense at this scale. But I think still a pretty good demonstration of what's possible. So now I think that kind of concludes the programming section of this video. We basically kind of did a pretty good job of implementing this transformer. But the picture doesn't exactly match up to what we've done. So what's going on with all these additional parts here? So let me finish explaining this architecture and why it looks so funky. Basically what's happening here is what we implemented here is a decoder only transformer. So there's no component here. This part is called the encoder. And there's no cross attention block here. Our block only has a self attention and the feed forward. So it is missing this third in between piece here. This piece does cross attention. So we don't have it and we don't have the encoder. We just have the decoder. The reason we have a decoder only is because we are just generating text and it's unconditioned on anything. We're just blabbering on according to a given data set. What makes it a decoder is that we are using the triangular mask in our transformer. So it has this autoregressive property where we can just go and sample from it. So the fact that it's using the triangular mask to mask out the attention makes it a decoder. And it can be used for language modeling. Now, the reason that the original paper had an encoder decoder architecture is because it is a machine translation paper. So it is concerned with a different setting. In particular, it expects some tokens that encode, say, for example, French. And then it is expected to decode the translation in English. So typically, these here are special tokens. So you are expected to read in this and condition on it. And then you start off the generation with a special token called start. So this is a special new token that you introduce and always place in the beginning. And then the network is expected to output neural networks are awesome. And then a special end token to finish the generation. So this part here will be decoded exactly as we have. We've done it. Neural networks are awesome. We'll be identical to what we did. But unlike what we did, they want to condition the generation on some additional information. And in that case, this additional information is the French sentence that they should be translating. So what they do now is they bring the encoder. Now, the encoder reads this part here. So we're only going to take the part of French and we're going to create tokens from it exactly as we've seen in our video. And we're going to put a transformer on it. But there's going to be no triangular mask. And so all the tokens are allowed to talk to each other as much as they want. And they're just encoding whatever the content of this French sentence was. They've encoded it. They've they basically come out in the top here. And then what happens here is in our decoder, which does the language modeling, there's an additional connection here to the outputs of the encoder. And that is brought in through cross attention. So the queries are still generated from X. But now the keys and the values are coming from the side. The keys and the values are coming from the top generated by the nodes that came outside of the decode, the encoder. And those tops, the keys and the values, they're the top of it. Feed in on the side into every single block of the decoder. And so that's why there's an additional cross attention. And really what it's doing is it's conditioning the decoding, not just on the past of this current decoding, but also on having seen the full, fully encoded French prompt, sort of. And so it's an encoder decoder model, which is why we have those two transformers and an additional block and so on. So we did not do this because we have no we have nothing to encode. There's no conditioning. We just have a text file and we just want to imitate it. And that's why we are using a decoder only transformer, exactly as done in GPT. OK, so now I wanted to do a very brief walkthrough of NanoGPT, which you can find on my GitHub. And NanoGPT is basically two files of interest. There's train.py and model.py. Train.py is all the boilerplate code for training the network. It is basically all the stuff that we had here is the training loop. It's just that it's a lot more complicated because we're saving and loading checkpoints and pre-trained weights and we are decaying the learning rate and compiling the model and using distributed training across multiple nodes or GPUs. So the train.py gets a little bit more hairy, complicated. There's more options, etc. But the model.py should look very, very similar to what we've done here. In fact, the model is almost identical. So first, here we have the causal self-attention block and all of this should look very, very recognizable to you. We're producing queries, keys, values. We're doing dot products. We're masking, applying softmax, optionally dropping out. And here we are pooling the values. What is different here is that in our code, I have separated out the multi-headed attention into just a single individual head. And then here I have multiple heads and I explicitly concatenate them. Whereas here, all of it is implemented in a batched manner inside a single causal self-attention. And so we don't just have a B and a T and a C dimension. We also end up with a fourth dimension, which is the heads. And so it just gets a lot more hairy because we have four dimensional array tensors now, but it is equivalent mathematically. So the exact same thing is happening as what we have. It's just it's a bit more efficient because all the heads are now treated as a batch dimension as well. Then we have the multilayer perceptron. It's using the GELU nonlinearity, which is defined here instead of RELU. And this is done just because OpenAI used it. And I want to be able to load their checkpoints. The blocks of the transformer are identical to communicate in the compute phase, as we saw. And then the GPT will be identical. We have the position encodings, token encodings, the blocks, the layer norm at the end, the final linear layer. And this should look all very recognizable. And there's a bit more here because I'm loading checkpoints and stuff like that. I'm separating out the parameters into those that should be weight decayed and those that shouldn't. But the generate function should also be very, very similar. So a few details are different, but you should definitely be able to look at this file and be able to understand a lot of the pieces now. So let's now bring things back to chatGPT. What would it look like if we wanted to train chatGPT ourselves? And how does it relate to what we learned today? Well, to train chatGPT, there are roughly two stages. First is the pre-training stage and then the fine-tuning stage. In the pre-training stage, we are training on a large chunk of Internet and just trying to get a first decoder-only transformer to babble text. So it's very, very similar to what we've done ourselves. Except we've done a tiny little baby pre-training step. And so in our case, this is how you print a number of parameters. I printed it and it's about 10 million. So this transformer that I created here to create a little Shakespeare transformer was about 10 million parameters. Our dataset is roughly 1 million characters, so roughly 1 million tokens. But you have to remember that OpenAI uses different vocabulary. They're not on the character level. They use these sub-word chunks of words. And so they have a vocabulary of 50,000 roughly elements. And so their sequences are a bit more condensed. So our dataset, the Shakespeare dataset, would be probably around 300,000 tokens in the OpenAI vocabulary, roughly. So we trained about 10 million parameter model on roughly 300,000 tokens. Now, when you go to the GPT-3 paper and you look at the transformers that they trained, they trained a number of transformers of different sizes. But the biggest transformer here has 175 billion parameters. So ours is, again, 10 million. They used this number of layers in a transformer. This is the N embed. This is the number of heads. And this is the head size. And then this is the batch size. So ours was 65. And the learning rate is similar. Now, when they trained this transformer, they trained on 300 billion tokens. So, again, remember, ours is about 300,000. So this is about a million-fold increase. And this number would not be even that large by today's standards. It would be going up 1 trillion and above. So they are training a significantly larger model on a good chunk of the Internet. And that is the pre-training stage. But otherwise, these hyperparameters should be fairly recognizable to you. And the architecture is actually, like, nearly identical to what we implemented ourselves. But, of course, it's a massive infrastructure challenge to train this. You're talking about typically thousands of GPUs having to, you know, talk to each other to train models of this size. So that's just the pre-training stage. Now, after you complete the pre-training stage, you don't get something that responds to your questions with answers and is not helpful and et cetera. You get a document completer. Right? So it babbles, but it doesn't babble Shakespeare. It babbles Internet. It will create arbitrary news articles and documents, and it will try to complete documents because that's what it's trained for. It's trying to complete the sequence. So when you give it a question, it would just potentially just give you more questions. It would follow with more questions. It will do whatever it looks like some closed document would do in the training data on the Internet. And so who knows, you're getting kind of like undefined behavior. It might basically answer two questions with other questions. It might ignore your question. It might just try to complete some news article. It's totally underlined, as we say. So the second fine-tuning stage is to actually align it to be an assistant. And this is the second stage. And so this chat GPT blog post from OpenAI talks a little bit about how this stage is achieved. We basically, there's roughly three steps to this stage. So what they do here is they start to collect training data that looks specifically like what an assistant would do. So they have documents that have the format where the question is on top and then an answer is below. And they have a large number of these, but probably not on the order of the Internet. This is probably on the order of maybe thousands of examples. And so they then fine-tune the model to basically only focus on documents that look like that. And so you're starting to slowly align it. So it's going to expect a question at the top, and it's going to expect to complete the answer. And these very, very large models are very sample efficient during their fine-tuning. So this actually somehow works. But that's just step one. That's just fine-tuning. So then they actually have more steps where, okay, the second step is you let the model respond, and then different raters look at the different responses and rank them for their preference as to which one is better than the other. They use that to train a reward model so they can predict, basically using a different network, how much of any candidate response would be desirable. And then once they have a reward model, they run PPO, which is a form of policy gradient reinforcement learning optimizer, to fine-tune this sampling policy so that the answers that the chat GPT now generates are expected to score a high reward according to the reward model. And so basically there's a whole aligning stage here, or fine-tuning stage. It's got multiple steps in between there as well. And it takes the model from being a document completer to a question answerer, and that's like a whole separate stage. A lot of this data is not available publicly. It is internal to OpenAI, and it's much harder to replicate this stage. And so that's roughly what would give you a chat GPT. And nano-GPT focuses on the pre-training stage. Okay, and that's everything that I wanted to cover today. So we trained, to summarize, a decoder-only transformer following this famous paper, Attention is All You Need, from 2017. And so that's basically a GPT. We trained it on a tiny Shakespeare and got sensible results. All of the training code is roughly 200 lines of code. I will be releasing this code base. So also it comes with all the Git log commits along the way as we built it up. In addition to this code, I'm going to release the notebook, of course, the Google Colab. And I hope that gave you a sense for how you can train these models, like, say, GPT-3, that will be architecturally basically identical to what we have, but they are somewhere between 10,000 and 1 million times bigger, depending on how you count. And so that's all I have for now. We did not talk about any of the fine-tuning stages that would typically go on top of this. So if you're interested in something that's not just language modeling, but you actually want to, you know, say, perform tasks, or you want them to be aligned in a specific way, or you want to detect sentiment or anything like that, basically any time you don't want something that's just a document completer, you have to complete further stages of fine-tuning, which we did not cover. And that could be simple, supervised fine-tuning, or it can be something more fancy, like we see in ChaiGPT, where we actually train a reward model and then do runs of PPO to align it with respect to the reward model. So there's a lot more that can be done on top of it. I think for now we're starting to get to about two hours mark, so I'm going to kind of finish here. I hope you enjoyed the lecture. And yeah, go forth and transform. See you later. 
"question","answer",
"Why do we need to zero out the gradient before backprop at each step?","When we call backward, we fill in the gradients. This will update self.grad, so the gradients will accumulate unless we explicity flush it by setting to zero.",
"What does the gradient tell us and how can we use it to minimize the loss?","The gradient tells us the direction to nudge each parameter in order to increace loss. Each update (to minimize loss) take the negative gradient multplied by a step size.",
"What is the mean squared error loss?","The mean squared error loss is the average of the squared differences between actual values and predicted values",
"What is Makemore?","Makemore is a simple charecter-level langugae model that will predict next char in a sequence given some prior charecters before it.",
"What is log likelihood loss and why do we use the negative log likelihood?","We take the sum of the log probability of the label (correct charecter) for each charecter in the string. If the label has a high probability, such as 1, will have a low loss because the log(1) is 0. If the label has a low probability, such as 0, will have a very low loss because the log(0) is -inf. But, if the label has a low probability, we want it to have a high loss, so we take the negative log.",
"How does cross entropy relate to negative log likelihood?","We use a softmax layer to compute probabilities from raw logits and then compute negative log likelihood loss from raw probabilities. Cross entropy just rolls these steps into 1.",
"What is the problem with extreme values for logits?","The exponentiation of very large positive logits can exceed the dynamic range of floating-point numbers, causing overflow issues.",
"Why do we use batch normalization?","We want roughly gaussian activaitions to avoid vanishing gradients and use a normalization layer to automate this.",
"What context window does a transformer have when predicting the output?","Transformer will never see more than `block_size` when predicting the output.",
"What is the problem with the Bigram model's context window?","The Bigram model is only looking at the last char to predict the next char.",
"How can self-attention improve on the limited context window of the Bigram model?","Self-attention lets all prior tokens to 'talk' to each other when predicting the next token.",
"How are keys and queries generated, and how do they interact?","We embed each token to a Key and Query. Each query does a dot-product with the key at each prior location. If a Key and Query are aligned, they will produce a high value.",
"For any token, what are x, k, v, and q?","x is private information to the token. q is what the token is interested in. k is what the token has. v is what the token will communicate to you if you find it interesting.",
"What is the difference between an encoder and decoder?","The decoder is typically just self-attention (communication) and feed-forward (compute). We condition on the past. It uses triangular mask on future tokens. It has an auto-regressive property where we can sample from it. The encoder can condition on the past or on a seperate source via cross-attention.",
"What are two innovations that improve optimization for deep neural nets?","First, residual connections create a gradient superhighway that goes directly from the supervision all the way to the input, unimpeded. At initialization, residual blocks effectivly allow the gradient to flow unimpeded and, over time, they come online and start to contribute. Second, layer norm will normalize the rows (or examples) in each batch independently." 
